{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to my website","text":"<p>Hey, I'm Timoth\u00e9e Mazzucotelli, @pawamoy on GitHub and other platforms. I develop open-source software, primarily Python tools and libraries. I do this full time: I earn my keep through sponsorship from individual users and companies, which is what makes these projects sustainable and gives you a chance to use them. Check out my Insiders program to learn more.</p> <p>Below is the list of all my blog posts.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#name","title":"NAME","text":"<p><code>pawamoy</code> \u2014 Timoth\u00e9e Mazzucotelli</p>"},{"location":"about/#synopsis","title":"SYNOPSIS","text":"<p><code>pawamoy [GLOBAL_OPTS] COMMAND [COMMAND_OPTS] [OBJECT | IDEA]...</code></p>"},{"location":"about/#description","title":"DESCRIPTION","text":"<p><code>pawamoy</code> lets you take control over the person identified by the name Timoth\u00e9e Mazzucotelli.</p> <p>Timoth\u00e9e Mazzucotelli was born in France and still lives there. He received education in Computer Science and obtained his Masters in C.S. in Universit\u00e9 de Strasbourg. He is therefore able to write code, amongst other things (see COMMANDS).</p>"},{"location":"about/#global-options","title":"GLOBAL OPTIONS","text":"<p>These options influence how the command will be run. Please note that some switches may not apply to some commands.</p> <p><code>--chatty</code></p> <p></p> <p>Increase verbosity. This flag is automatically switched on when command <code>drink</code> is used with option <code>--alcohol</code>. Default: false.</p> <p><code>--enthusiast</code></p> <p></p> <p>This option is always useful, especially when learning new things. Use it without caution!</p> <p><code>--fast</code></p> <p></p> <p>Do things fast. Behavior becomes non-deterministic: things will be done well in average, but sometimes they will not. This option can yield more interesting results though.</p> <p><code>--happy</code></p> <p></p> <p>Everyone wants to be happy right? Use this option regularly to ensure proper sanity of mind.</p> <p><code>--introvert</code></p> <p></p> <p>Act with more reserve. Talk less. This option overrides and disables <code>--chatty</code>. There is a high probability that <code>--over-thinking</code> will be switched on using <code>--introvert</code>. This option can be used to avoid unnecessary jokes during professional interactions, but try not to use it too much as it will reduce enthusiasm and increase risk of switching <code>--silent</code> on.</p> <p><code>--open</code></p> <p></p> <p>Stay open! Everything you hear or see will be received with more curiosity and enthusiasm. Best used in combination with <code>learn</code> and <code>work</code> commands.</p> <p><code>--over-thinking</code></p> <p></p> <p>Spend more time thinking about things than actually doing them.</p> <p><code>--perfectionist</code></p> <p></p> <p>Nothing is ever good enough. Use this option to enable a \"constantly-improving-and-refactoring\" behavior. Beware: time spent in this mode is exponentially increased, and there is no guarantee to obtain a final product!</p> <p><code>--reluctant</code></p> <p></p> <p>When inspiration is low, reluctant mode is activated. Things will be harder at first, but once the machinery is warmed-up, this mode will be deactivated and behavior will go back to normal.</p> <p><code>--safe</code></p> <p></p> <p>Sometimes things are dangerous. Use this mode to increase cautiousness and avoid accidents.</p> <p><code>--silent</code></p> <p></p> <p>Do not say a word and be silent. This mode can be used to silently sneak behind someone and surprise them. Be sure to know the person though. This mode is also used at night, to avoid waking up the significant other.</p> <p><code>--slow</code></p> <p></p> <p>Side-effect of <code>--reluctant</code>, also triggered when tired. Everyone needs a bit of slowness from time to time right?</p>"},{"location":"about/#commands","title":"COMMANDS","text":"<p><code>code</code></p> <p></p> <p>Write or design code. It implies thinking, and can imply drawing. This command can be run regularly, without moderation. It should not be used 100% of the time though, because other vital tasks need attention.</p> <p><code>drink</code></p> <p></p> <p>Drink liquids. Options are: <code>--water</code> (the default), <code>--juice</code>, <code>--alcohol</code> and <code>--soda</code>. Juices are good in the morning, while alcohol is better for social events, though not mandatory. Soda is really an extra, for example when eating pizza (both go well together). Water is mandatory. A bottle of it must always be available at night.</p> <p><code>eat</code></p> <p></p> <p>Eat food. Almost every kind of food is accepted as positional argument. Ability to eat insects is not yet implemented, but might be in the future.</p> <p><code>exercise</code></p> <p></p> <p>Exercise command should be run regularly, like two or three times a week. Option <code>--bike</code> is built-in and very often used as the main transport mean. Currently, option <code>--badminton</code> is available, and soon maybe <code>--basketball</code> will be implemented.</p> <p><code>learn</code></p> <p></p> <p>Learn new things. It takes time and depending on the thing to learn, <code>--reluctant</code> might be enabled at first. In any case, don't forget to use the <code>--open</code> global option to ease the process. One thing that seems to be instantly learned and remembered is new keyboard shortcuts.</p> <p><code>listen</code></p> <p></p> <p>Focus on listening. Probability of talking is decreased. This command is well used when someone is asking a question. It helps preventing an anticipation bias that often leads to an incorrect comprehension of the question, and thus an incorrect answer.</p> <p><code>love</code></p> <p></p> <p>You can love many things and people. Don't restrict yourself. Isn't love a choice after all? Brutal black metal was not loved at first, it took a bit of time and training to be able to listen to it, but now it's one of the most-cherished thing!</p> <p><code>play</code></p> <p></p> <p>Play single or multi-player. Card games, video games, especially horror ones, all sorts of games! Option <code>--vr</code> is already implemented but is waiting for adequate hardware.</p> <p><code>read</code></p> <p></p> <p>Read a book, an article, a joke, docs, a tutorial, news. Read it on a desktop screen, on a smartphone screen, on an e-book screen, or... on paper directly? Technology is crazy. While using this option, please take care of your eyes: enable blue-light filters and adapt luminosity.</p> <p><code>sleep</code></p> <p></p> <p>Sleep must be done regularly and in sufficient quantity to ensure proper performance in any other activity. Run this command at least once per day (better during the night) for at least eight hours. More is better. Less, and headache will appear, and <code>--reluctant</code> flag will be turned on until more sleep is done.</p> <p><code>talk</code></p> <p></p> <p>Talk about universal determinism at parties.</p> <p><code>think</code></p> <p></p> <p>Don't start to write code for your complex project idea immediately! Think it before! But be careful: not too much. You'll want to implement all the options. Yes, all. It will end as a generic program to do anything, and it will fail.</p> <p><code>work</code></p> <p></p> <p>Chop chop! It's time to work! But your work is your passion, so it's not really work, is it?</p> <p><code>write</code></p> <p></p> <p>Write man pages, blog posts, documentation, code, fiction. You should write more fiction.</p>"},{"location":"about/#bugs","title":"BUGS","text":"<p><code>pawamoy</code> has an extra pair of ribs. This bug does not come from the two engineers that designed <code>pawamoy</code>. It is due to some binary data corruption during replication over network. It can't be fixed.</p>"},{"location":"about/#copyright","title":"COPYRIGHT","text":"<p>Copyright 1991-2009 Mazzucotelli Foundation, Inc.</p> <p>This is proprietary software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>"},{"location":"about/#environment-variables","title":"ENVIRONMENT VARIABLES","text":"<p><code>HOME</code> - Define the home <code>pawamoy</code> will use. \"Moon\", \"Area 51\", or \"Public bathrooms\" are not valid values.</p> <p><code>ENERGY</code> - An integer between 1 and 100. Using 0 will cause <code>pawamoy</code> to self-destruct. Use with caution. Use commands <code>eat</code> then <code>sleep</code> when <code>ENERGY</code> is low to increase it again.</p> <p><code>HEALTH</code> - An integer between 1 and 100. Using 0 is reserved for the end-of-life date of <code>pawamoy</code>. Don't use it before.</p> <p><code>HUNGER</code> - An integer between 1 and 100. 0 means extra-full, 100 means starving. Use command <code>eat</code> to decrease <code>HUNGER</code>.</p> <p><code>TEMPERATURE</code> - In degree Celsius. Try to increase it just a bit when <code>HEALTH</code> is lowering. This is experimental. Use it at my own risk.</p>"},{"location":"about/#files","title":"FILES","text":"<p><code>/boot/config</code> - This file was useful only once, because <code>pawamoy</code> can never reboot. Once shut down, it stays shut down.</p> <p><code>/sys/cpu</code> - This file was auto-generated, and is self-mutating. Please don't mess too much with this file.</p> <p><code>/etc/pawamoy/principles</code> - You can modify principles here. Don't add too many.</p> <p><code>/var/log/pawamoy</code> - Interesting statistics and analytics about <code>pawamoy</code> usage.</p>"},{"location":"about/#license","title":"LICENSE","text":"<p><code>pawamoy</code> is released under the terms of the <code>Human Decency</code> license. Please use it accordingly. You cannot duplicate <code>pawamoy</code>. At least for now.</p>"},{"location":"about/#see-also","title":"SEE ALSO","text":"<p><code>@pawamoy(github)</code>, <code>@pawamoy(fosstodon)</code> <code>@pawamoy(stackoverflow)</code>, <code>@pawamoy(twitter)</code></p>"},{"location":"about/#notes","title":"NOTES","text":"<p>This man page is a perpetual work in progress. Expect some delays between <code>pawamoy</code> releases and its documentation updates.</p>"},{"location":"credits/","title":"Credits","text":"<p>I want to say thank you to all these projects, companies, foundations, consortiums and alliances on which I stood to create things! And another gigantic THANK YOU to the Open Source community: authors, contributors, maintainers, sustainers, bloggers, writers, testers, backers, and the rest!</p> <p> </p> <p>This blog is graciously hosted by GitHub, and is built with MkDocs and the Material for Mkdocs theme.</p>"},{"location":"insiders/","title":"Insiders","text":"<p>I develop open-source software, primarily Python tools and libraries. I do this full time: I earn my keep through sponsorship from individual users and companies, which is what makes these projects sustainable and gives you a chance to use them.</p> <p>I follow a sponsorware release strategy: new features are at first exclusively available to sponsors. Read on to learn what sponsorships achieve, how to become a sponsor to get access to Insiders, and what's in it for you!</p> <p></p> <p></p>"},{"location":"insiders/#what-is-insiders","title":"What is Insiders?","text":"<p>Insiders are private forks of my projects, hosted as private GitHub repositories. Almost<sup>1</sup> all new features are developed as part of these forks, which means that they are immediately available to all eligible sponsors, as they are made members of these repositories.</p> <p>Every feature is tied to a funding goal in monthly subscriptions. When a funding goal is hit, the features that are tied to it are merged back into the public projects and released for general availability, making them available to all users. Bugfixes are always released in tandem.</p> <p>Sponsorships start as low as $10 a month.<sup>2</sup></p>"},{"location":"insiders/#what-sponsorships-achieve","title":"What sponsorships achieve","text":"<p>Sponsorships make these projects sustainable, as they buy the maintainers of these projects time \u2013 a very scarce resource \u2013 which is spent on the development of new features, bug fixing, stability improvement, issue triage and general support. The biggest bottleneck in Open Source is time.<sup>3</sup></p> <p>If you're unsure if you should sponsor my projects, check out the list of completed funding goals to learn whether you're already using features that were developed with the help of sponsorships. You're most likely using at least a handful of them, thanks to our awesome sponsors!</p>"},{"location":"insiders/#whats-in-it-for-me","title":"What's in it for me?","text":"<p>The moment you become a sponsor, you'll get immediate access to 24 additional features that you can start using right away, and which are currently exclusively available to sponsors:</p> <ul> <li> mkdocstrings/c \u2014 [Project] A C handler for mkdocstrings </li> <li> pawamoy/markdown-exec \u2014 Custom icons in tree fences </li> <li> pawamoy/mkdocs-pygments \u2014 [Project] Highlighting themes for code blocks</li> <li> mkdocstrings/typescript \u2014 [Project] A TypeScript handler for mkdocstrings</li> <li> mkdocstrings/griffe-typedoc \u2014 [Project] Signatures for entire TypeScript programs using TypeDoc</li> <li> mkdocstrings/python \u2014 Class inheritance diagrams with Mermaid</li> <li> mkdocstrings/python \u2014 Annotations modernization</li> <li> mkdocstrings/griffe \u2014 Expressions modernization</li> <li> mkdocstrings/griffe \u2014 Check API of Python packages from PyPI</li> <li> mkdocstrings/python \u2014 Parameter headings</li> <li> mkdocstrings/python \u2014 Automatic cross-references to parameters</li> <li> mkdocstrings/python \u2014 Automatic cross-references for default parameter values in signatures</li> <li> mkdocstrings/griffe \u2014 Markdown output format for the <code>griffe check</code> command</li> <li> mkdocstrings/griffe \u2014 GitHub output format for the <code>griffe check</code> command</li> <li> pawamoy/devboard \u2014 [Project] A development dashboard for your projects</li> <li> pawamoy/insiders-project \u2014 [Project] Manage your Insiders projects</li> <li> mkdocstrings/griffe-tui \u2014 [Project] A textual user interface for Griffe</li> <li> mkdocstrings/python \u2014 Automatic rendering of function signature overloads</li> <li> mkdocstrings/shell \u2014 [Project] Shell scripts/libraries handler for mkdocstrings</li> <li> pawamoy/markdown-pycon \u2014 [Project] Markdown extension to parse <code>pycon</code> code blocks without indentation or fences</li> <li> mkdocstrings/python \u2014 Auto-summary of object members</li> <li> mkdocstrings/griffe-warnings-deprecated \u2014 [Project] Griffe extension for <code>@warnings.deprecated</code> (PEP 702)</li> <li> mkdocstrings/griffe-pydantic \u2014 [Project] Griffe extension for Pydantic</li> <li> pawamoy/pypi-insiders \u2014 [Project] Self-hosted PyPI server with automatic updates for Insiders versions of projects</li> </ul>"},{"location":"insiders/#mkdocstrings-projects","title":"mkdocstrings projects","text":"<p>mkdocstrings is a plugin for MkDocs, a static site generator written in Python. It brings autodoc capabilities to MkDocs, to enable automatic and configurable documentation of Python APIs.</p> <ul> <li> <p> <code>mkdocstrings/griffe</code> Signatures for entire Python programs. Generate API documentation or find breaking changes in your API.</p> </li> <li> <p> <code>mkdocstrings/griffe2md</code> Output API docs to Markdown using Griffe.</p> </li> <li> <p> <code>mkdocstrings/griffe-inherited-docstrings</code> Griffe extension for inheriting docstrings.</p> </li> <li> <p> <code>mkdocstrings/griffe-pydantic</code> Griffe extension for Pydantic.</p> </li> <li> <p> <code>mkdocstrings/griffe-tui</code> A textual user interface for Griffe.</p> </li> <li> <p> <code>mkdocstrings/griffe-typedoc</code> Signatures for entire TypeScript programs using TypeDoc.</p> </li> <li> <p> <code>mkdocstrings/griffe-warnings-deprecated</code> Griffe extension for <code>@warnings.deprecated</code> (PEP 702).</p> </li> <li> <p> <code>mkdocstrings/c</code> A C handler for mkdocstrings.</p> </li> <li> <p> <code>mkdocstrings/python</code> A Python handler for mkdocstrings.</p> </li> <li> <p> <code>mkdocstrings/shell</code> A shell scripts/libraries handler for mkdocstrings.</p> </li> <li> <p> <code>mkdocstrings/typescript</code> A TypeScript handler for mkdocstrings.</p> </li> </ul>"},{"location":"insiders/#other-toolslibraries","title":"Other tools/libraries","text":"<p>These tools are focused on documentation, developer-experience and productivity.</p> <ul> <li> <p> <code>pawamoy/devboard</code> A development dashboard for your projects.</p> </li> <li> <p> <code>pawamoy/insiders-project</code> Manage your Insiders projects.</p> </li> <li> <p> <code>pawamoy/markdown-exec</code> Utilities to execute code blocks in Markdown files.</p> </li> <li> <p> <code>pawamoy/markdown-pycon</code> Markdown extension to parse <code>pycon</code> code blocks without indentation.</p> </li> <li> <p> <code>pawamoy/mkdocs-manpage</code> MkDocs plugin to generate a manpage from the documentation site.</p> </li> <li> <p> <code>pawamoy/mkdocs-pygments</code> Highlighting themes for code blocks.</p> </li> <li> <p> <code>pawamoy/pypi-insiders</code> Self-hosted PyPI server with automatic updates for Insiders versions.</p> </li> </ul>"},{"location":"insiders/#how-to-become-a-sponsor","title":"How to become a sponsor","text":"<p>Thanks for your interest in sponsoring! In order to become an eligible sponsor with your GitHub account, visit pawamoy's sponsor profile, and complete a sponsorship of $10 a month or more. You can use your individual or organization GitHub account for sponsoring.</p> <p>Sponsorships lower than $10 a month are also very much appreciated! They won't grant you access to Insiders, but they will be counted towards reaching sponsorship goals. Every sponsor helps us implementing new features and releasing them to the public.</p> <p>Important: If you're sponsoring @pawamoy through a GitHub organization, please send a short email to insiders@pawamoy.fr with the name of your organization and the GitHub account of the individual that should be added as a collaborator.<sup>4</sup></p> <p>You can cancel your sponsorship anytime.<sup>5</sup></p> <p> \u00a0 Join our  awesome sponsors</p> <p>   If you sponsor publicly, you're automatically added here with a link to   your profile and avatar to show your support for my projects.   Alternatively, if you wish to keep your sponsorship private, you'll be a   silent +1. You can select visibility during checkout and change it   afterwards. </p>"},{"location":"insiders/#funding","title":"Funding","text":""},{"location":"insiders/#goals","title":"Goals","text":"<p>The following section lists all funding goals. Each goal contains a list of features prefixed with a checkmark symbol, denoting whether a feature is  already available or  planned, but not yet implemented. When the funding goal is hit, the features are released for general availability.</p> <p></p>"},{"location":"insiders/#1000-gravifridge-fluid-renewal","title":"$ 1,000 \u2014 GraviFridge Fluid Renewal","text":"<ul> <li> mkdocstrings/griffe \u2014 Markdown output format for the <code>griffe check</code> command</li> <li> mkdocstrings/griffe \u2014 GitHub output format for the <code>griffe check</code> command</li> <li> mkdocstrings/griffe-pydantic \u2014 [Project] Griffe extension for Pydantic</li> <li> mkdocstrings/griffe-tui \u2014 [Project] A textual user interface for Griffe</li> <li> mkdocstrings/griffe-warnings-deprecated \u2014 [Project] Griffe extension for <code>@warnings.deprecated</code> (PEP 702)</li> <li> mkdocstrings/python \u2014 Auto-summary of object members</li> <li> mkdocstrings/python \u2014 Automatic rendering of function signature overloads</li> <li> mkdocstrings/python \u2014 Parameter headings</li> <li> mkdocstrings/python \u2014 Automatic cross-references to parameters</li> <li> mkdocstrings/python \u2014 Automatic cross-references for default parameter values in signatures</li> <li> mkdocstrings/shell \u2014 [Project] Shell scripts/libraries handler for mkdocstrings</li> <li> pawamoy/insiders-project \u2014 [Project] Manage your Insiders projects</li> <li> pawamoy/markdown-pycon \u2014 [Project] Markdown extension to parse <code>pycon</code> code blocks without indentation or fences</li> <li> pawamoy/pypi-insiders \u2014 [Project] Self-hosted PyPI server with automatic updates for Insiders versions of projects</li> </ul>"},{"location":"insiders/#1500-hyperlamp-navigation-tips","title":"$ 1,500 \u2014 HyperLamp Navigation Tips","text":"<ul> <li> mkdocstrings/griffe \u2014 Check API of Python packages from PyPI</li> <li> mkdocstrings/griffe \u2014 Expressions modernization</li> <li> mkdocstrings/griffe-typedoc \u2014 [Project] Signatures for entire TypeScript programs using TypeDoc</li> <li> mkdocstrings/python \u2014 Class inheritance diagrams with Mermaid</li> <li> mkdocstrings/python \u2014 Annotations modernization</li> <li> pawamoy/devboard \u2014 [Project] A development dashboard for your projects</li> <li> pawamoy/markdown-exec \u2014 Custom icons in tree fences</li> <li> pawamoy/mkdocs-pygments \u2014 [Project] Highlighting themes for code blocks</li> </ul>"},{"location":"insiders/#2000-fusiondrive-ejection-configuration","title":"$ 2,000 \u2014 FusionDrive Ejection Configuration","text":"<ul> <li> mkdocstrings/c \u2014 [Project] A C handler for mkdocstrings</li> <li> mkdocstrings/python \u2014 Relative cross-references</li> <li> mkdocstrings/typescript \u2014 [Project] A TypeScript handler for mkdocstrings</li> </ul>"},{"location":"insiders/#goals-completed","title":"Goals completed","text":"<p>This section lists all funding goals that were previously completed, which means that those features were part of Insiders, but are now generally available and can be used by all users.</p> <p></p>"},{"location":"insiders/#500-plasmavac-user-guide","title":"$ 500 \u2014 PlasmaVac User Guide","text":"<ul> <li> mkdocstrings/griffe2md \u2014 [Project] Output API docs to Markdown using Griffe</li> <li> mkdocstrings/griffe-inherited-docstrings \u2014 [Project] Griffe extension for inheriting docstrings</li> <li> mkdocstrings/python \u2014 Cross-references for type annotations in signatures</li> <li> mkdocstrings/python \u2014 Symbol types in headings and table of contents</li> <li> pawamoy/markdown-exec \u2014 Pyodide fence</li> <li> pawamoy/mkdocs-manpage \u2014 [Project] MkDocs plugin to generate a manpage from the documentation site</li> </ul>"},{"location":"insiders/#frequently-asked-questions","title":"Frequently asked questions","text":""},{"location":"insiders/#payment","title":"Payment","text":"<p>We don't want to pay for sponsorship every month. Are there any other options?</p> <p>Yes. You can sponsor on a yearly basis by switching your GitHub account to a yearly billing cycle. If for some reason you cannot do that, you could also create a dedicated GitHub account with a yearly billing cycle, which you only use for sponsoring (some sponsors already do that).</p> <p>If you have any problems or further questions, please reach out to insiders@pawamoy.fr.</p>"},{"location":"insiders/#terms","title":"Terms","text":"<p>Are we allowed to use Insiders under the same terms and conditions as the public project?</p> <p>Yes. Whether you're an individual or a company, you may use the Insiders versions precisely under the same terms as the public versions, which are generally given by the ISC License. However, we kindly ask you to respect our fair use policy:</p> <ul> <li> <p>Please don't distribute the source code of Insiders. You may freely use it for public, private or commercial projects, privately fork or mirror it, but please don't make the source code public, as it would counteract the sponsorware strategy.</p> </li> <li> <p>If you cancel your subscription, you're automatically removed as a collaborator and will miss out on all future updates of Insiders. However, you may use the latest version that's available to you as long as you like. Just remember that GitHub deletes private forks.</p> </li> </ul> <ol> <li> <p>In general, every new feature is first exclusively released to sponsors, but sometimes upstream dependencies enhance existing features that must be supported.\u00a0\u21a9</p> </li> <li> <p>Note that $10 a month is the minimum amount to become eligible for Insiders. While GitHub Sponsors also allows to sponsor lower amounts or one-time amounts, those can't be granted access to Insiders due to technical reasons. Such contributions are still very much welcome as they help ensuring the project's sustainability.\u00a0\u21a9</p> </li> <li> <p>Making an Open Source project sustainable is exceptionally hard: maintainers burn out, projects are abandoned. That's not great and very unpredictable. The sponsorware model ensures that if you decide to use my Insiders projects, you can be sure that bugs are fixed quickly and new features are added regularly.\u00a0\u21a9</p> </li> <li> <p>It's currently not possible to grant access to each member of an organization, as GitHub only allows for adding users. Thus, after sponsoring, please send an email to insiders@pawamoy.fr, stating which account should become a collaborator of the Insiders repository. We're working on a solution which will make access to organizations much simpler. To ensure that access is not tied to a particular individual GitHub account, create a bot account (i.e. a GitHub account that is not tied to a specific individual), and use this account for the sponsoring. After being added to the list of collaborators, the bot account can create a private fork of the private Insiders GitHub repository, and grant access to all members of the organizations.\u00a0\u21a9</p> </li> <li> <p>If you cancel your sponsorship, GitHub schedules a cancellation request which will become effective at the end of the billing cycle. This means that even though you cancel your sponsorship, you will keep your access to Insiders as long as your cancellation isn't effective. All charges are processed by GitHub through Stripe. As we don't receive any information regarding your payment, and GitHub doesn't offer refunds, sponsorships are non-refundable.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/install_printer_on_linux/","title":"Install a printer on Linux","text":"<p>If <code>system-config-printer</code> does not work, you will need to do it via CUPS web interface. Go to http://localhost:631/admin, click on \"Add printer\". It will ask for a username and password. Try \"root\" and your \"root\" password. If is does not work, then edit the file \"/etc/cups/cupsd.conf\". Instead of \"Basic\", put \"None\" at \"DefaultAuthType\". Also remove every line containing \"@SYSTEM\". Restart CUPS with <code>sudo service cups restart</code>. Go again on the web interface, click on \"Add printer\".</p> <p>Download your printer's driver at www.support.xerox.com by choosing the Generic PPD. It's an executable file (.exe) that you can open with your archive manager on Linux. Extract the PPD file and use it in CUPS.</p> <p>Should be good now, except it's considered a generic printer and will mess up your with your fonts. Great.</p> <ul> <li>https://www.linuxquestions.org/questions/linux-software-2/cups-username-and-password-156986/</li> <li>http://forum.support.xerox.com/t5/Printing/CUPS-Driver-PPD-file-for-Xerox-3345-WorkCentre-printer-for-Linux/td-p/203672</li> <li>https://github.com/zdohnal/system-config-printer/issues/36</li> </ul>"},{"location":"posts/add-alembic-migrations-to-existing-fastapi-ormar-project/","title":"How to add Alembic migrations to an existing FastAPI + Ormar project","text":"<p>I had trouble adding an <code>ON DELETE CASCADE</code> constraint to foreign keys after they were already created, so here's a small post explaning how I did it. This specific constraint, <code>ON DELETE CASCADE</code>, is not relevant here. This post will help you for any kind of change you want to apply on database already used in production.</p> <p>First, I knew I was going to need \"migrations\", because my database was already used in production. I didn't previously add any migration. so I had to create the initial one, reflecting the current state of the database, and a second one to add the cascade delete constraints.</p> <p>As documented in Ormar's pages themselves, the tool to write migrations is Alembic. Alembic has an \"autogenerate\" feature that can compare the \"metadata\" of the database (an SQLAlchemy object in memory) against the actual database to generate a migration file.</p> <p>I'm using PDM to manage my dependencies, so I added Alembic to them with:</p> <pre><code>pdm add alembic\n</code></pre> <p>I did not add it to development dependencies because I can't SSH into the production machine to run a one-off command. It has to be a production dependency so I can run migrations (upgrade the state of the database) every time the app is deployed and started.</p> <p>Next, following the Alembic tutorial, I initialized it with:</p> <pre><code>pdm run alembic init alembic\n</code></pre> <p>I then had to modify a bit the <code>alembic.ini</code> and <code>alembic/env.py</code> files it created, to set the URL of the database and to import the right metadata object:</p> <pre><code># works locally only, see later how to deal with multiple environments\nsqlalchemy.url = sqlite:///db.sqlite\n</code></pre> <pre><code>from my_app.db import metadata\n\ntarget_metadata = metadata\n</code></pre> <p>I was now able to generate the initial migration with the following command, making sure I removed the <code>ondelete=\"CASCADE\"</code> arguments from my models' foreign keys:</p> <pre><code>pdm run alembic revision --autogenerate -m \"Initial migration.\"\n</code></pre> <p>The first issue I had was that Alembic was generating a migration that would drop tables instead of creating them. After some investigation, I realized that it was because the <code>metadata</code> object I imported was empty. With Ormar, the metadata object is populated only when loading the models definitions. So instead of importing it from my <code>db</code> module which only instantiated it, I imported it from my <code>models</code> module which uses it to declare my models classes:</p> <pre><code>from my_app.models import metadata\n\ntarget_metadata = metadata\n</code></pre> <p>I deleted the broken migration, and recreated a correct one:</p> <pre><code>pdm run alembic revision --autogenerate -m \"Initial migration.\"\n</code></pre> <p>To mark this migration as \"applied\", I ran the Alembic upgrade command:</p> <pre><code>pdm run alembic upgrade head\n</code></pre> <p>Now it was time to add the second migration, the one that adds the \"on delete cascade\" constraint. I added the <code>ondelete=\"CASCADE\"</code> arguments to my models' foreign keys, add created the second migration:</p> <pre><code>pdm run alembic revision --autogenerate -m 'Add `ondelete=\"CASCADE\"` to foreign keys.'\n</code></pre> <p>But then I got this error:</p> <pre><code>NotImplementedError: No support for ALTER of constraints in SQLite dialect.\nPlease refer to the batch mode feature which allows for SQLite migrations using a copy-and-move strategy.\n</code></pre> <p>Oh no. Thankfully, a quick search on the internet got me to this SO post and answer: you have to use <code>render_as_batch=True</code> to be able to alter constraint when using an SQLite database. In <code>alembic/env.py</code>:</p> <pre><code>context.configure(\n    connection=connection,\n    target_metadata=target_metadata,\n    render_as_batch=True,\n)\n\n# in both run_migrations_offline() and run_migrations_online()!\n</code></pre> <p>I ran the previous command again, and it worked. I applied the migration with <code>pdm run alembic upgrade head</code> again.</p> <p>At this point, I have migrations that I will be able to apply on the production database. But how do I do that?</p> <p>First, I need to support local and production environments. The way to do it is with an environment variable. The SQLite database URL will be stored in the <code>SQLITE_DB</code> environment variable. All I have to do now is to modify <code>alembic/env.py</code> to use this variable first, and fallback to the value configured in <code>alembic.ini</code>:</p> <pre><code>def get_url():\n    return os.getenv(\"SQLITE_DB\", config.get_main_option(\"sqlalchemy.url\"))\n\n    # in run_migrations_offline:\n    url = get_url()\n\n    # in run_migrations_online:\n    configuration = config.get_section(config.config_ini_section)\n    configuration[\"sqlalchemy.url\"] = get_url()\n    connectable = engine_from_config(\n        configuration,\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n</code></pre> <p>I was inspired by this FastAPI example.</p> <p>Now I need to set this variable for production, for example in an <code>.env</code> file. For me it was in a Helm chart:</p> <pre><code>...\nenv:\n- name: SQLITE_DB\n  value: \"sqlite:///different/path/to/db.sqlite\"\n</code></pre> <p>Finally, I have to call the <code>alembic upgrade head</code> command each time I start the application. This can be done in FastAPI's \"startup\" event:</p> <pre><code>import os\nfrom pathlib import Path\n\nimport sqlalchemy\nfrom alembic.config import Config\nfrom alembic.commad import upgrade\nfrom fastapi import FastAPI\n\nfrom my_app.db import metadata, database\n\napp = FastAPI()\napp.state.database = database\n\n\n@app.on_event(\"startup\")\nasync def startup() -&gt; None:\n    \"\"\"Startup application.\"\"\"\n    db_url = os.getenv(SQLITE_DB)\n    db_path = db_url.replace(\"sqlite://\", \"\")\n    if not Path(db_path).exists():\n        engine = sqlalchemy.create_engine(db_url)\n        metadata.drop_all(engine)\n        metadata.create_all(engine)\n\n    upgrade(Config(), \"head\")\n\n    database = app.state.database\n    if not database.is_connected:\n        await database.connect()\n</code></pre> <p>In my next post, you will see how to write tests for such setups, but also how to configure all this in a more robust and elegant way.</p>"},{"location":"posts/adding-links-to-formatted-and-syntax-highlighted-code/","title":"Adding links to a Black formatted, Pygments syntax highlighted Python code block","text":"<p>One of the features of mkdocstrings-python is its ability to automatically render cross-references to objects of the documented package in code block signatures.</p> <p>Adding links to code blocks is possible in HTML, but doing so while preserving Pygments syntax highlighting and Black formatting was not an easy task. This post explains how it works.</p> <p>Let say we document a Python package that exposes one function and one class: <code>do</code> and <code>Action</code>. The function takes an instance of the class as parameter:</p> <pre><code>class Action:\n    \"\"\"An action.\"\"\"\n\n\ndef do(action: Action):\n    \"\"\"Do an action.\"\"\"\n</code></pre> <p>Let say we now want to document both using MkDocs and mkdocstrings. In our Markdown page we would add the autodoc instructions:</p> <pre><code>::: package.Action\n\n::: package.do\n</code></pre> <p>...which, depending on how we configured mkdocstrings, would render something like this:</p> <p>The challenge is to render the signature of <code>do</code> like this instead, with <code>Action</code> linking to the rendered docs for the corresponding class:</p> <p>So how do we do this?</p> <p>Obviously we can use the <code>signature_crossrefs</code> option of mkdocstrings-python (which at the time of writing is available to my sponsors only), but that's not the point of this post.</p> <p>Let's take a look at the first signature. We can inspect the HTML generated for the signature of the <code>do</code> function, it looks like this (with added line breaks for readability):</p> <pre><code>&lt;pre&gt;\n  &lt;code&gt;\n    &lt;span class=\"n\"&gt;do&lt;/span&gt;\n    &lt;span class=\"p\"&gt;(&lt;/span&gt;\n    &lt;span class=\"n\"&gt;action&lt;/span&gt;\n    &lt;span class=\"p\"&gt;:&lt;/span&gt; &lt;span class=\"n\"&gt;Action&lt;/span&gt;\n    &lt;span class=\"p\"&gt;)&lt;/span&gt;\n  &lt;/code&gt;\n&lt;/pre&gt;\n</code></pre> <p>Can we parse this, find spans with class <code>n</code>, and add links there? We could, but there are a few issues with this approach:</p> <ul> <li>we don't know which names (<code>n</code>) should be linked</li> <li>we don't have the names' full path (<code>Action</code>'s path really is <code>actions.Action</code> here)</li> <li>parsing and modifying HTML is costly</li> </ul> <p>We can't either add links first, then syntax highlight or format the code: neither Pygments nor Black support coloring/formatting code while retaining HTML tags around names.</p> <p>The only way to do this correctly and efficiently is by adding pre-processing and post-processing steps.</p> <ol> <li>First, we need the full path of names that must be linked. In mkdocstrings, thanks to Griffe, expressions (like signatures) are stored as a list-like structure, on which we can iterate. Elements of this list-like structure are strings or instances of a special class that represent linkable names. Thanks to these, we know what to link, and we have the names' full path.</li> <li>When preparing our string for formatting/syntax highlighting, we simply join string elements together. For each name, we precompute its HTML link, store it in a map with a unique identifer, and join this identifier to the main string. These identifiers must answer to a few constraints: they must be unique (obviously), they must have the same length as the original name (for formatting reasons), and they must be valid Python variable names (for formatting reasons too).</li> <li>We format and syntax highlight this main string.</li> <li>We finally replace each unique identifier stored in our map by the precomputed HTML link. Tada!</li> </ol> <p>Let's illustrate this process with our example setup above.</p> <ol> <li> <p>The signature of the function <code>do</code> could be stored like this:</p> <pre><code>expression = [\n    \"do\",\n    \"(\",\n    \"action\",\n    \": \",\n    Name(source=\"Action\", full_path=\"actions.Action\"),\n    \")\",\n]\n</code></pre> </li> <li> <p>We build our main string, while computing and storing links for names:</p> <pre><code>def build_string(expression, stored_links):\n    string_parts = []\n    for element in expression:\n        if isinstance(element, str):\n            string_parts.append(element)\n        else:\n            unique_id = get_unique_id(name.source)  # _dx7ej\n            stored_links[unique_id] = get_url_to(\n                name.full_path\n            )  # &lt;a href=\"#actions.Action\"&gt;Action&lt;/a&gt;\n            string_parts.append(unique_id)\n    return \"\".join(string_parts)\n\n\nstored_links = {}\nmain_string = build_string(expression, stored_links)\n</code></pre> </li> <li> <p>We format and highlight the string using Black and Pygments, in that order:</p> <pre><code>formatted_string = black_format(main_string)\nhighlighted_string = pygments_highlight(formatted_string)\n</code></pre> </li> <li> <p>We replace the unique ids by their values:</p> <pre><code>final_string = highlighted_string\nfor unique_id, link in stored_links.items():\n    final_string = final_string.replace(unique_id, link)\n</code></pre> </li> </ol> <p>Finally, let's illustrate the different states of our signature:</p> <ol> <li> <pre><code>[\"do\", \"(\", \"action\", \": \", Name(source=\"Action\", full_path=\"actions.Action\"), \")\"]\n</code></pre> </li> <li> <pre><code>\"do(action: _dx7ej)\"\n</code></pre> </li> <li> <pre><code>&lt;pre&gt;\n  &lt;code&gt;\n    &lt;span class=\"n\"&gt;do&lt;/span&gt;\n    &lt;span class=\"p\"&gt;(&lt;/span&gt;\n    &lt;span class=\"n\"&gt;action&lt;/span&gt;\n    &lt;span class=\"p\"&gt;:&lt;/span&gt; &lt;span class=\"n\"&gt;_dx7ej&lt;/span&gt;\n    &lt;span class=\"p\"&gt;)&lt;/span&gt;\n  &lt;/code&gt;\n&lt;/pre&gt;\n</code></pre> </li> <li> <pre><code>&lt;pre&gt;\n  &lt;code&gt;\n    &lt;span class=\"n\"&gt;do&lt;/span&gt;\n    &lt;span class=\"p\"&gt;(&lt;/span&gt;\n    &lt;span class=\"n\"&gt;action&lt;/span&gt;\n    &lt;span class=\"p\"&gt;:&lt;/span&gt; &lt;span class=\"n\"&gt;&lt;a href=\"#actions.Action\"&gt;Action&lt;/a&gt;&lt;/span&gt;\n    &lt;span class=\"p\"&gt;)&lt;/span&gt;\n  &lt;/code&gt;\n&lt;/pre&gt;\n</code></pre> </li> </ol> <p>Ensuring the ids are unique within the final highlighted string is not easy, since we don't know the contents of the highlighted string in advance. In mkdocstrings-python we use ASCII digits as well as letters, and prefix the ids with an underscore, to reduce the risk of collision with other existing variables in the expression. The length constraint increases the collision risk when variables have short names. We always create ids of minimum length 3, even for variable names of length 1 or 2.</p> <p>The whole process is even more involved in mkdocstrings-python: we have an additional intermediate step with autorefs, and we have to integrate the solution within Jinja templates!</p>"},{"location":"posts/adding-links-to-formatted-and-syntax-highlighted-code/#actions.Action","title":"Action","text":"<p>An action.</p>"},{"location":"posts/adding-links-to-formatted-and-syntax-highlighted-code/#actions.do","title":"do","text":"<pre><code>do(action: Action)\n</code></pre> <p>Do an action.</p>"},{"location":"posts/adding-links-to-formatted-and-syntax-highlighted-code/#actions.do","title":"do","text":"<pre><code>do(action: Action)\n</code></pre> <p>Do an action.</p>"},{"location":"posts/challenge-fill-space-one-line-90-degree-same-direction/","title":"Challenge: fill a 2D space with one continuous line doing 90\u00b0 angle turns in the same direction (left/right)","text":"<p>Last night I was doing that in my head: filling a 2D space with one continuous line doing turns at a 90\u00b0 angle, always in the same direction. Other people do this as well, right?</p> <p>Well, today I actually drawn it:</p> <p></p> <p>I started top left, and finish bottom right. I turned right all the time. It took me about 20 minutes.</p> <p>It was not so easy!</p> <p>The difficult parts are:</p> <ul> <li>keeping the line straightly vertical or horizontal</li> <li>not getting stuck (because we don't want to make the line touch)</li> <li>not repeating the same patterns too much (though repetition of symmetrical patterns could be very pretty!)</li> </ul> <p>Here are some mistakes I made (the most obvious ones): </p> <p>But anyway, it was my first attempt, and I kinda like the childish aspect of it. 10/10 would do again!</p> <p>Show me your attempts in the comments !</p> <p>Note: I used a cotton canvas which cost me 1,50\u20ac, but you can just use a sheet of paper if you don't plan to show how good you are to everybody by hanging it on a wall.</p>"},{"location":"posts/django-auth-server-for-shiny/","title":"Django application as an authentication / authorization server for Shiny","text":"<p>As you may know, Shiny Server comes in two versions: open-source and professional. The professional adds security and authentication features like password protected applications, and controlled access via SSL and LDAP, Active Directory, Google OAuth, PAM, proxied authentication, or passwords. If you need these authentication features but don't want or can't spend $9,995 per year for the professional edition, then I got a solution for you!</p> <p>In this post, I will show how to wrap one or several Shiny applications into a Django application to add authentication and access control to your Shiny apps. The shining star here will not be Django, as you could replace it by any other web application you want, but the famous NginX reverse proxy, accompanied by its devoted auth-request module.</p> <p>The code used in this post is also available as a repository on GitHub. It contains a Docker configuration so you can try it easily.</p> <p>On the menu</p> <ul> <li>Overview</li> <li>Wrapping a Shiny app into a Django-powered page</li> <li>Proxying Shiny requests to the Shiny app</li> <li>Adding an authentication step for every Shiny request</li> <li>Try it with a Dockerized project</li> </ul> <p>What most of you will be interested in is starting at section 2: Proxying, though Wrapping is interesting as well if you want to create an interface to access multiple Shiny apps.</p>"},{"location":"posts/django-auth-server-for-shiny/#overview","title":"Overview","text":"<p>Let's look at some pictures to see what we want to accomplish. The first picture shows our client-server architecture. The client can communicate with the server on the port 80 or 443 (HTTP or HTTPs), but not on the ports 8000 or 8100, which are used internally by Django and Shiny. This can be configured through the firewall.</p> <p></p> <p>The second picture shows what happens when the client requests the URL that the Shiny app is served on. As we wrap the Shiny app into a Django-powered page, the request is proxied directly to Django by NginX. Django then gets the initial Shiny page HTML contents with an internal HTTP Get request, and renders it in a <code>iframe</code> (we will see the details later). It then returns this rendered page to NginX, which returns it to the client.</p> <p></p> <p>The third picture shows each subsequent requests from the client to the server through a WebSocket, and how NginX is asking authorization to Django. When NginX receives the request, it sends a sub-request to Django, asking for permission to proxy the request to Shiny and return the response to the client. If Django says yes (HTTP 200), NginX proxies the request to Shiny. If Django says no (HTTP 403 or any other error code), NginX rejects the request by returning HTTP 403 as a response to the client.</p> <p></p> <p>OK, let's try it! To begin, create a directory that we will use for this tutorial:</p> <pre><code>mkdir django-shiny\ncd django-shiny\n</code></pre>"},{"location":"posts/django-auth-server-for-shiny/#wrapping-a-shiny-app-into-a-django-powered-page","title":"Wrapping a Shiny app into a Django-powered page","text":"<p>This first section will help you setup an example project to follow this tutorial, but the first two steps described are optional. You can immediately jump to the Proxying Shiny requests section where we will a use pre-setup example project using Docker. You will need to install Docker if you don't already have it. See Install Docker for installation instructions.</p> <p>The third step however might be interesting to read if you need to wrap your Shiny app into a Django-powered page and website, instead of just using Django as an external authentication / authorization backend.</p> <ol> <li>The Shiny app</li> <li>The Django app</li> <li>Injecting the HTML contents in an Iframe</li> </ol>"},{"location":"posts/django-auth-server-for-shiny/#the-shiny-app","title":"The Shiny app","text":"<p>Let's get a Shiny app example from RStudio's gallery. The code is available on GitHub in this repository.</p> <p>Clone it in a sub-directory called <code>shinyapp</code>:</p> <pre><code>git clone --depth=1 https://github.com/rstudio/shiny-examples\nmv shiny-examples/001-hello shinyapp\nrm -rf shiny-examples\n</code></pre> <p>We also need to install the Shiny R package. If you don't already have R installed, you can install a recent version with the following commands:</p> <pre><code>sudo add-apt-repository \"deb http://cran.rstudio.com/bin/linux/ubuntu trusty/\"\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9\nsudo add-apt-repository ppa:marutter/rdev\nsudo apt-get update\nsudo apt-get install -y r-base\n</code></pre> <p>Run this command to install Shiny:</p> <pre><code>sudo R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\"\n</code></pre> <p>To run the Shiny application on port 8100, use the following command:</p> <pre><code>sudo R -e \"shiny::runApp(appDir='shinyapp', port=8100)\"\n</code></pre> <p>Try to go to <code>http://localhost:8100</code> to see if the app is running.</p>"},{"location":"posts/django-auth-server-for-shiny/#the-django-app","title":"The Django app","text":"<p>We will create a new Django project called <code>djangoapp</code>. If you don't already have Django installed on your system, install it in a virtualenv with <code>pip install Django</code>, or system-wide with <code>sudo pip install Django</code>.</p> <p>To create the project, run the following command:</p> <pre><code>django-admin startproject djangoapp\n</code></pre> <p>We need to initialize the SQLite database first.</p> <pre><code>python djangoapp/manage.py migrate\n</code></pre> <p>You can now run the Django application on port 8000 with the following command:</p> <pre><code>python djangoapp/manage.py runserver localhost:8000\n</code></pre> <p>And try to go to <code>http://localhost:8000</code> to see if the app is running.</p>"},{"location":"posts/django-auth-server-for-shiny/#injecting-the-html-contents-in-an-iframe","title":"Injecting the HTML contents in an Iframe","text":"<p>At this point you should have the following tree:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 djangoapp/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 db.sqlite3\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 djangoapp/\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 settings.py\n\u2502   \u2502   \u251c\u2500\u2500 \ud83d\udcc4 urls.py\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 wsgi.py\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 manage.py\n\u2514\u2500\u2500 \ud83d\udcc1 shinyapp/\n    \u251c\u2500\u2500 \ud83d\udcc4 app.R\n    \u251c\u2500\u2500 \ud83d\udcc4 DESCRIPTION\n    \u2514\u2500\u2500 \ud83d\udcc4 Readme.md\n</code></pre> <p>We will proceed in three main steps:</p> <ol> <li>Create a simple view that renders our wrapping HTML page</li> <li>Create this wrapping HTML page that will:<ol> <li>Create an Iframe and add it to the DOM</li> <li>Get the Shiny HTML contents in JSON format, with an asynchronous call</li> <li>Inject these contents in the Iframe</li> </ol> </li> <li>Create the view to return Shiny HTML contents as JSON by:<ol> <li>Getting the contents with a GET request</li> <li>Parsing it with BeautifulSoup's HTML parser</li> <li>Dumping and returning it as JSON</li> </ol> </li> </ol>"},{"location":"posts/django-auth-server-for-shiny/#create-the-wrapping-view","title":"Create the wrapping view","text":"<p>Let's create our wrapping view. First make sure you have listed <code>djangoapp</code> in the Django settings' <code>INSTALLED_APPS</code>:</p> <pre><code># settings.py\n\nINSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"djangoapp\",\n]\n</code></pre> <p>Then we can add the URL in <code>urls.py</code>:</p> <pre><code>from django.contrib import admin\nfrom django.urls import path\n\nfrom . import views\n\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n    path(\"shiny/\", views.shiny, name=\"shiny\"),\n]\n</code></pre> <p>And now we create the view in a new <code>views.py</code> file:</p> <pre><code>touch djangoapp/djangoapp/views.py\n</code></pre> <pre><code>from django.shortcuts import render, redirect\n\n\ndef shiny(request):\n    return render(request, \"djangoapp/shiny.html\")\n</code></pre>"},{"location":"posts/django-auth-server-for-shiny/#create-the-wrapping-html-page","title":"Create the wrapping HTML page","text":"<p>Since we tell the view to render the <code>djangoapp/shiny.html</code> template, we need to create it:</p> <pre><code>mkdir -p djangoapp/djangoapp/templates/djangoapp\ntouch djangoapp/djangoapp/templates/djangoapp/shiny.html\n# too much of djangoapp already, I know\n</code></pre> <p>And write its contents. We simply add a title, to know we are in the wrapping view, and then we add a script to retrieve the Shiny app contents:</p> <pre><code>&lt;h1&gt;We are in the wrapping page!&lt;/h1&gt;\n\n&lt;div id=\"contents\"&gt;&lt;/div&gt;\n\n&lt;script src=\"https://code.jquery.com/jquery-3.3.1.min.js\"&gt;&lt;/script&gt;\n\n&lt;script&gt;\n  $(document).ready(function () {\n\n    $.getJSON('{% url \"shiny_contents\" %}', function (data) {\n\n      var iframe = document.createElement(\"iframe\");\n      $('#contents').append(iframe);\n\n      iframe.contentWindow.document.open();\n      iframe.contentWindow.document.write(data.html_contents);\n      iframe.contentWindow.document.close();\n\n      // Attempt circumvention\n      if (iframe.contentWindow.WebSocket)\n          WebSocket = iframe.contentWindow.WebSocket;\n    });\n\n  });\n\n&lt;/script&gt;\n</code></pre> <p>Several things happen here:</p> <ul> <li>we declare a <code>div</code> of ID <code>contents</code> in which we will add an <code>iframe</code>,</li> <li>we make use of JQuery's <code>$(document).ready</code> and <code>$.getJSON</code> methods to load HTML contents from an URL returning JSON,</li> <li>we create the <code>iframe</code>, add it in the document, then write the HTML contents inside of it. We also reassign the <code>WebSocket</code> variable to the value of the <code>iframe</code> one.</li> </ul>"},{"location":"posts/django-auth-server-for-shiny/#create-the-json-view","title":"Create the JSON view","text":"<p>As you maybe guessed, work is not finished. We need to add the <code>shiny_contents</code> URL and view in the Django app. The view must return the contents of the Shiny app initial page as JSON.</p> <p>Add the URL in <code>urls.py</code>:</p> <pre><code>from django.contrib import admin\nfrom django.urls import path\n\nfrom . import views\n\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n    path(\"shiny/\", views.shiny, name=\"shiny\"),\n    path(\"shiny_contents/\", views.shiny_contents, name=\"shiny_contents\"),\n]\n</code></pre> <p>Add the view to <code>views.py</code>:</p> <pre><code>from django.http import JsonResponse\nfrom django.shortcuts import render\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\n\ndef shiny(request):\n    return render(request, \"djangoapp/shiny.html\")\n\n\ndef shiny_contents(request):\n    response = requests.get(\"http://localhost:8100\")\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    return JsonResponse({\"html_contents\": str(soup)})\n</code></pre> <p>We are using requests and BeautifulSoup to get the HTML contents and return it as text, dumped as JSON. If you know a better way, let me know in the comments!</p> <p>To install requests and BeautifulSoup: <code>pip install requests beautifulsoup4</code>.</p> <p>OK, let's try! Run both the Shiny application and the Django application, then go to http://localhost:8000/shiny to see how it goes!</p> <p>You should see something like the following image:</p> <p></p> <p>But since Shiny requests are not proxied, they are simply lost in-between, and your Shiny app will not respond, nor get a nice look because the static assets will not be loaded. We will fix this in the next section: Proxying Shiny requests.</p> <p>This is it for the setup and the Django-wrapped Shiny page. For the rest of the tutorial, I will explain how to configure NginX and Django to act as proxy and authorization servers. The result is available for you to try in this repository: https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example. See the Try it out section.</p>"},{"location":"posts/django-auth-server-for-shiny/#proxying-shiny-requests-to-the-shiny-app","title":"Proxying Shiny requests to the Shiny app","text":"<p>This is time for NginX to come on stage. We need to create our new, project-specific configuration file in <code>/etc/nginx/sites-available/djangoshiny</code>.</p> <p>We will tell NginX to proxy every request with an URL like <code>shiny/*</code> to our Shiny app. All other requests will be proxied to the Django app.</p> <pre><code># declare your Django app\nupstream djangoapp_server {\n    server localhost:8000;\n}\n\n# declare your Shiny app\nupstream shinyapp_server {\n    server localhost:8100;\n}\n\n# required for WebSockets\nmap $http_upgrade $connection_upgrade {\n    default upgrade;\n    '' close;\n}\n\nserver {\n\n    listen 80;\n    server_name localhost;\n\n    client_max_body_size 100M;\n\n    # normal requests go to Django\n    location / {\n        proxy_pass http://djangoapp_server;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n        if (!-f $request_filename) {\n            proxy_pass http://djangoapp_server;\n            break;\n        }\n    }\n\n    # \"shiny\" requests go to Shiny\n    location ~ /shiny/.+ {\n\n        rewrite ^/shiny/(.*)$ /$1 break;\n\n        proxy_pass http://shinyapp_server;\n        proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/;\n\n        # required for WebSockets\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n\n        proxy_read_timeout 20d;\n        proxy_buffering off;\n    }\n\n}\n</code></pre> <p>Once you're ready, enable this configuration by linking it like this:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/djangoshiny /etc/nginx/sites-enabled/djangoshiny\n</code></pre> <p>Now reload NginX, and that's it! If you launch both Django on port 8000, and Shiny on port 8100, you should be able to connect to <code>http://localhost/shiny</code> and enjoy your Django-wrapped Shiny application functioning properly!</p> <p>There is still one thing we need to add: authentication and authorization. I won't show how to create sign-in / sign-up / sign-out views: you can find all the documentation to enable Django authentication system online. What I will show you is how to make Django act as an authorization server for Shiny.</p>"},{"location":"posts/django-auth-server-for-shiny/#adding-an-authentication-step-for-every-shiny-request","title":"Adding an authentication step for every Shiny request","text":"<p>We want all requests proxied to Shiny to be authorized by Django. For this we are gonna use the auth-request module. This module is not compiled in NginX by default on distributions like Ubuntu / Debian. If you want to recompile NginX with auth-request enabled, check how I do it in this Dockerfile.</p> <p>An easier solution is to use the Docker setup from the same repository, with the official NginX image which already supports auth-request.</p> <p>So, once your NginX is ready, add this authorization step in the configuration file:</p> <pre><code>    location ~ /shiny/.+ {\n\n        # we tell nginx to call that location for each request\n        auth_request /auth;\n\n        rewrite ^/shiny/(.*)$ /$1 break;\n\n        proxy_pass http://shinyapp_server;\n        proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/;\n\n        # this part is needed for WebSockets to work\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection $connection_upgrade;\n\n        proxy_read_timeout 20d;\n        proxy_buffering off;\n    }\n\n    # the /auth location will send a subrequest to django, URL shiny_auth/\n    location = /auth {\n        internal;\n\n        proxy_pass http://localhost:8000/shiny_auth/;\n\n        proxy_pass_request_body off;\n        proxy_set_header Content-Length \"\";\n        proxy_set_header X-Original-URI $request_uri;\n    }\n</code></pre> <p>Of course, we also need to write the view called by the <code>shiny_auth/</code> URL. It's a very simple one. First add the URL in <code>urls.py</code>:</p> <pre><code>from django.contrib import admin\nfrom django.urls import path\n\nfrom . import views\n\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n    path(\"shiny/\", views.shiny, name=\"shiny\"),\n    path(\"shiny_contents/\", views.shiny_contents, name=\"shiny_contents\"),\n    path(\"shiny_auth/\", views.shiny_auth),\n]\n</code></pre> <p>And then the view in <code>views.py</code>:</p> <pre><code>from django.http import HttpResponse\n\n\ndef shiny_auth(request):\n    if request.user.is_authenticated:\n        return HttpResponse(status=200)\n    return HttpResponse(status=403)\n</code></pre> <p>Et voil\u00e0! If the user is authenticated, Django will say \"OK\". If the user is not authenticated, Django will say \"No\". But you could implement any logic you need instead of just checking if the user is authenticated or not. You could have several Shiny apps served on different URLs, powered by Django (in wrapping pages, as we saw earlier), and grant access to users with a permission system, etc..</p>"},{"location":"posts/django-auth-server-for-shiny/#try-it-with-a-dockerized-project","title":"Try it with a Dockerized project","text":"<p>Clone the repository and build the images with the following commands:</p> <pre><code>git clone https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example docker-django-shiny\ncd docker-django-shiny\nsudo make all\n</code></pre> <p>This first build should take quite some time. It will build Shiny and Django images, get static assets, create the database, create a super user, and then run the application. You will then be able to go to <code>http://localhost:8000</code> to see it in action.</p> <p>To print the available <code>make</code> commands, simply run <code>make</code>.</p>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/","title":"Django admin dashboard with Suit and Highcharts","text":"<p>One day my boss said \"I want to see some statistical data about the users\". I immediately thought about adding a custom page in the admin panel instead of creating a new view with admin restriction. I was already using django-suit which is a great theme (and more) for the Django admin panel, so I searched for a way to add a custom view within Suit. Looking at the issue from brunocascio on the repo, I found the comment written by rouxxx and linking to this post about django dashboard. I had then all I needed to start a proof of concept.</p> <p>Contents</p> <ul> <li>Step 1: set up the files</li> <li>Step 2: activate your brand new dashboard</li> <li>Step 3: Highcharts!</li> <li>More customization</li> <li>The app I wrote using this</li> </ul>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-1-set-up-the-files","title":"Step 1: set up the files","text":"<p>First, create a new app within your Django project:</p> <pre><code>./manage.py startapp dashboard\n</code></pre> <p>Then create the additional <code>sites.py</code> module in the new <code>dashboard</code> app.</p> <pre><code># -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\nfrom django.contrib.admin.sites import AdminSite\nfrom django.conf.urls import url\nfrom dashboard.views import DashboardMainView\n\n\nclass AdminMixin(object):\n    \"\"\"Mixin for AdminSite to allow custom dashboard views.\"\"\"\n\n    def get_urls(self):\n        \"\"\"Add dashboard view to admin urlconf.\"\"\"\n        urls = super(AdminMixin, self).get_urls()\n        del urls[0]\n        custom_url = [\n            url(r\"^$\", self.admin_view(DashboardMainView.as_view()), name=\"index\")\n        ]\n\n        return custom_url + urls\n\n\nclass DashboardSite(AdminMixin, AdminSite):\n    \"\"\"\n    A Django AdminSite with the AdminMixin to allow registering custom\n    dashboard view.\n    \"\"\"\n</code></pre> <p>Next, update the contents of <code>views.py</code>:</p> <pre><code># -*- coding: utf-8 -*-\n\nfrom __future__ import unicode_literals\nfrom django.views.generic import TemplateView\n\n\nclass DashboardMainView(TemplateView):\n    template_name = \"dashboard/main.html\"\n\n    def get(self, request, *args, **kwargs):\n        context = self.get_context_data(**kwargs)\n        return self.render_to_response(context=context)\n</code></pre> <p>And finally, create the template <code>main.html</code> (thanks brunocascio):</p> <pre><code>{% extends \"admin/base_site.html\" %}\n{% load i18n admin_static %}\n\n{% block title %}\n  My title\n{% endblock %}\n\n{% block breadcrumbs %}{% endblock %}\n{#{% block sidebar %}{% endblock %}#}\n{#{% block header_time %}{% endblock %}#}\n\n{% block content %}\n  My content\n{% endblock %}\n</code></pre>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-2-activate-your-brand-new-dashboard","title":"Step 2: activate your brand new dashboard","text":"<p>Add your <code>dashboard</code> app in your <code>settings.INSTALLED_APPS</code>, but also replace <code>django.contrib.admin</code> by <code>django.contrib.admin.apps.SimpleAdminConfig</code> (thanks rouxxx):</p> <pre><code>INSTALLED_APPS = (\n    \"suit\",  # Beautiful admin interface\n    # Replace 'django.contrib.admin' for dashboard\n    # 'django.contrib.admin',\n    \"django.contrib.admin.apps.SimpleAdminConfig\",\n    # ...\n    \"dashboard\",\n    # ...\n)\n</code></pre> <p>In your main <code>urls.py</code>, you can now use your custom AdminSite:</p> <pre><code>from dashboard.sites import DashboardSite\n\nadmin.site = DashboardSite()\nadmin.autodiscover()\n</code></pre> <p>EDIT: with Django 1.9.5, you will need one more line:\\ (See Gustavo's comment in the comments section.)</p> <pre><code>admin.site = DashboardSite()\nadmin.sites.site = admin.site\nadmin.autodiscover()\n</code></pre> <p>Your <code>urlpatterns</code> should not be subject to change:</p> <pre><code>urlpatterns = [\n    url(r\"^admin/\", include(admin.site.urls)),\n    # ...\n]\n</code></pre> <p>That's it, go check your new empty dashboard!</p> <p></p>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-3-highcharts","title":"Step 3: Highcharts!","text":"<p>I was already familiar with Highcharts since I use it in my project, so it made sense to use it in the admin dashboard too.</p> <p>Somewhere in your project, you will have functions that compute or retrieve data from your database(s) or else. Add these data in the <code>context</code> dictionary of the <code>DashboardMainView</code> in <code>dashboard/views.py</code>.</p> <p>Example:</p> <pre><code>from core.data import get_country_name_by_code\nfrom statistics.sitewide import nb_patients_per_country\n\n\nclass DashboardMainView(TemplateView):\n    template_name = \"dashboard/main.html\"\n\n    def get(self, request, *args, **kwargs):\n        context = self.get_context_data(**kwargs)\n        context[\"nb_patients_per_country\"] = sorted(\n            [\n                {\"country\": get_country_name_by_code(k), \"patients\": v}\n                for k, v in nb_patients_per_country().items()\n                if v &gt; 0\n            ],\n            key=lambda x: x[\"patients\"],\n            reverse=True,\n        )\n\n        # I filtered the 0 values out, and used get_country_name_by_code to\n        # have the countries names translated!\n\n        return self.render_to_response(context=context)\n</code></pre> <p>Then in your template, add links to Highchart, and use the previous data to create, for example, a pie chart:</p> <pre><code>{% block content %}\n  &lt;script src=\"http://code.highcharts.com/highcharts.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"http://code.highcharts.com/highcharts-more.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"http://code.highcharts.com/modules/exporting.js\"&gt;&lt;/script&gt;\n\n  &lt;div id=\"highchart-0\"&gt;&lt;/div&gt;\n\n  &lt;script type='text/javascript'&gt;\n    var dataset;\n\n    dataset = {\n      \"chart\": {\n        \"type\": \"pie\",\n        \"plotBackgroundColor\": null,\n        \"plotBorderWidth\": null,\n        \"plotShadow\": false\n      },\n\n      \"title\": {\n        \"text\": \"{% trans \"Number of patients per country\" %}\"\n      },\n\n      \"series\": [{\n        \"name\": \"Countries\",\n        \"colorByPoint\": true,\n        \"data\": [\n          {% for tuple in nb_patients_per_country %}\n            { \"name\": \"{{ tuple.country }}\", \"y\": {{ tuple.patients }} },\n          {% endfor %}\n        ]\n      }],\n\n      \"tooltip\": {\n        \"formatter\": function() {\n          return this.y + '/' + this.total + ' (' + Highcharts.numberFormat(this.percentage, 1) + '%)';\n        }\n      },\n\n      \"plotOptions\": {\n        \"pie\": {\n          \"showInLegend\": true,\n          \"allowPointSelect\": true,\n          \"cursor\": \"pointer\",\n          \"dataLabels\": {\n            \"enabled\": true,\n            \"format\": \"&lt;b&gt;{point.name}&lt;/b&gt;: {point.percentage:.1f} %\"\n            \"style\": {\n              \"color\": function() {\n                return (Highcharts.theme &amp;&amp; Highcharts.theme.contrastTextColor) || 'black'\n              }\n            }\n          }\n        }\n      }\n    };\n\n    $('#highchart-0').highcharts(dataset);\n  &lt;/script&gt;\n{% endblock %}\n</code></pre> <p>Here is the result:</p> <p></p> <p>For other types of chart, go check the Highcharts demos and their excellent API documentation!</p>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#more-customization","title":"More customization","text":""},{"location":"posts/django-dashboard-with-suit-and-highcharts/#customizable-contents","title":"Customizable contents","text":"<ul> <li>Work with columns</li> <li>Add more pages to your dashboard</li> </ul>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#columns","title":"Columns","text":"<p>If like me you would like to use columns in the dashboard, you will be disappointed since we cannot use Bootstrap's column classes (<code>col-md</code> and stuff) for I don't know what reason. Instead, Suit provides the <code>suit-column</code> class which lets you stack multiple columns on one row. But you can't specify the width and they won't have the same size. Here is a CSS starting point to poorly imitate Bootstrap's columns:</p> <pre><code>.suit-row-1-col .suit-column {\n    width: 100%;\n}\n\n.suit-row-2-col .suit-column {\n    width: 50%;\n}\n\n.suit-row-3-col .suit-column {\n    width: 33.33%;\n}\n\n.suit-row-4-col .suit-column {\n    width: 25%;\n}\n\n.suit-row-5-col .suit-column {\n    width: 20%;\n}\n\n.suit-row-6-col .suit-column {\n    width: 16.66%;\n}\n\n.suit-row-7-col .suit-column {\n    width: 14.28%;\n}\n\n.suit-row-8-col .suit-column {\n    width: 12.5%;\n}\n\n.suit-row-9-col .suit-column {\n    width: 11.11%;\n}\n\n.suit-row-10-col .suit-column {\n    width: 10%;\n}\n\n.suit-row-11-col .suit-column {\n    width: 9.09%;\n}\n\n.suit-row-12-col .suit-column {\n    width: 8.333%;\n}\n</code></pre> <p>...to use like this:</p> <pre><code>&lt;div class=\"suit-row suit-row-3-col\"&gt;\n  &lt;div class=\"suit-column\"&gt;\n    &lt;p&gt;1&lt;/p&gt;\n  &lt;/div&gt;\n  &lt;div class=\"suit-column\"&gt;\n    &lt;p&gt;2&lt;/p&gt;\n  &lt;/div&gt;\n  &lt;div class=\"suit-column\"&gt;\n    &lt;p&gt;3&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>...and which results like this:</p> <p></p> <p>But this tweak is very limited because you can't have columns with different sizes on the same row...</p>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#more-dashboard-pages","title":"More dashboard pages","text":"<p>Just add your additional URLs in <code>dashboard.sites.AdminMixin</code> (they should maybe be added at the end of the original URLs: <code>return custom_url + urls + additional_urls</code>), create the views and the templates and it's done.</p>"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#django-suit-dashboard","title":"Django Suit Dashboard","text":"<p>I wrote a Django app to do this: django-suit-dashboard. Go take a look! (and please keep in mind it's just a beta and it could/will be improved).</p>"},{"location":"posts/docker-compose-django-postgres-nginx/","title":"Docker Compose with NginX, Django, Gunicorn and multiple Postgres databases","text":"<p>This post explains how to setup your Docker configuration for a web application based on the Django framework. I got a lot of inspiration from other tutorials and Docker examples: you can check these resources with the links at the bottom of the post. You can also directly check the repository that reflects this tutorial.</p> <p>In this particular example, we will use Gunicorn, but it should be easy enough to replace it with an alternative Python WSGI HTTP server such as uwsgi. We will also make use of pipenv, with related Pipfile and Pipfile.lock, instead of plain pip and requirements.txt files.</p> <p>Here is the plan:</p> <ol> <li>Overview: to get a better understanding of the whole thing</li> <li>Dockerfile: a simple Django application served by Gunicorn</li> <li>Pipenv: spice things up with Pipfile and Pipfile.lock</li> <li>Compose: add a container for NginX</li> <li>Compose: add containers for one or more Postgres databases</li> <li>Static files: collecting, storing and serving</li> <li>Resources</li> </ol>"},{"location":"posts/docker-compose-django-postgres-nginx/#overview-to-get-a-better-understanding-of-the-whole-thing","title":"Overview: to get a better understanding of the whole thing","text":"<p>So, let's start with some drawings in order to get a better idea of what we want to accomplish, and how everything will fit together.</p> <p>In this first, very simple image, you can see that we want three containers: one for NginX, one for Django + Gunicorn (they always go together), and one for our database. The NginX container communicate with the Django+Gunicorn one, which itself connects to the Postgres container. Pretty straight-forward, right?</p> <p></p> <p>In our configuration, it means we will declare three containers, or three services if we talk in terms of Docker Compose.</p> <p>Except that we need bridges between the containers, in order for them to communicate. Let's add these bridges:</p> <p></p> <p>In <code>docker-compose.yml</code>, we will declare these bridges thanks to the <code>networks</code> directive, and connect them to the right containers.</p> <p>Of course, you may want or need several databases for your project. So here is an updated image with two database containers. It's simply a matter of adding a new brige:</p> <p></p> <p>Once you know how to do it for two databases, it's very easy to add more.</p> <p>Now, this is enough for local development. But each time you restart your containers or services, the data in the Postgres databases will be lost. In production, we need these data to be persistent. If we keep the data in production, let's keep them in local environment as well. To do this, we will use volumes, a feature of Docker:</p> <p></p> <p>Alright, that is enough for the overview, let's get our hands dirty!</p>"},{"location":"posts/docker-compose-django-postgres-nginx/#dockerfile-a-simple-django-application-served-by-gunicorn","title":"Dockerfile: a simple Django application served by Gunicorn","text":"<p>If you don't already have a simple Django project available for testing, I invite you to create one with <code>django-admin startproject hello</code>.</p> <p>Here is the directory/file tree you should have in order to follow this tutorial:</p> <pre><code>.                         # Your current directory, created for this tutorial\n\u2514\u2500\u2500 hello                 # The Django project\n \u00a0\u00a0 \u251c\u2500\u2500 hello             # The main Django app of your project\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 settings.py\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 urls.py\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wsgi.py\n    \u2514\u2500\u2500 manage.py\n</code></pre> <p>Now that you have a working Django project, you can run it by going into the <code>hello</code> directory and type <code>./manage.py runserver</code>. Go to http://localhost:8000 to see the result.</p> <p>Instead of running it with the Django <code>runserver</code> management command, let's try with Gunicorn. First, install it with <code>pip install gunicorn</code>, be it in a virtualenv or system-wide with <code>sudo pip install gunicorn</code>.</p> <p>It's as easy as running <code>gunicorn --bind :8000 hello.wsgi:application</code> from inside the Django project. If you are one directory above, use <code>gunicorn --chdir hello --bind :8000 hello.wsgi:application</code>.</p> <p>We have all we need to write our Dockerfile:</p> <pre><code># start from an official image\nFROM python:3.6\n\n# arbitrary location choice: you can change the directory\nRUN mkdir -p /opt/services/djangoapp/src\nWORKDIR /opt/services/djangoapp/src\n\n# install our two dependencies\nRUN pip install gunicorn django\n\n# copy our project code\nCOPY . /opt/services/djangoapp/src\n\n# expose the port 8000\nEXPOSE 8000\n\n# define the default command to run when starting the container\nCMD [\"gunicorn\", \"--chdir\", \"hello\", \"--bind\", \":8000\", \"hello.wsgi:application\"]\n</code></pre> <p>The Dockerfile must be placed at the root of your test directory. As a reminder:</p> <pre><code>.                         # Your current directory, created for this tutorial\n\u251c\u2500\u2500 hello                 # The Django project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hello             # The main Django app of your project\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 manage.py\n\u2514\u2500\u2500 Dockerfile            # Your Dockerfile\n</code></pre> <p>We are now able to build our container with <code>docker build . -t hello</code>, and to start it with <code>docker run -p 8000:8000 hello</code>. The <code>-p 8000:8000</code> option says to bind the port 8000 of the host to the port 8000 of the container, allowing you to go to http://localhost:8000 and see your application running as if you were inside of the container.</p>"},{"location":"posts/docker-compose-django-postgres-nginx/#pipenv-spice-things-up-with-pipfile-and-pipfilelock","title":"Pipenv: spice things up with Pipfile and Pipfile.lock","text":"<p>This step is completely optional. If you prefer to use plain pip and requirements files, you can skip this section.</p> <p>First install pipenv with <code>pip install pipenv</code>, or system-wide with <code>sudo pip install pipenv</code>. Since we only need Django and Gunicorn, our Pipfile will be very simple:</p> <pre><code>[[source]]\nurl = \"https://pypi.python.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n\n[packages]\nDjango = \"*\"\ngunicorn = \"*\"\n\n\n[requires]\n# our Dockerfile is based on Python 3.6\npython_version = \"3.6\"\n</code></pre> <p>Just like the Dockerfile, Pipfile must be placed at the root of the project.</p> <pre><code>.\n\u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 manage.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 Pipfile\n</code></pre> <p>Simply run <code>pipenv lock</code> to create Pipfile.lock from Pipfile.</p> <p>Now we need to update our Dockerfile to use pipenv:</p> <pre><code># start from an official image\nFROM python:3.6\n\n# arbitrary location choice: you can change the directory\nRUN mkdir -p /opt/services/djangoapp/src\nWORKDIR /opt/services/djangoapp/src\n\n# install our dependencies\n# we use --system flag because we don't need an extra virtualenv\nCOPY Pipfile Pipfile.lock /opt/services/djangoapp/src/\nRUN pip install pipenv &amp;&amp; pipenv install --system\n\n# copy our project code\nCOPY . /opt/services/djangoapp/src\n\n# expose the port 8000\nEXPOSE 8000\n\n# define the default command to run when starting the container\nCMD [\"gunicorn\", \"--chdir\", \"hello\", \"--bind\", \":8000\", \"hello.wsgi:application\"]\n</code></pre> <p>You can rebuild the image with <code>docker build . -t hello</code> and try to run it again to see if everything works correctly.</p>"},{"location":"posts/docker-compose-django-postgres-nginx/#compose-add-a-container-for-nginx","title":"Compose: add a container for NginX","text":"<p>Since we will then have two containers, one for Django + Gunicorn, and one for NginX, it's time to start our composition with Docker Compose and <code>docker-compose.yml</code>. Create your <code>docker-compose.yml</code> file at the root of the project, like following:</p> <pre><code>.\n\u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 manage.py\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 Pipfile\n</code></pre> <p>We are gonna use the version 3 of the configuration syntax. First, we add the Django+Gunicorn service:</p> <pre><code>version: '3'\n\nservices:\n  djangoapp:\n    build: .\n    volumes:\n      - .:/opt/services/djangoapp/src\n    ports:\n      - 8000:8000\n</code></pre> <p>We simply tell Docker Compose that the <code>djangoapp</code> service must use an image that is built from the current directory, therefore looking for our Dockerfile. The <code>volumes</code> directive tells to bind the current directory of the host to the <code>/opt/services/djangoapp/src</code> directory of the container. The changes in our current directory will be reflected in real-time in the container directory. And reciprocally, changes that occur in the container directory will occur in our current directory as well.</p> <p>Build and run the service with <code>docker-compose up</code>. The name of the image will be automatically chosen by Docker Compose (it will be the name of the current directory with <code>_djangoapp</code> appended).</p> <p>Ok, let's add our NginX service now:</p> <pre><code>version: '3'\n\nservices:\n\n  djangoapp:\n    build: .\n    volumes:\n      - .:/opt/services/djangoapp/src\n\n  nginx:\n    image: nginx:1.13\n    ports:\n      - 8000:80\n    volumes:\n      - ./config/nginx/conf.d:/etc/nginx/conf.d\n    depends_on:  # &lt;-- wait for djangoapp to be \"ready\" before starting this service\n      - djangoapp\n</code></pre> <p>Note that we removed the <code>ports</code> directive from our <code>djangoapp</code> service. Indeed we will not communicate directly with Gunicorn anymore, but with NginX. We still want to access our app at http://localhost:8000, and we want NginX to listen to the port 80 in the container, so we use <code>ports: - 8000:80</code>.</p> <p>Note: in a production environment, we would use <code>80:80</code> instead.</p> <p>We also bind a local directory to the <code>/etc/nginx/conf.d</code> container directory. Let's create it and see what's inside:</p> <pre><code>mkdir -p config/nginx/conf.d\ntouch config/nginx/conf.d/local.conf\n</code></pre> <p>You should now have the following files and directories:</p> <pre><code>.\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nginx\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 conf.d\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 local.conf\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hello\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 manage.py\n\u251c\u2500\u2500 Pipfile\n\u2514\u2500\u2500 Pipfile.lock\n</code></pre> <p>The <code>config/nginx/conf.d/local.conf</code> file contains our NginX configuration:</p> <pre><code># first we declare our upstream server, which is our Gunicorn application\nupstream hello_server {\n    # docker will automatically resolve this to the correct address\n    # because we use the same name as the service: \"djangoapp\"\n    server djangoapp:8000;\n}\n\n# now we declare our main server\nserver {\n\n    listen 80;\n    server_name localhost;\n\n    location / {\n        # everything is passed to Gunicorn\n        proxy_pass http://hello_server;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n    }\n}\n</code></pre> <p>But before we try this out, remember that we need a bridge to make our services able to communicate?</p> <p>Update your <code>docker-compose.yml</code> as follow:</p> <pre><code>version: '3'\n\nservices:\n\n  djangoapp:\n    build: .\n    volumes:\n      - .:/opt/services/djangoapp/src\n    networks:  # &lt;-- here\n      - nginx_network\n\n  nginx:\n    image: nginx:1.13\n    ports:\n      - 8000:80\n    volumes:\n      - ./config/nginx/conf.d:/etc/nginx/conf.d\n    depends_on:\n      - djangoapp\n    networks:  # &lt;-- here\n      - nginx_network\n\nnetworks:  # &lt;-- and here\n  nginx_network:\n    driver: bridge\n</code></pre> <p>Run <code>docker-compose up</code> and see if you can still see the Django default page at http://localhost:8000.</p>"},{"location":"posts/docker-compose-django-postgres-nginx/#compose-add-containers-for-one-or-more-postgres-databases","title":"Compose: add containers for one or more Postgres databases","text":"<p>We now want to use Postgres instead of the starting default SQLite database. We will need to update several things: our Pipfile, because we need the <code>psycopg2</code> Python package, the Postgres driver; our Django project settings; and our <code>docker-compose.yml</code> file.</p> <ul> <li> <p>Pipfile becomes:</p> <pre><code>[[source]]\nurl = \"https://pypi.python.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n\n[packages]\nDjango = \"*\"\ngunicorn = \"*\"\n\"psycopg2\" = \"*\"\n\n\n[requires]\n# our Dockerfile is based on Python 3.6\npython_version = \"3.6\"\n</code></pre> <p>Don't forget to run <code>pipenv lock</code> to update your lock file, and rebuild your Docker image with <code>docker-compose build</code>.</p> </li> <li> <p>In the Django project settings, update the DATABASE setting from:</p> <pre><code>DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": os.path.join(BASE_DIR, \"db.sqlite3\"),\n    }\n}\n</code></pre> <p>...to:</p> <pre><code>DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.postgresql_psycopg2\",\n        \"NAME\": \"database1\",\n        \"USER\": \"database1_role\",\n        \"PASSWORD\": \"database1_password\",\n        \"HOST\": \"database1\",  # &lt;-- IMPORTANT: same name as docker-compose service!\n        \"PORT\": \"5432\",\n    }\n}\n</code></pre> <p>As you can see, we used <code>database1</code> everywhere, for the name, user, password and host. In fact, we can change these values to whatever suits us. But we must ensure the database container will use the same values! To do that, we will copy these values in a configuration file destined to be read by our database container.</p> <p>Create a <code>db</code> directory in the <code>config</code> one, and add the <code>database1_env</code> file:</p> <pre><code>mkdir config/db\ntouch config/db/database1_env\n</code></pre> <p>The contents of <code>config/db/database1_env</code> must then be:</p> <pre><code>POSTGRES_USER=database1_role\nPOSTGRES_PASSWORD=database1_password\nPOSTGRES_DB=database1\n</code></pre> <p>These variable are used by the Postgres Docker image, for more information please check out the documentation on docs.docker.com or hub.docker.com.</p> <p>It means that, when started, the Postgres container will create a database called <code>database1</code>, assigned to the role <code>database1_role</code> with password <code>database1_password</code>. If you change these values, remember to also change them in the DATABASES setting.</p> </li> <li> <p>We are now ready to add our service in <code>docker-compose.yml</code>. The added service must have the same name than what is declared in the DATABASES setting:</p> <pre><code>version: '3'\n\nservices:\n\n  djangoapp:\n    build: .\n    volumes:\n      - .:/opt/services/djangoapp/src\n    networks:\n      - nginx_network\n      - database1_network  # &lt;-- connect to the bridge\n    depends_on:  # &lt;-- wait for db to be \"ready\" before starting the app\n      - database1\n\n  nginx:\n    image: nginx:1.13\n    ports:\n      - 8000:80\n    volumes:\n      - ./config/nginx/conf.d:/etc/nginx/conf.d\n    depends_on:\n      - djangoapp\n    networks:\n      - nginx_network\n\n  database1:  # &lt;-- IMPORTANT: same name as in DATABASES setting, otherwise Django won't find the database!\n    image: postgres:10\n    env_file:  # &lt;-- we use the previously defined values\n      - config/db/database1_env\n    networks:  # &lt;-- connect to the bridge\n      - database1_network\n    volumes:\n      - database1_volume:/var/lib/postgresql/data\n\nnetworks:\n  nginx_network:\n    driver: bridge\n  database1_network:  # &lt;-- add the bridge\n    driver: bridge\n\nvolumes:\n  database1_volume:\n</code></pre> <p>You should be able to understand everything here. However, we added two new things: the <code>database1: volumes:</code> directive, and the root <code>volumes:</code> directive. You need to declare your volumes in the root <code>volumes:</code> directive if you want them to be kept persistently. Then, you can bind a volume to a directory in the container. Here, we bind our declared <code>database1_volume</code> to the <code>database1</code> container's <code>/var/lib/postgresql/data</code> directory. Everything added to this directory will be persistently stored in the volume called <code>database1_volume</code>. So each subsequent run of the container will have access to the previous data! It means you can stop and restart your service without losing the data.</p> </li> </ul> <p>OK, let's try it. As we are using Django, we need to \"migrate\" the database first. To do this, we will simply use Docker Compose to start our <code>djangoapp</code> service and run the migration command inside it:</p> <pre><code>docker-compose build  # to make sure everything is up-to-date\ndocker-compose run --rm djangoapp /bin/bash -c \"cd hello; ./manage.py migrate\"\n</code></pre> <p>From now on, it should be really easy to add other databases: just add other database services (<code>database2</code>) with their networks volumes (remember to connect the networks and bind the volumes), update your DATABASES setting in the Django project, and create the environment file for each database in <code>config/db</code>.</p>"},{"location":"posts/docker-compose-django-postgres-nginx/#static-files-collecting-storing-and-serving","title":"Static files: collecting, storing and serving","text":"<p>UPDATE: a lot of us (including myself) seem to have trouble updating the static files in the designated Docker volume. In retrospect, collecting the static files in the Dockerfile might not be a good idea, because of the caching mechanisms of Docker. A simple and effective solution, mentioned by Isaac T Chikutukutu in the comments, is to remove this command from the Dockerfile and manually run it when needed: <code>docker-compose run djangoapp hello/manage.py collectstatic --no-input</code></p> <p>I edited the rest of the post accordingly.</p> <p>(End of update)</p> <p>Let's not forget about the static files! In order for NginX to serve them, we will update the <code>config/nginx/conf.d/local.conf</code> file, as well as our <code>docker-compose.yml</code> file. Static files will be stored in volumes. We also need to set the <code>STATIC_ROOT</code> and <code>MEDIA_ROOT</code> variables in the Django project settings.</p> <ul> <li> <p>NginX configuration:</p> <pre><code>upstream hello_server {\n    server djangoapp:8000;\n}\n\nserver {\n\n    listen 80;\n    server_name localhost;\n\n    location / {\n        proxy_pass http://hello_server;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $host;\n        proxy_redirect off;\n    }\n\n    location /static/ {\n        alias /opt/services/djangoapp/static/;\n    }\n\n    location /media/ {\n        alias /opt/services/djangoapp/media/;\n    }\n}\n</code></pre> </li> <li> <p>Django project settings:</p> <pre><code># as declared in NginX conf, it must match /opt/services/djangoapp/static/\nSTATIC_ROOT = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"static\")\n\n# do the same for media files, it must match /opt/services/djangoapp/media/\nMEDIA_ROOT = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"media\")\n</code></pre> </li> <li> <p>Volumes in <code>docker-compose.yml</code>:</p> <pre><code>version: '3'\n\nservices:\n\n  djangoapp:\n    build: .\n    volumes:\n      - .:/opt/services/djangoapp/src\n      - static_volume:/opt/services/djangoapp/static  # &lt;-- bind the static volume\n      - media_volume:/opt/services/djangoapp/media  # &lt;-- bind the media volume\n    networks:\n      - nginx_network\n      - database1_network\n    depends_on:\n      - database1\n\n  nginx:\n    image: nginx:1.13\n    ports:\n      - 8000:80\n    volumes:\n      - ./config/nginx/conf.d:/etc/nginx/conf.d\n      - static_volume:/opt/services/djangoapp/static  # &lt;-- bind the static volume\n      - media_volume:/opt/services/djangoapp/media  # &lt;-- bind the media volume\n    depends_on:\n      - djangoapp\n    networks:\n      - nginx_network\n\n  database1:\n    image: postgres:10\n    env_file:\n      - config/db/database1_env\n    networks:\n      - database1_network\n    volumes:\n      - database1_volume:/var/lib/postgresql/data\n\nnetworks:\n  nginx_network:\n    driver: bridge\n  database1_network:\n    driver: bridge\n\nvolumes:\n  database1_volume:\n  static_volume:  # &lt;-- declare the static volume\n  media_volume:  # &lt;-- declare the media volume\n</code></pre> </li> <li> <p>And finally, collect the static files each time you need to update them, by running:</p> <pre><code>docker-compose run djangoapp hello/manage.py collectstatic --no-input\n</code></pre> </li> </ul>"},{"location":"posts/docker-compose-django-postgres-nginx/#resources","title":"Resources","text":"<p>Here are the resources I used to write this tutorial:</p> <ul> <li>Nginx+Flask+Postgres multi-container setup with Docker Compose<ul> <li>The repository</li> </ul> </li> <li>Docker how to Django + uwsgi/gunicorn + nginx?</li> <li>Django tutorial using Docker, Nginx, Gunicorn and PostgreSQL.</li> <li>Django Development With Docker Compose and Machine</li> <li>Deploy Django, Gunicorn, NGINX, Postgresql using Docker</li> <li>Docker, how to expose a socket over a port for a Django Application</li> </ul> <p>And here is the repository that reflects this tutorial (with a few more things).</p> <p>Don't hesitate to share other interesting resources in the comment section!</p>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/","title":"Documentation in your shell scripts using shellman","text":"<p>When I write a script, I like to have a <code>-h, --help</code> option to help me remember what it does and how it works. But I was never delighted to write this help text. Besides, when your script's options change, you have to update the help text.</p> <p>I also always liked man pages, their search feature and their ability to scroll up and down and not leave any output in the console. But maintaining a man page is even more tedious than maintaining a help text.</p> <p>This is why I thought of using documentation in shell scripts. So I wrote shellman.</p> <ul> <li>Writing doc</li> <li>Using shellman</li> <li>An example</li> <li>How it looks<ul> <li>Text</li> <li>Man</li> <li>Markdown</li> </ul> </li> <li>More</li> </ul>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#writing-doc","title":"Writing doc","text":"<p>Shellman can read special comments in your file. These special comments are the documentation of your script or library. They simply begin with two sharps instead of one:</p> <pre><code># A normal comment.\n## A documentation comment.\n</code></pre> <p>Also, to give a special meaning to your documentation, each documentation line can be tagged, just like you would do in doxygen documentation:</p> <pre><code>## \\brief A brief description.\n## \\desc A long description.\n## You can write many lines in this tag.\n</code></pre>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#using-shellman","title":"Using shellman","text":"<p>After reading and loading all the documentation written in a file, shellman will then be able to output it on stdout (on in another file) in different formats, such as text, man page or markdown.</p> <pre><code>shellman my_script.sh  # text output by default\nman &lt;(shellman --format man my_script.sh)\n# Available formats for -f, --format option: text, man, markdown\n</code></pre> <p>Shellman also has a <code>--check</code> option that will not output anything, but instead check for written documentation in a file. A simple <code>--check</code> option will return 0 or 1, but display no warnings. Add a <code>--warn</code> option to print warnings on stderr. It will help you fix your documentation, or even do some linting and continuous integration on your scripts' documentation.</p>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#an-example","title":"An example","text":"<p>Here is an example of how I write documentation in my scripts now:</p> <pre><code>#!/bin/bash\n\n## \\brief shell script debugger\n\n## \\desc Run a script in path with the -x bash option (and more).\n## You should set the PS4 variable for better output.\n\n## \\env PS4\n## Debugger prompt. This is the prefix that bash prepends\n## to the current instruction when using -x option.\n\nmain() {\n  case \"$1\" in\n\n    ## \\option -t, --test\n    ## Read the script and warn for encountered syntax errors.\n    ## Do not actually run the script.\n    -t|--test) FLAGS=-n; shift ;;\n\n    ## \\option -v, --verbose\n    ## Run the script with verbose option.\n    -v|--verbose) FLAGS=-xv; shift ;;\n\n    ## \\option -n, --dry-run\n    ## Options test and verbose combined. Validate the syntax\n    ## and print the script to stdout.\n    -n|--dry-run) FLAGS=-xvn; shift ;;\n\n    ## \\option -h, --help\n    ## Print this help and exit.\n    -h|--help) shellman \"$0\"; exit 0 ;;  # Here is the black magic!\n\n    *) FLAGS=-x ;;\n\n  esac\n  SCRIPT=$1\n  shift\n\n  /bin/bash \"${FLAGS}\" \"${SCRIPT}\" \"$@\"\n}\n\n## \\usage dbg [-tvn] SCRIPT\nmain \"$@\"\n</code></pre>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#how-it-looks","title":"How it looks","text":"<p>The different outputs would be like the following.</p>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#text","title":"Text","text":"<pre><code>Usage: dbg [-tvn] SCRIPT\n\nRun a script in path with the -x bash option (and more).\nYou should set the PS4 variable for better output.\n\nOptions:\n  -t, --test\n    Read the script and warn for encountered syntax errors.\n    Do not actually run the script.\n\n  -v, --verbose\n    Run the script with verbose option.\n\n  -n, --dry-run\n    Options test and verbose combined. Validate the syntax\n    and print the script to stdout.\n\n  -h, --help\n    Prints this help and exit.\n</code></pre>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#man","title":"Man","text":""},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#markdown","title":"Markdown","text":"<pre><code>**dbg** - shell script debugger\n# Usage\n`dbg [-tvn] SCRIPT`  \n\nRun a script in path with the -x bash option (and more).\nYou should set the PS4 variable for better output.\n\n# Options\n- `-t, --test`:\n  Read the script and warn for encountered syntax errors.\nDo not actually run the script.\n- `-v, --verbose`:\n  Run the script with verbose option.\n- `-n, --dry-run`:\n  Options test and verbose combined. Validate the syntax\nand print the script to stdout.\n- `-h, --help`:\n  Prints this help and exit.\n\n# Environment variables\n- `PS4`:\n  Debugger prompt. This is the prefix that bash prepends to\nthe current instruction when using -x option.\n</code></pre> <p>... which would look like:</p> <p></p>"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#more","title":"More","text":"<p>Shellman supports more tags, in particular: function tags. See its documentation.</p>"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/","title":"How to install NVidia drivers on BunsenLabs/Debian 8 to setup dual screens","text":"<p>Honestly, this post is mostly a personal memo. I will NOT go through this again!</p> <ul> <li>The fun way</li> <li>The quick way</li> <li>~~If~~ When things go wrong (you should probably read this anyway)</li> <li>Appendix</li> </ul>"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#the-fun-way","title":"The fun way","text":"<p>First, to save your time and mind, install the Linux headers!</p> <pre><code>sudo apt install linux-headers-$(uname -r)\n</code></pre> <p>I spent two painful hours this afternoon trying to figure out why it wouldn't work. It happens that I didn't install these headers!</p> <p>Also install these requirements:</p> <pre><code>sudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev\n</code></pre> <p>Now, install the Nvidia Detect program. It will tell you where to get the driver from:</p> <pre><code>sudo apt install nvidia-detect\n</code></pre> <p>Run it:</p> <pre><code>$ nvidia-detect\nDetected NVIDIA GPUs:\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:1402] (rev a1)\nYour card is only supported by a newer driver that is available in jessie-backports.\nSee http://backports.debian.org for instructions how to use backports.\nYou may also find newer driver packages in experimental.\nIt is recommended to install the\n    nvidia-driver/jessie-backports\npackage.\n</code></pre> <p>So, in my case, I have to install the driver from Jessie backports.</p> <p>Beware!! It might not be your case! Pay attention to what <code>nvidia-detect</code> says, and do what it says :D!</p> <p>So, for Jessie backports, you would install the driver like this:</p> <pre><code>sudo apt install -t jessie-backports nvidia-driver\n</code></pre> <p>But the driver is not the only thing you need to install. I'm gonna install the rest from Jessie backports as well, adapt to your needs!</p> <pre><code>sudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia\n</code></pre> <p>Reboot! At this point, it won't hurt your OS (it will actually help).</p> <p></p> <p>Rebooting...</p> <p>Alright, you should now be able to run <code>nvidia-xconfig</code> without trouble:</p> <pre><code>sudo nvidia-xconfig\n</code></pre> <p>This command will create or update the file <code>/etc/X11/xorg.conf</code>.</p> <p>You can now restart your X server and send me a not-positive comment below if nothing works and your system is broken.</p> <pre><code>sudo systemctl restart lightdm.service\n</code></pre> <p>Login, then finalize with:</p> <pre><code>sudo nvidia-settings\n</code></pre> <p></p> <p>Set your screen(s) resolutions and everything, save the configuration into <code>/etc/X11/xorg.conf</code>, maybe reboot one last time, and you should be good to go!</p>"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#the-quick-way","title":"The quick way","text":"<p>It's just a summary of the previous commands. Don't actually copy-paste-run it: have a break at the lonely <code>nvidia-detect</code> line.</p> <pre><code>sudo apt install linux-headers-$(uname -r)\nsudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev\nsudo apt install nvidia-detect\n\nnvidia-detect  # update the apt commands after this\n\nsudo apt install -t jessie-backports nvidia-driver\nsudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia\n# reboot\nsudo nvidia-xconfig\nsudo systemctl restart lightdm.service\nsudo nvidia-settings  # update your config and save it\n# reboot\n</code></pre>"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#if-when-things-go-wrong","title":"~~If~~ When things go wrong","text":"<ul> <li>You should consider following another tutorial.</li> <li>If the X server does not start, hit <code>Ctrl-Alt-F1</code> to get a non-graphical terminal. From there, simply delete the bad <code>/etc/X11/xorg.conf</code> and run <code>systemctl restart lightdm.service</code> again to get back your beloved GUI.</li> <li>If you installed the wrong things: it's pretty safe to purge all <code>nvidia-*</code> packages, but it is not safe to purge <code>xserver-xorg-*</code> ones. Except the specific <code>xserver-xorg-video-nvidia</code>. Or if you like the Linux (re)installation process.</li> <li>After an update/upgrade: if you upgraded your system with <code>apt update; apt upgrade</code> and your X server does not start anymore, it might be because your Linux headers have not been updated. Remember to always update your Linux headers after the kernel has been upgraded! <code>sudo apt install linux-headers-$(uname -r)</code></li> </ul>"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#appendix","title":"Appendix","text":"<p>Example of <code>/etc/X11/xorg.conf</code> for dual screen setup:</p> <pre><code># nvidia-settings: X configuration file generated by nvidia-settings\n# nvidia-settings:  version 375.26  (buildd@debian)  Wed Jan 18 14:43:15 UTC 2017\n\n# nvidia-xconfig: X configuration file generated by nvidia-xconfig\n# nvidia-xconfig:  version 340.46  (buildd@brahms)  Tue Oct  7 08:00:32 UTC 2014\n\nSection \"ServerLayout\"\n    Identifier     \"Layout0\"\n    Screen      0  \"Screen0\" 0 0\n    InputDevice    \"Keyboard0\" \"CoreKeyboard\"\n    InputDevice    \"Mouse0\" \"CorePointer\"\n    Option         \"Xinerama\" \"0\"\nEndSection\n\nSection \"Files\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Mouse0\"\n    Driver         \"mouse\"\n    Option         \"Protocol\" \"auto\"\n    Option         \"Device\" \"/dev/psaux\"\n    Option         \"Emulate3Buttons\" \"no\"\n    Option         \"ZAxisMapping\" \"4 5\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Keyboard0\"\n    Driver         \"kbd\"\nEndSection\n\nSection \"Monitor\"\n    Identifier     \"Monitor0\"\n    VendorName     \"Unknown\"\n    ModelName      \"HP LE2202x\"\n    HorizSync       24.0 - 94.0\n    VertRefresh     50.0 - 76.0\n    Option         \"DPMS\"\nEndSection\n\nSection \"Device\"\n    Identifier     \"Device0\"\n    Driver         \"nvidia\"\n    VendorName     \"NVIDIA Corporation\"\n    BoardName      \"GeForce GTX 950\"\nEndSection\n\nSection \"Screen\"\n\n# Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 640x480 +1920+0\"\n# Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+128\"\n    Identifier     \"Screen0\"\n    Device         \"Device0\"\n    Monitor        \"Monitor0\"\n    DefaultDepth    24\n    Option         \"Stereo\" \"0\"\n    Option         \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+90\"\n    Option         \"SLI\" \"Off\"\n    Option         \"MultiGPU\" \"Off\"\n    Option         \"BaseMosaic\" \"off\"\n    SubSection     \"Display\"\n        Depth       24\n    EndSubSection\nEndSection\n</code></pre>"},{"location":"posts/how-to-deal-with-jinja2-spacing/","title":"How to deal with spacing in Jinja2 templates","text":"<p>It started with a comment in a GitHub issue.</p> <p>I often find it difficult to wrangle spacing jinja2 templates especially around optional clauses.</p> <p>I couldn't wrap my head either about this. So I decided to write a script and test every combination of newlines, indentation and dashes or no dashes in <code>{%-</code>, with the goal being to find combinations that will not have an extra middle line when an optional Jinja clause is false.</p>"},{"location":"posts/how-to-deal-with-jinja2-spacing/#script","title":"Script","text":"<pre><code>from jinja2 import Environment\n\n\nflip = (True, False)\nn = \"\\n\"\n\n\ndef get_template(n1, n2, n3, n4, d1, d2, d3, d4, i):\n    return (\n        f\"  a\"\n        f\"{(n + ('  ' if i else '')) if n1 else ''}\"\n        f\"{{%{'-' if d1 else ''} if x {'-' if d2 else ''}%}}\"\n        f\"{n if n2 else ''}\"\n        f\"  b\"\n        f\"{(n + ('  ' if i else '')) if n3 else ''}\"\n        f\"{{%{'-' if d3 else ''} endif {'-' if d4 else ''}%}}\"\n        f\"{n if n4 else ''}\"\n        f\"  c\"\n    )\n\n\ndef get_templates():\n    for newline1 in flip:\n        for newline2 in flip:\n            for newline3 in flip:\n                for newline4 in flip:\n                    for dash1 in flip:\n                        for dash2 in flip:\n                            for dash3 in flip:\n                                for dash4 in flip:\n                                    for indent in flip:\n                                        yield get_template(\n                                            newline1,\n                                            newline2,\n                                            newline3,\n                                            newline4,\n                                            dash1,\n                                            dash2,\n                                            dash3,\n                                            dash4,\n                                            indent,\n                                        )\n\n\nif __name__ == \"__main__\":\n    good = []\n    env = Environment()\n    for n, template_string in enumerate(sorted(set(get_templates())), 1):\n        template = env.from_string(template_string)\n        true = template.render(x=True)\n        false = template.render(x=False)\n        if true == \"  a\\n  b\\n  c\" and false == \"  a\\n  c\":\n            good.append(template_string)\n    print(\"\\n\\n---\\n\\n\".join(good))\n    print(f\"\\ntested: {n}\\nvalid: {len(good)}\")\n</code></pre>"},{"location":"posts/how-to-deal-with-jinja2-spacing/#results","title":"Results","text":"<p>Only 19 different combinations are valid on the 448 tested ones:</p> <pre><code>  a\n  {% if x -%}\n  b\n  {% endif -%}\n  c\n\n---\n\n  a\n  {% if x -%}\n  b\n  {% endif -%}  c\n\n---\n\n  a\n  {% if x -%}  b\n  {% endif -%}\n  c\n\n---\n\n  a\n  {% if x -%}  b\n  {% endif -%}  c\n\n---\n\n  a\n  {%- if x %}\n  b\n  {%- endif %}\n  c\n\n---\n\n  a\n  {%- if x %}\n  b{% endif %}\n  c\n\n---\n\n  a\n  {%- if x %}\n  b{%- endif %}\n  c\n\n---\n\n  a\n{% if x %}  b\n{% endif %}  c\n\n---\n\n  a\n{%- if x %}\n  b\n{%- endif %}\n  c\n\n---\n\n  a\n{%- if x %}\n  b{% endif %}\n  c\n\n---\n\n  a\n{%- if x %}\n  b{%- endif %}\n  c\n\n---\n\n  a{% if x %}\n  b\n  {%- endif %}\n  c\n\n---\n\n  a{% if x %}\n  b\n{%- endif %}\n  c\n\n---\n\n  a{% if x %}\n  b{% endif %}\n  c\n\n---\n\n  a{% if x %}\n  b{%- endif %}\n  c\n\n---\n\n  a{%- if x %}\n  b\n  {%- endif %}\n  c\n\n---\n\n  a{%- if x %}\n  b\n{%- endif %}\n  c\n\n---\n\n  a{%- if x %}\n  b{% endif %}\n  c\n\n---\n\n  a{%- if x %}\n  b{%- endif %}\n  c\n\ntested: 448\nvalid: 19\n</code></pre> <p>Each one of these will render as</p> <pre><code>  a\n  b\n  c\n</code></pre> <p>...when the condition is true, and as</p> <pre><code>  a\n  c\n</code></pre> <p>...when the condition is false.</p> <p>Now you just have to pick the style you prefer!</p> <p>I find these three particularly readable (the first two are better when <code>b</code> spans on multiple lines):</p> <pre><code>  a\n  {%- if x %}\n  b\n  {%- endif %}\n  c\n</code></pre> <pre><code>  a\n  {% if x -%}\n  b\n  {% endif -%}\n  c\n</code></pre> <pre><code>  a{% if x %}\n  b{% endif %}\n  c\n</code></pre> <p>The third example readability can also be improved:</p> <pre><code>  a   {%- if x %}\n  b   {%- endif %}\n  c\n</code></pre> <p>Here is an example with blank lines in <code>b</code>'s contents. The first one collapses contents up, while the second collapses content down.</p> <pre><code>class Run:\n    def pause(self):\n        print(\"pausing\")\n    {%- if stoppable %}\n\n\n    def stop(self):\n        print(\"stopping\")\n    {%- endif %}\n\n\n    def resume(self):\n      print(\"resuming\")\n</code></pre> <pre><code>class Run:\n    def pause(self):\n        print(\"pausing\")\n\n\n    {% if stoppable -%}\n    def stop(self):\n        print(\"stopping\")\n\n\n    {% endif -%}\n    def resume(self):\n        print(\"resuming\")\n</code></pre>"},{"location":"posts/howto-edit-git-commit-contents/","title":"How to edit the contents of a git commit","text":"<p>When you type <code>change git commit</code> or similar keywords in a search engine, you find many answers explaning how to rewrite a commit message, but not how to actually modify the contents of the commit.</p> <p>This post quickly explains how to do that.</p> <p>Lets begin by listing our commits:</p> <p></p> <p> In this screenshot I use my <code>gl</code> alias which expands to: <pre><code>git log --pretty=format:'%C(yellow)%h %Cgreen%ad %Cblue%an%Cgreen%d %Creset%s' --date=short --graph\n</code></pre> <p></p> <p>Here I want to modify the contents of the third commit, <code>feat: Refactor and add features</code>, because I mistakenly committed modifications to the changelog which I didn't want.</p> <p>So I run an interactive git rebase down to this commit by running:</p> <pre><code>git rebase -i HEAD~3\n</code></pre> <p></p> <p>This command launches your git editor (vim here) to tell git what to do. Here we tell git to stop right after the commit we want to modify:</p> <p></p> <p>We save and quit this temporary file (with <code>:wq</code>), and git tells us that it stopped at the desired commit:</p> <p></p> <p> Don't pay attention to the commit SHA: I took the screenshot afterwards so they don't match  </p> <p>Now you can start modifying, adding or deleting files!</p> <p>In my case I wanted to remove wrong sections in <code>CHANGELOG.md</code>, as well as remove conflict-resolution lines in <code>pyproject.toml</code>.</p> <p></p> <p> In this screenshot I use my <code>gs</code> alias which expands to <code>git status -sb</code>.  </p> <p>All you have to do now is to amend the current commit (the one at which we stopped, the one we wanted to modify):</p> <pre><code>git commit -a --amend --no-edit\n# --no-edit because we don't want to edit the message\n</code></pre> <p></p> <p>And finally, let git finish the interactive rebase:</p> <pre><code>git rebase --continue\n</code></pre> <p></p> <p>Done!</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/","title":"Local HTTP server sending fake files for testing purposes","text":"<p>I have developed a Python client/library for <code>aria2</code> called <code>aria2p</code>.</p> <p>To test my code, I spawn <code>aria2c</code> processes to interact with them and see if everything works correctly. I know I could mock the calls, but I feel like actually testing the interaction between my client and <code>aria2c</code> is a better way to ensure my code does its best.</p> <p>Until recently the test suite fed actual downloads to <code>aria2c</code> (like Linux distributions ISOs, metalinks, magnets, or torrents), but of course the inevitable happened: some URLs broke.</p> <p>So I decided to go full local. This post describes my attempt at spawning local HTTP servers to serve fake/virtual files efficiently, files that are then fed to <code>aria2c</code>.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#the-basics-httpserver","title":"The basics: <code>http.server</code>","text":"<p>As you may already know, Python's standard library provides a ready-to-go HTTP server that can serve files from a folder. It lives in the <code>http.server</code> module, and you can run it in a single command:</p> <pre><code>python -m http.server\n</code></pre> <p>This will serve the contents of your current working directory on <code>http://0.0.0.0:8000</code>.</p> <p>I was already using it in some tests when I needed to feed very small files to <code>aria2c</code>. I was serving <code>aria2p</code>'s code folder to use the LICENSE file: 759 bytes.</p> <p>So, the simplest solution that immediately came to mind was: dump text into files with predefined sizes, like 1B, 100B, 1KiB, 100KiB, 1MiB, 10MiB, 100MiB, etc. And do this each time you run the test suite, in a temporary directory that you will serve using Python's built-in HTTP server.</p> <p>But that sounds a bit wasteful, right? Wasting disk cycles on expensive SSDs is not really attractive. So it led me to the next solution.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#serving-files-from-memory","title":"Serving files from memory","text":"<p>Instead of creating files on the disk, why not just create them in memory?</p> <p>So I looked at the sources in <code>http.server</code>, particularly how <code>SimpleHTTPRequestHandler</code> sends the files contents to the client, and eventually ended up with this code:</p> <pre><code># http_server.py\nimport shutil\nimport socketserver\nimport sys\nfrom http import HTTPStatus\nfrom http.server import BaseHTTPRequestHandler\nfrom io import BytesIO\n\n\nclass RequestHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        size = self.path.lstrip(\"/\")\n        self.send_response(HTTPStatus.OK)\n        self.send_header(\"Content-type\", \"application/octet-stream\")\n        self.send_header(\"Content-Length\", size)\n        self.end_headers()\n        shutil.copyfileobj(BytesIO(b\"1\" * int(size)), self.wfile)\n\n\ndef run_http_server(port):\n    with socketserver.TCPServer((\"\", port), RequestHandler) as httpd:\n        try:\n            httpd.serve_forever()\n        finally:\n            httpd.server_close()\n\n\nif __name__ == \"__main__\":\n    print(\"Running HTTP server on port 8000\")\n    run_http_server(8000)\n</code></pre> <p>This snippet can be run directly with <code>python http_server.py</code>. The <code>run_http_server</code> can also be imported somewhere else.</p> <p>This server accepts GET requests on the path <code>/{size}</code>, which allows us to choose the size of the file we want to download. For example: <code>GET http://localhost:8000/1024</code> for 1Kib. The files are then only composed of ones.</p> <p>There is a problem however: this server will not support a heavy load (lots of tests running in parallel), because it can handle only one request at a time.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#multi-threading","title":"Multi-threading","text":"<p>Thankfully, <code>socketserver</code> also provides a <code>ThreadingTCPServer</code> we can use. It's as simple as:</p> <pre><code>def run_http_server(port=PORT):\n    with socketserver.ThreadingTCPServer((\"\", port), RequestHandler) as httpd:\n        try:\n            httpd.serve_forever()\n        finally:\n            httpd.server_close()\n</code></pre>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#reusable-address","title":"Reusable address","text":"<p>Even with the <code>finally</code> clause, I noticed that the address kept being \"in use\" for up to a minute sometimes, after the server was killed.</p> <p>This can be alleviated with the <code>allow_reuse_address</code> attribute of the socket server:</p> <pre><code>class Server(socketserver.ThreadingTCPServer):\n    allow_reuse_address = True\n\n\ndef run_http_server(port=PORT):\n    with Server((\"\", port), RequestHandler) as httpd:\n        try:\n            httpd.serve_forever()\n        finally:\n            httpd.server_close()\n</code></pre>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#still-not-enough","title":"Still not enough","text":"<p>But even with all this, many tests were still failing, and the standard error captured by <code>pytest</code> showed a lot of <code>BrokenPipe</code> errors coming from the server.</p> <p>I think it's because this server is way too simple, and handles only GET requests. Maybe <code>aria2c</code> is using HEAD as well? What about its \"pausing\" feature, allowing to pause downloads and resume them? Will the server handle it correctly?</p> <p>So, instead of digging more into this, I decided to try running a proper server.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#with-fastapi","title":"With FastAPI","text":"<p>I've been using FastAPI recently, so I gave it a try. A very small amount of code was needed:</p> <pre><code># http_server.py\nfrom io import BytesIO\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n\n@app.get(\"/{size}\")\nasync def get(size):\n    return StreamingResponse(\n        BytesIO(b\"1\" * size), media_type=\"application/octet-stream\"\n    )\n</code></pre> <p>I went with a <code>StreamingResponse</code> because the <code>FileResponse</code> only accepts paths to actual files.</p> <p>To run the server, I run this command: <code>uvicorn http_server:app</code>. And it seems to work great! All the tests that were previously failing now pass.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#trying-to-be-clever","title":"Trying to be clever","text":"<p>I'm still not really satisfied with this solution. It feels wasteful to allocate memory for each handled request. I tried storing the <code>BytesIO</code> object into a dictionary, with sizes as keys, but I guess their buffer get consumed when sending the response, so this doesn't work on subsequent requests using the same size. Instead of storing the <code>BytesIO</code> objects, we can store the strings themselves!</p> <pre><code>from io import BytesIO\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\nallocated = {}\n\n\n@app.get(\"/{size}\")\nasync def get(size: int):\n    if size not in allocated:\n        allocated[size] = b\"1\" * size\n    return StreamingResponse(\n        BytesIO(allocated[size]), media_type=\"application/octet-stream\"\n    )\n</code></pre> <p>But even then, why allocating memory at all? Couldn't we have a really \"virtual\" file, some kind of generator that generates chunks of data, sent as a streaming response, and that stops at the desired size?</p> <p>Well the <code>StreamingResponse</code> accepts async generators, so here's what I tried:</p> <pre><code>async def virtual_file(size, chunks=4096):\n    while size &gt; 0:\n        yield b\"1\" * min(size, chunks)\n        size -= chunks\n\n\n@app.get(\"/{size}\")\nasync def get(size: int):\n    return StreamingResponse(virtual_file(size), media_type=\"application/octet-stream\")\n</code></pre> <p>It seems to work, and the memory footprint is much, much lower, but I get a lot of <code>socket.send() raised exception.</code> messages from FastAPI, and a test occasionally fails. Anyway, I think it's good enough.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#additional-niceties","title":"Additional niceties","text":"<p>To make it easier to choose a medium-to-big file size, we can accept a unit suffix:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n\ndef translate_size(size):\n    try:\n        return int(size)\n    except ValueError:\n        pass\n    size = size.lower()\n    if size.endswith(\"k\"):\n        multiplier = 2**10\n    elif size.endswith(\"m\"):\n        multiplier = 2**20\n    elif size.endswith(\"g\"):\n        multiplier = 2**30\n    else:\n        raise ValueError(\"size unit not supported:\", size)\n    return int(size.rstrip(\"kmg\")) * multiplier\n\n\nasync def virtual_file(size, chunks=4096):\n    while size &gt; 0:\n        yield b\"1\" * min(size, chunks)\n        size -= chunks\n\n\n@app.get(\"/{size}\")\nasync def get(size: str):\n    return StreamingResponse(\n        virtual_file(translate_size(size)),\n        media_type=\"application/octet-stream\",\n    )\n</code></pre> <p>Note that the route <code>size</code> argument is not an integer anymore but a string. It allows us to send GET requests like <code>/4k</code> or <code>/100m</code>, values that are then translated to bytes.</p>"},{"location":"posts/local-http-server-fake-files-testing-purposes/#pytest-fixture","title":"Pytest fixture","text":"<p>Since I run tests in parallel thanks to the <code>pytest-xdist</code> plugin, I must make sure I run only one instance of the HTTP server.</p> <p>We can accomplish this by checking the worker ID (to handle both parallel and non-parallel cases) and by using a lock.</p> <p>The worker ID will be <code>master</code> for non-parallel runs. It can be obtained with the <code>worker_id</code> fixture.</p> <p>For the lock, I chose to use <code>mkdir</code> as it's an atomic operation. If the directory already exists, the operation fails (someone else already got the lock).</p> <p>Once the server is running, we must make sure it's ready, so we send GET requests until it responds.</p> <p>We put all this in a function and declare it a \"fixture\" with \"session\" scope and \"automatic use\":</p> <pre><code># tests/conftest.py\nimport subprocess\nimport sys\nimport time\n\nimport pytest\nimport requests  # or httpx\n\n\ndef spawn_and_wait_server(port=8000):\n    process = subprocess.Popen(\n        [\n            sys.executable,\n            \"-m\",\n            \"uvicorn\",\n            \"tests.http_server:app\",\n            \"--port\",\n            str(port),\n        ]\n    )\n    while True:\n        try:\n            requests.get(f\"http://localhost:{port}/1\")\n        except:\n            time.sleep(0.1)\n        else:\n            break\n    return process\n\n\n# credits to pytest-xdist's README\n@pytest.fixture(scope=\"session\", autouse=True)\ndef http_server(tmp_path_factory, worker_id):\n    if worker_id == \"master\":\n        # single worker: just run the HTTP server\n        process = spawn_and_wait_server()\n        yield process\n        process.kill()\n        process.wait()\n        return\n\n    # get the temp directory shared by all workers\n    root_tmp_dir = tmp_path_factory.getbasetemp().parent\n\n    # try to get a lock\n    lock = root_tmp_dir / \"lock\"\n    try:\n        lock.mkdir(exist_ok=False)\n    except FileExistsError:\n        yield  # failed, don't run the HTTP server\n        return\n\n    # got the lock, run the HTTP server\n    process = spawn_and_wait_server()\n    yield process\n    process.kill()\n    process.wait()\n</code></pre> <p>With this, each time you run your test suite, exactly one instance of the HTTP server will run.</p>"},{"location":"posts/migrate-disqus-comments-to-utterances-github/","title":"Migrate Disqus comments to Utterances (in GitHub issues) with Python","text":"<p>When I replaced Jekyll and my Jekyll-ReadTheDocs theme with MkDocs and a blog-customised version of the Material for MkDocs theme, the URLs of my posts changed.</p> <p>I was using Disqus for comments, and they provide a way to migrate threads from old URLs to new URLs. Unfortunately, this time it didn't work for some reason (I had already done it once in the past and it worked fine).</p> <p>I've read more and more criticism about Disqus related to privacy, so I looked at a replacement. The Disqus thread migration was not working so it was the perfect occasion!</p> <p>I've read a few webpages and got interested in Isso. Unfortunately again, I did not manage to install it on my Raspberry Pi.</p> <p>So I went with a much simpler solution: Utterances. You basically enable a GitHub app on your repository, add a script in your posts pages, and voil\u00e0: your new comment section powered by GitHub issues.</p> <p>I'm not completely satisfied because readers will need to log in with their GitHub account to comment (no anonymous comments), and you can't have nested discussions (well, just like in GitHub issues).</p> <p>But this was really easy to setup !</p> <p>Now I just needed to migrate the Disqus comments into GitHub issues. To do this semi-automatically, I wrote the following script. \"Semi-automatically\" because I still had to write a \"test\" comment in each one of my posts so Utterances would initiate/create the issues. I guess it could be implemented in the script directly, but since I just had a dozen of posts, I took the easy/repetitive way.</p>"},{"location":"posts/migrate-disqus-comments-to-utterances-github/#the-script","title":"The script","text":"<p>Warning</p> <p>You'll have to create the initial issues before running the script! Serve your blog locally, and write a \"test\" comment in every post that has comments.</p> <p>First, export your Disqus comments.</p> <p>Then you'll need two Python libraries:</p> <ul> <li><code>pygithub</code></li> <li><code>xmltodict</code></li> </ul> <p>You'll also need to create a token on GitHub, with just the <code>public repos</code> access.</p> <p>Write the following environment variables in a file, and source it.</p> <pre><code>export FILEPATH=\"comments.xml\"\nexport USERNAME=\"yourUsername\"\nexport TOKEN=\"yourToken\"\nexport REPOSITORY=\"yourUsername/yourUsername.github.io\"\nexport BASE_URL=\"https://yourUsername.github.io/\"\n</code></pre> <p>Now you can copy/paste and run this script:</p> <pre><code>import os\nimport time\nimport xmltodict\nfrom github import Github\n\nFILEPATH = os.environ[\"FILEPATH\"]\nUSERNAME = os.environ[\"USERNAME\"]\nTOKEN = os.environ[\"TOKEN\"]\nREPOSITORY = os.environ[\"REPOSITORY\"]\nBASE_URL = os.environ[\"BASE_URL\"]\n\n\ndef disqus_to_github():\n    g = Github(TOKEN)\n    repo = g.get_repo(REPOSITORY)\n    issues = repo.get_issues()\n\n    with open(FILEPATH) as fd:\n        data = xmltodict.parse(fd.read())\n\n    data = data[\"disqus\"]\n\n    threads = [dict(t) for t in data[\"thread\"]]\n    posts = sorted((dict(p) for p in data[\"post\"]), key=lambda d: d[\"createdAt\"])\n\n    # only keep threads with comments\n    twc_ids = set(p[\"thread\"][\"@dsq:id\"] for p in posts)\n    threads = {t[\"@dsq:id\"]: t for t in threads if t[\"@dsq:id\"] in twc_ids}\n\n    # associate the thread to each post\n    for post in posts:\n        post[\"thread\"] = threads[post[\"thread\"][\"@dsq:id\"]]\n\n    # associate the related GitHub issue to each thread\n    # warning: the issues need to exist before you run this script!\n    # write a \"test\" comment in each one of your post with comments\n    # to make Utterances create the initial issues\n    for thread in threads.values():\n        for issue in issues:\n            if issue.title == thread[\"link\"].replace(BASE_URL, \"\"):\n                thread[\"issue\"] = issue\n                break\n\n    # iterate on posts and create issues comments accordingly\n    for i, post in enumerate(posts, 1):\n        name = post[\"author\"][\"name\"]\n        user = post[\"author\"].get(\"username\")\n        mention = \" @\" + user if user and not user.startswith(\"disqus_\") else \"\"\n        date = post[\"createdAt\"]\n        message = post[\"message\"]\n        issue = post[\"thread\"][\"issue\"]\n        body = f\"*Original date: {date}*\\n\\n{message}\"\n        # don't add original author when it's you\n        if user != USERNAME:\n            body = f\"*Original author:* **{name}{mention}**  \\n{body}\"\n        print(f\"Posting {i}/{len(posts)} to issue {issue.number}    \\r\", end=\"\")\n        issue.create_comment(body)\n        # prevent hitting rate limits!\n        time.sleep(0.5)\n\n    print()\n\n\nif __name__ == \"__main__\":\n    disqus_to_github()\n</code></pre> <p>I wrote this for a one-time, personal use only, so it could easily crash when you try it! Just use your Python skills and adapt it </p>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/","title":"Passing Makefile arguments to a command, as they are typed in the command line.","text":"<p>In my Python projects, I use a combination of Duty (my task runner) and <code>make</code>.</p> <p>My <code>Makefile</code> declares the same tasks as the ones written in <code>duties.py</code>, but in a generic manner:</p> <pre><code>TASKS = check test release\n\n.PHONY: $(TASKS)\n$(TASKS):\n    @poetry run duty $@\n</code></pre> <p>So, instead of running <code>poetry run duty check</code>, I can run <code>make check</code>. Convenient, right?</p> <p>Except that some duties (tasks) accept arguments. For example:</p> <pre><code>poetry run duty release version=0.1.2\n</code></pre> <p>So how do I allow the <code>make</code> equivalent?</p> <pre><code>make release version=0.1.2\n</code></pre>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/#experiments","title":"Experiments","text":"<p>Do I write a specific rule in the Makefile for the <code>release</code> task?</p> <pre><code>TASKS = check test release\n\n.PHONY: release\nrelease:\n    @poetry run duty release version=$(version)\n\n.PHONY: $(TASKS)\n$(TASKS):\n    @poetry run duty $@\n</code></pre> <p>Meh. My Makefile rules are not generic anymore. Besides, what if the argument is optional? If I don't pass it when running <code>make</code>, the command will end up being <code>poetry run duty release version=</code>, which is just wrong.</p> <p>Instead, I'd like to find a generic way to insert the arguments in the command just as they are typed on the command line:</p> <pre><code>make release\n# =&gt; poetry run duty release\n</code></pre> <pre><code>make release version=0.1.2\n# =&gt; poetry run duty release version=0.1.2\n</code></pre> <p>Well, after a few hours playing with Makefiles features, I got a nice solution!</p>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/#sorcery","title":"Sorcery","text":"<p>Let me sprinkle this dark magic right here:</p> <pre><code>args = $(foreach a,$($(subst -,_,$1)_args),$(if $(value $a),$a=\"$($a)\"))\n\ncheck_args = files\ndocs_serve_args = host port\nrelease_args = version\ntest_args = match\n\nTASKS = \\\n    check \\\n    docs-serve \\\n    release \\\n    test\n\n.PHONY: $(TASKS)\n$(TASKS):\n    @poetry run duty $@ $(call args,$@)\n</code></pre> <p>What happens here?!</p>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/#recipe","title":"Recipe","text":"<pre><code>args = $(foreach a,$($(subst -,_,$1)_args),$(if $(value $a),$a=\"$($a)\"))\n</code></pre> <p>The heart of the magic. We declare a function called <code>args</code>. We later call it with <code>$(call args,$@)</code>.</p> <p>It could be described like this:</p> <pre><code>args reference := first parameter\nreplace - by _ in args reference\nappend \"_args\" to args reference\nget argument names by dereferencing args reference\nfor each argument name\n  get argument value by dereferencing argument name\n  if argument value is not empty\n    print \"argument name = argument value\"\n</code></pre> <p>This is why we declare our arguments like this:</p> <pre><code>check_args = files\ndocs_serve_args = host port\nrelease_args = version\ntest_args = match\n</code></pre> <p>When running <code>make docs-serve host=0.0.0.0</code>, the <code>args</code> function will do the following:</p> <pre><code>args_ref := \"docs-serve\"\nargs_ref becomes \"docs_serve\" (replace)\nargs_ref becomes \"docs_serve_args\"\nargs_names is value of \"docs_serve_args\" variable\nargs_names therefore is \"host port\"\narg \"host\":\n  variable \"host\" is not empty\n  print \"host=0.0.0.0\"\narg \"port\":\n  variable \"port\" is empty\n  print nothing\n</code></pre> <p>So when calling <code>$(call args,$@)</code>, <code>$@</code> is replaced by the rule name, which is <code>docs-serve</code> in this example, and <code>host=0.0.0.0</code> is added to the command.</p> <p>We successfully re-built the arguments passed on the command line!</p>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/#side-effects","title":"Side-effects","text":"<p>Arguments can be passed to <code>make</code> in no particular order. The following commands are all equivalent:</p> <pre><code>make hello world=earth foo bar=baz\nmake hello foo bar=baz world=earth\nmake bar=baz hello foo world=earth\n</code></pre> <p>It can be seen as an advantage but as an inconvenient as well, because you cannot have arguments with the same name for different commands. Or at least, you could not use these commands and arguments at the same time.</p> <pre><code>args = $(foreach a,$($(subst -,_,$1)_args),$(if $(value $a),$a=\"$($a)\"))\n\nrule1_args = version\nrule2_args = version name\n\nTASKS = rule1 rule2\n\n.PHONY: $(TASKS)\n$(TASKS):\n    @poetry run duty $@ $(call args,$@)\n</code></pre> <p>Here <code>rule1</code> and <code>rule2</code> both accept a <code>version</code> argument.</p> <pre><code>make rule1 version=1  # OK\n\nmake rule1 version=1 rule2  # not OK\n# it will result in \"poetry run duty rule1 version=1\"\n# then \"poetry run duty rule2 version=1\"!\n\nmake rule1 version=1 rule2 version=2  # ???\n# I couldn't get the courage to try more of these dark arts\n# so I don't know what would happen here...\n</code></pre>"},{"location":"posts/pass-makefile-args-as-typed-in-command-line/#addendum","title":"Addendum","text":"<p>Not a real addendum.</p> <p>Share your tricks!</p>"},{"location":"posts/plugins-as-python-native-namespace-packages/","title":"Python packages and plugins as namespace packages","text":"<p>A user of <code>mkdocstrings</code> wrote a Crystal handler for their own use-case. They asked on the Gitter channel if we could allow to load external handlers, so they don't have to fork the project and install the fork, but rather just install their lightweight package containing just the handler.</p> <p>We both saw Python namespace packages as good candidates, so I experimented a bit, and here are my conclusions.</p> <p>Based on the documentation, there are 3 ways of creating a Python namespace package:</p> <ul> <li>native namespace packages</li> <li>pkgutil-style namespace packages</li> <li>pkg-resources-style namespace packages</li> </ul> <p>I only considered the first two options (I'll let you read the docs to see why).</p>"},{"location":"posts/plugins-as-python-native-namespace-packages/#native-namespace-packages","title":"Native namespace packages","text":"<p>Native namespace packages are Python 3 compatible only. Good, I don't support Python 2 in my projects.</p> <p>To write a native namespace package, you just have to do one thing: drop the <code>__init__.py</code>!</p> <pre><code>my_namespace_package/\n    my_module_a.py\n\nmy_namespace_package/\n    my_module_b.py\n</code></pre> <p>Install both packages and you will be able to import both <code>my_module_a</code> and <code>my_module_b</code> from <code>my_namespace_package</code>.</p> <p>If you want to provide deeper modules, you will have to drop all <code>__init__.py</code> files along the way:</p> <pre><code># main package\npackage/\n    level1/\n        level2/\n            builtin_module.py\n    subpackage/\n        __init__.py\n        sub.py\n    a.py\n\n# plugin\npackage/\n    level1/\n        level2/\n            plugin_module.py\n</code></pre> <p>Notice how we don't have any <code>__init__.py</code> in <code>package</code>, <code>level1</code> and <code>level2</code>. This is required for the plugin package to add modules there.</p> <p>In this example, <code>subpackage</code> has an <code>__init__.py</code> file, it means that our plugin package could not add a module into it.</p>"},{"location":"posts/plugins-as-python-native-namespace-packages/#pkgutil-style-namespace-packages","title":"<code>pkgutil</code>-style namespace packages","text":"<p>The <code>pkgutil</code> way is compatible with Python 2 and Python 3. It allows to keep <code>__init__.py</code> modules.</p> <pre><code># main package\npackage/\n    __init__.py\n    level1/\n        __init__.py\n        level2/\n            __init__.py\n            builtin_module.py\n\n# plugin\npackage/\n    __init__.py\n    level1/\n        __init__.py\n        level2/\n            __init__.py\n            plugin_module.py\n</code></pre> <p>Each <code>__init__.py</code> must contain this code:</p> <pre><code>from pkgutil import extend_path\n\n__path__ = extend_path(__path__, __name__)\n</code></pre> <p>Basically, it's a bit like the native way, except that you keep your <code>__init__.py</code> modules, but they must be identical across all packages!</p> <p>The <code>__init__.py</code> modules of the plugin will overwrite the <code>__init__.py</code> modules of the main package. If they are not identical, things will break (mostly imports).</p> <p>So, with the native way, you cannot write code in <code>__init__.py</code> because those modules must not exist, and with the <code>pkgutil</code> way you must not write code in <code>__init__.py</code> because you don't want to have users duplicate it in their own.</p> <p>At this state, I prefer the native way, as it's less files to write.</p>"},{"location":"posts/plugins-as-python-native-namespace-packages/#what-i-would-have-liked","title":"What I would have liked","text":"<p>Maybe namespace packages are not the best option for plugins.</p> <p>But I would have loved being able to write my main package normally, with <code>__init__.py</code>, and as much code in those as I want, and write the plugin package without any <code>__init__.py</code>, so as to \"plug\" (i.e. merge) additional modules into the main package:</p> <pre><code># main package\npackage/\n    __init__.py\n    level1/\n        __init__.py\n        level2/\n            __init__.py\n            builtin_module.py\n\n# plugin\npackage/\n    level1/\n        level2/\n            plugin_module.py\n</code></pre> <p>The order of installation would not matter of course, as merging the main package into the plugin one, or the opposite, would result in the same final package on the disk:</p> <pre><code># main package\npackage/\n    __init__.py\n    level1/\n        __init__.py\n        level2/\n            __init__.py\n            builtin_module.py\n            plugin_module.py\n</code></pre> <p>The main package author doesn't even have to think about plugins and prepare the field by removing <code>__init__.py</code> where it is needed. Instead, they just write their package normally.</p> <p>Then users can write a namespace package, mimicking the main package structure, but without <code>__init__.py</code> files, and BOOM, they successfully wrote a plugin!</p> <p>Native namespace packages look like they are trying to be implicit, but in my taste they are still not implicit enough.</p> <p>Also, merging namespace packages into packages of the same name would allow easy patching of projects! Something not quite working like you want to? Quickly create a namespace package with just the patched module, and list it as a dependency.</p> <p>What do you use to allow plugins in your Python projects ?</p>"},{"location":"posts/python-static-code-analysis-tools/","title":"Python static code analysis tools","text":"<p>Python static code analysis is often used in Continuous Integration. I use it in every Python or Django package I develop, into a Tox configuration. The difficult part is to choose the right tools, because there are many, and to configure them correctly.</p> <p>Thankfully, there also are tools using other tools to reduce the amount of configuration we have to do. This ends up in a big mix of tools, which is represented in the following chart.</p> <p></p> <p>In this post, we will see the advantages and disadvantages of these tools, and compare them a bit.</p> <p>Chart realized with Draw.io.</p>"},{"location":"posts/python-static-code-analysis-tools/#the-aggregator-tools","title":"The \"aggregator\" tools","text":"<p>Some tools have a unique purpose, while others aim at doing more. This is the case of Prospector, Pylama, Pylint and Flake8.</p> <p>I see two types of aggregators here:</p> <ol> <li>the ones that do everything themselves (mostly): Pylint,</li> <li>and the ones that combine the result of many smaller tools: Prospector, Pylama and Flake8.</li> </ol> <p>What is funny here is that both Prospector and Pylama use not only tools used by Pylint, but Pylint as well! There is a bit of overlap here.</p>"},{"location":"posts/python-static-code-analysis-tools/#pylint","title":"Pylint","text":"<p>What I like in Pylint is its ability to rate your code, and keep a record of its evolution, telling you if it has improved or regressed between each run. I also love it's Similar tool, a one of its kind, used to output the similarities in your code. You can run it with <code>python -m pylint.checkers.similar FILES</code>. It's very fast, but will start to take some time with a lot of files.</p>"},{"location":"posts/python-static-code-analysis-tools/#pylama","title":"Pylama","text":"<p>Pylama seems not to have much activity since last year and has unresolved issues and open pull requests. At the time of writing, it's Pydocstyle integration is broken. I also tried using it from Python because I wanted to count the number of issues in some code, and was unsuccessful to do so. Still, I used Pylama for some time because I liked a lot how it can be configured, directly in <code>setup.cfg</code>. Its options are simple to use and simple to configure. I find ignoring warnings in Pylama much more easier than in Prospector, as they are all ignored on the same line, and are consistent between them (only names composed of a letter and a code). Pylama can also be deactivated in specific locations of your code with comments like <code># pylama:ignore=D105,D106</code>, and recognizes <code># noqa</code> comments.</p>"},{"location":"posts/python-static-code-analysis-tools/#prospector","title":"Prospector","text":"<p>Prospector is my favorite here: I used it for quite some time before switching to Pylama, and I'm back with it now. I was afraid that it was deperishing, because maintainers did not seem to answer pull requests and fix broken integration, but many fixes have been pushed now. I think it's a great project and it deserves more activity!</p> <p>What I like about Prospector is that it uses many tools to check many things, and it does it well: as I said earlier, there is some overlap between tools; Prospector handles that by keeping a record of similar warnings and only keep one of each to avoid redundancy in its reports! This is not done by default in Pylama, so you have to manually ignore redundant warnings in its configuration.</p> <p>Prospector also helped me discover tools I never heard of before, like Dodgy, Vulture, Pyroma and Frosted. I would like to see integrations for Radon (used by Pylama) as it offers the same thing as McCabe plus other stuff, as well as for Bandit which is a Python code security checker.</p> <p>What I miss in Prospector is the ability to configure it directly in <code>setup.cfg</code>, which I enjoyed a lot with Pylama.</p> <p>This is another story but I am trying hard to reduce the number of configuration files in my projects. I think tools should be able to find their configuration files into a specific location of your project, not only in the root directory, where configuration files pile up. I would be very happy to see some sort of standard for configuration files location rising, just as what EditorConfig is doing. Maybe a ConfigConfig standard? With a .configconfig file at the root of the repository telling where to find the configuraton files, and tools being able to read it, or by default search at the root or in the <code>config</code> folder?</p>"},{"location":"posts/python-static-code-analysis-tools/#flake8","title":"Flake8","text":"<p>I don't have much to say about Flake8 here. I did try it some time ago, but since Prospector does what it does, I won't use it. But it's only my personal choice and it can be a really good choice for others if they don't need to run all the possible code analysis tools in the world (unlike me :D).</p>"},{"location":"posts/python-static-code-analysis-tools/#the-do-one-thing-but-do-it-well-tools","title":"The \"do-one-thing-but-do-it-well\" tools","text":"<p>Work in progress, coming soon!</p> <p>Meanwhile, please share your thoughts! Did I say something wrong? Did I forget great tools? Did I write \"tools\" to much? I will gladly update the post with your sharings!</p>"},{"location":"posts/python-static-code-analysis-tools/#also-worth-noticing","title":"Also worth noticing","text":"<p>Go check Safety from PyUp.io, which check your requirements (dependencies) for known-vulnerabilities! Also Pyt for which I didn't take enough time to understand what it does and how it works, and great Code Quality web-services like Landscape.io and Codacy!</p>"},{"location":"posts/python-static-code-analysis-tools/#links","title":"Links:","text":"<ul> <li>Bandit: https://github.com/openstack/bandit</li> <li>Codacy: https://www.codacy.com/</li> <li>Dodgy: https://github.com/landscapeio/dodgy</li> <li>Draw.io: https://www.draw.io/</li> <li>Flake8: https://github.com/PyCQA/flake8</li> <li>Frosted: https://github.com/timothycrosley/frosted</li> <li>Landscape.io: https://landscape.io/</li> <li>McCabe: https://github.com/PyCQA/mccabe</li> <li>Prospector: https://github.com/landscapeio/prospector</li> <li>Pydocstyle: https://github.com/PyCQA/pydocstyle</li> <li>Pylama: https://github.com/klen/pylama</li> <li>Pylint: https://github.com/PyCQA/pylint</li> <li>Pyroma: https://github.com/regebro/pyroma</li> <li>Pyt: https://github.com/python-security/pyt</li> <li>PyUp.io: https://pyup.io/</li> <li>Radon: https://github.com/rubik/radon</li> <li>Safety: https://github.com/pyupio/safety</li> <li>Tox: https://github.com/tox-dev/tox</li> <li>Vulture: https://github.com/jendrikseipp/vulture</li> </ul>"},{"location":"posts/same-pytest-fixtures-with-different-scopes/","title":"Same Pytest fixtures with different scopes","text":"<p>How to use the same Pytest fixtures with different scopes, to prevent slowing down your test suite?</p> <p>I've recently started experimenting with mutiple parametrizations above tests, to run tests on every combination of possible values of different options. Concretely, I'm testing the output of a function that generates HTML. Since this function can potentially generate lots of HTML, I also use inline-snapshot to automatically store the output in external files and update my test code to reference them for the next runs. I then just have to check the generated HTML to assert it matches my expectations, and fix it if it doesn't.</p> <pre><code>import pytest\nfrom inline_snapshot import snapshot\n\n@pytest.mark.parametrize(\"option1\", [True, False, None])\n@pytest.mark.parametrize(\"option2\", [\"value1\", \"value2\"])\n@pytest.mark.parametrize(\"option3\", [(), (\"a\", \"b\")])\ndef test_options_combinations(option1, option2, option3):\n    assert generate_html(option1, option2, option3) == snapshot()\n</code></pre> <p>With such parametrization, I easily end up with hundreds of tests (they grow exponentially with each additional parametrization or option value). When the parametrized tests use function-scoped fixtures, they slow down the tests a lot, even if the fixtures are not very expensive. They're just executed over and over hundreds of time, so it starts being noticeable (one second, five seconds, fifteen seconds, etc.). These fixtures are function-scoped because they can be customized through Pytest's <code>request</code> fixture. But for the parametrized tests, they don't need customization. So I tried to duplicate the fixtures and make them session-scoped, but Pytest complains that they use function-scoped fixtures. After a bit of tinkering, here is what I ended up with:</p> <ul> <li>move fixtures' actual code into external helpers</li> <li>use these helpers in fixtures</li> <li>fixtures are now short and can easily be duplicated with different scopes</li> </ul>"},{"location":"posts/same-pytest-fixtures-with-different-scopes/#move-fixtures-actual-code-into-external-helpers","title":"Move fixtures' actual code into external helpers","text":"<p>Let's illustrate with code.</p> <p>Initially, I had my Pytest fixture declared like this:</p> <pre><code>\ud83d\udcc1 project/\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u2514\u2500\u2500 \ud83d\udcc4 conftest.py\n</code></pre> tests/conftest.py<pre><code>from collections import ChainMap\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport pytest\nfrom mkdocs.config.defaults import MkDocsConfig\n\n@pytest.fixture(name=\"mkdocs_conf\")\ndef fixture_mkdocs_conf(request: pytest.FixtureRequest, tmp_path: Path) -&gt; Iterator[MkDocsConfig]:\n    conf = MkDocsConfig()\n    while hasattr(request, \"_parent_request\") and hasattr(request._parent_request, \"_parent_request\"):\n        request = request._parent_request\n\n    conf_dict = {\n        \"site_name\": \"foo\",\n        \"site_url\": \"https://example.org/\",\n        \"site_dir\": str(tmp_path),\n        \"plugins\": [{\"mkdocstrings\": {\"default_handler\": \"python\"}}],\n        **getattr(request, \"param\", {}),\n    }\n\n    mdx_configs: dict[str, Any] = dict(ChainMap(*conf_dict.get(\"markdown_extensions\", [])))\n\n    conf.load_dict(conf_dict)\n    assert conf.validate() == ([], [])\n\n    conf[\"mdx_configs\"] = mdx_configs\n    conf[\"markdown_extensions\"].insert(0, \"toc\")  # Guaranteed to be added by MkDocs.\n\n    conf = conf[\"plugins\"][\"mkdocstrings\"].on_config(conf)\n    conf = conf[\"plugins\"][\"autorefs\"].on_config(conf)\n    yield conf\n    conf[\"plugins\"][\"mkdocstrings\"].on_post_build(conf)\n</code></pre> <p>The code is not very important. We just see here that the fixture is quite long. If I wanted to duplicate it and make it session-scoped (instead of function-scoped by default), I'd have to copy-paste all this code and the only thing that would change is the <code>tmp_path</code> dependency (function-scoped), which would become <code>tmp_path_factory</code> (session-scoped). That would be wasteful.</p> <p>Instead, I moved this code into a <code>tests/helpers.py</code> module:</p> <pre><code>\ud83d\udcc1 project/\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u251c\u2500\u2500 \ud83d\udcc4 conftest.py\n    \u2514\u2500\u2500 \ud83d\udcc4 helpers.py\n</code></pre> tests/helpers.py<pre><code>from collections import ChainMap\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport pytest\nfrom mkdocs.config.defaults import MkDocsConfig\n\n@contextmanager\ndef mkdocs_conf(request: pytest.FixtureRequest, tmp_path: Path) -&gt; Iterator[MkDocsConfig]:\n    conf = MkDocsConfig()\n    while hasattr(request, \"_parent_request\") and hasattr(request._parent_request, \"_parent_request\"):\n        request = request._parent_request\n\n    conf_dict = {\n        \"site_name\": \"foo\",\n        \"site_url\": \"https://example.org/\",\n        \"site_dir\": str(tmp_path),\n        \"plugins\": [{\"mkdocstrings\": {\"default_handler\": \"python\"}}],\n        **getattr(request, \"param\", {}),\n    }\n\n    mdx_configs: dict[str, Any] = dict(ChainMap(*conf_dict.get(\"markdown_extensions\", [])))\n\n    conf.load_dict(conf_dict)\n    assert conf.validate() == ([], [])\n\n    conf[\"mdx_configs\"] = mdx_configs\n    conf[\"markdown_extensions\"].insert(0, \"toc\")  # Guaranteed to be added by MkDocs.\n\n    conf = conf[\"plugins\"][\"mkdocstrings\"].on_config(conf)\n    conf = conf[\"plugins\"][\"autorefs\"].on_config(conf)\n    yield conf\n    conf[\"plugins\"][\"mkdocstrings\"].on_post_build(conf)\n</code></pre> <p>You'll notice that it's the exact same code, except it's now wrapped as a <code>@contextmanager</code> and the <code>fixture_</code> prefix is removed from its name. Why a context manager? We could let it be a generator, but that would mean we have to yield from it to make sure it tears down at the end:</p> <pre><code>yield from helpers.mkdocs_conf(...)\n</code></pre> <p>And we might not want to yield from it but rather just use it as argument to another function. The context manager allows us to do that:</p> <pre><code>with helpers.mkdocs_confg(...) as mkdocs_conf:\n    yield other_function(mkdocs_conf)\n</code></pre>"},{"location":"posts/same-pytest-fixtures-with-different-scopes/#use-these-helpers-in-fixtures","title":"Use these helpers in fixtures","text":"<p>Now let's rebuild our initial function-scoped fixture with this helper:</p> tests/conftest.py<pre><code>from collections import ChainMap\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport pytest\nfrom mkdocs.config.defaults import MkDocsConfig\n\n# The following import requires that you create a `tests/__init__.py` module.\nfrom tests import helpers\n\n\n@pytest.fixture(name=\"mkdocs_conf\")\ndef fixture_mkdocs_conf(request: pytest.FixtureRequest, tmp_path: Path) -&gt; Iterator[MkDocsConfig]:\n    with helpers.mkdocs_conf(request, tmp_path) as mkdocs_conf:\n        yield mkdocs_conf\n</code></pre> <p>Nice and short.</p>"},{"location":"posts/same-pytest-fixtures-with-different-scopes/#duplicate-fixtures-with-different-scopes","title":"Duplicate fixtures with different scopes","text":"<p>It is now extremely easy to duplicate the fixture as session-scoped:</p> <pre><code>@pytest.fixture(name=\"session_mkdocs_conf\", scope=\"session\")\ndef fixture_session_mkdocs_conf(request: pytest.FixtureRequest, tmp_path_factory: pytest.TempPathFactory) -&gt; Iterator[MkDocsConfig]:\n    with helpers.mkdocs_conf(request, tmp_path_factory.mktemp(\"project\")) as mkdocs_conf:\n        yield mkdocs_conf\n</code></pre> <p>We could imagine duplicating the fixture for each possible scope, which are function, class, module, package and session.</p> <p>Now it happens that I have other fixtures that depend on this <code>mkdocs_conf</code> fixture. They'll be just as easy to duplicate as session-scoped fixtures. Here is the complete code of my conftest module with both function and session-scoped fixtures. The code of the helpers is not important, so isn't shown here.</p> tests/conftest.py<pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Iterator\n\nimport pytest\n\nfrom tests import helpers\n\nif TYPE_CHECKING:\n    from pathlib import Path\n\n    from markdown.core import Markdown\n    from mkdocs.config.defaults import MkDocsConfig\n    from mkdocstrings.plugin import MkdocstringsPlugin\n\n    from mkdocstrings_handlers.python.handler import PythonHandler\n\n\n# --------------------------------------------\n# Function-scoped fixtures.\n# --------------------------------------------\n@pytest.fixture(name=\"mkdocs_conf\")\ndef fixture_mkdocs_conf(request: pytest.FixtureRequest, tmp_path: Path) -&gt; Iterator[MkDocsConfig]:\n    with helpers.mkdocs_conf(request, tmp_path) as mkdocs_conf:\n        yield mkdocs_conf\n\n\n@pytest.fixture(name=\"plugin\")\ndef fixture_plugin(mkdocs_conf: MkDocsConfig) -&gt; MkdocstringsPlugin:\n    return helpers.plugin(mkdocs_conf)\n\n\n@pytest.fixture(name=\"ext_markdown\")\ndef fixture_ext_markdown(mkdocs_conf: MkDocsConfig) -&gt; Markdown:\n    return helpers.ext_markdown(mkdocs_conf)\n\n\n@pytest.fixture(name=\"handler\")\ndef fixture_handler(plugin: MkdocstringsPlugin, ext_markdown: Markdown) -&gt; PythonHandler:\n    return helpers.handler(plugin, ext_markdown)\n\n\n# --------------------------------------------\n# Session-scoped fixtures.\n# --------------------------------------------\n@pytest.fixture(name=\"session_mkdocs_conf\", scope=\"session\")\ndef fixture_session_mkdocs_conf(request: pytest.FixtureRequest, tmp_path_factory: pytest.TempPathFactory) -&gt; Iterator[MkDocsConfig]:\n    with helpers.mkdocs_conf(request, tmp_path_factory.mktemp(\"project\")) as mkdocs_conf:\n        yield mkdocs_conf\n\n\n@pytest.fixture(name=\"session_plugin\", scope=\"session\")\ndef fixture_session_plugin(session_mkdocs_conf: MkDocsConfig) -&gt; MkdocstringsPlugin:\n    return helpers.plugin(session_mkdocs_conf)\n\n\n@pytest.fixture(name=\"session_ext_markdown\", scope=\"session\")\ndef fixture_session_ext_markdown(session_mkdocs_conf: MkDocsConfig) -&gt; Markdown:\n    return helpers.ext_markdown(session_mkdocs_conf)\n\n\n@pytest.fixture(name=\"session_handler\", scope=\"session\")\ndef fixture_session_handler(session_plugin: MkdocstringsPlugin, session_ext_markdown: Markdown) -&gt; PythonHandler:\n    return helpers.handler(session_plugin, session_ext_markdown)\n</code></pre> <p>Now in my tests I can either use <code>handler</code> or <code>session_handler</code>, to pick the right scope depending on the test's needs and how many times it runs through parametrizations.</p> <pre><code># This test customizes the MkDocs configuration,\n# and runs only 3 x 7 = 21 times,\n# so I use the function-scoped handler fixture.\n@pytest.mark.parametrize(\n    \"plugin\",\n    [\n        {\"theme\": \"mkdocs\"},\n        {\"theme\": \"readthedocs\"},\n        {\"theme\": {\"name\": \"material\"}},\n    ],\n    indirect=[\"plugin\"],\n)\n@pytest.mark.parametrize(\n    \"identifier\",\n    [\n        \"mkdocstrings.extension\",\n        \"mkdocstrings.inventory\",\n        \"mkdocstrings.loggers\",\n        \"mkdocstrings.plugin\",\n        \"mkdocstrings.handlers.base\",\n        \"mkdocstrings.handlers.rendering\",\n        \"mkdocstrings_handlers.python\",\n    ],\n)\ndef test_render_themes_templates_python(identifier: str, handler: PythonHandler) -&gt; None:\n    data = handler.collect(identifier, {})\n    handler.render(data, {})\n</code></pre> <pre><code>...\n\n# This test runs about 400 times with different options,\n# and doesn't need any customization of the MkDocs configuration,\n# so I use the session-scoped handler fixture.\n@pytest.mark.parametrize(\"inherited_members\", options[\"inherited_members\"])\n@pytest.mark.parametrize(\"members\", options[\"members\"])\n@pytest.mark.parametrize(\"members_order\", options[\"members_order\"])\n@pytest.mark.parametrize(\"filters\", options[\"filters\"])\n@pytest.mark.parametrize(\"summary\", options[\"summary\"])\ndef test_end_to_end_for_members(\n    session_handler: PythonHandler,\n    inherited_members: list[str] | bool | None,\n    members: list[str] | bool | None,\n    members_order: str,\n    filters: list[str] | None,\n    summary: bool | dict[str, bool] | None,\n) -&gt; None:\n    final_options = {**locals()}\n    html = _render(session_handler, final_options)\n    snapshot_key = tuple(sorted(final_options.items()))\n    assert outsource(html, suffix=\".html\") == snapshots_members[snapshot_key]\n</code></pre> <p>Do you have similar use-cases? How did you handle them? Maybe there's something obvious I missed about fixtures? Maybe there are Pytest plugins that make this easier?</p>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/","title":"Save the logs generated during a pytest run as a job artifact on GitLab/GitHub CI","text":"<p>While I was writing tests for one of my latest project, <code>aria2p</code>, I noticed that some tests that were passing on my local machine were now failing on the GitLab CI runner.</p> <p>I decided I needed to write all exceptions to log files so I could inspect what happened on GitLab.</p> <p>Since these tests were for the interactive interface of <code>aria2p</code>, using <code>asciimatics</code> and therefore an alternate buffer, I could not add <code>print</code> statements in the code to generate useful debug information that would be printed in the GitLab job logs.</p> <p>But I was already logging every exception! I just had to enable the logging functionality while running tests, and write them to the disk so I could create an artifact! The artifact could then be downloaded, and inspected in great details to see exactly what happened.</p> <p>What I use:</p> <ul> <li><code>loguru</code> as a logging utility. It's really easy to use, and I have to admit that I just like the colors </li> <li><code>pytest</code> to run the test suite. It's very powerful and has lots of nice plugins, like <code>pytest-cov</code>, <code>pytest-sugar</code> and <code>pytest-xdist</code>.</li> <li>GitLab CI as Continuous Integration service.</li> </ul>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#logging-with-loguru","title":"Logging with loguru","text":"<p>Let say you have an interactive function that accepts user input and execute code according to this input. Anything could happen, right? So you log every exception that might happen like this:</p> <pre><code>from loguru import logger\n\n\ndef run():\n    try:\n        ...  # actual code\n    except Exception as error:\n        logger.exception(error)\n</code></pre>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#testing-the-code","title":"Testing the code","text":"<p>Now in <code>pytest</code>, you would write a test function:</p> <pre><code>from my_package import run\n\n\ndef test_run():\n    interface = run()\n    # verify things afterwards\n    assert interface.state == 1\n</code></pre> <p>This code is not really important. What we're interested in is how we will write the logs of each test function into its own file on the disk.</p>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#writing-logs-for-each-test","title":"Writing logs for each test","text":"<p><code>pytest</code> allows you to run code before each test using fixtures. It's a bit more powerful than that (check the docs), but this is how we use it in this post.</p> <p>In your <code>tests/</code> directory, add a <code>conftest.py</code> file that <code>pytest</code> will automatically use.</p> <p>Our fixture is written like this:</p> <pre><code>from pathlib import Path\nfrom loguru import logger\nimport pytest\n\n\n@pytest.fixture(autouse=True)\ndef write_logs(request):\n    # put logs in tests/logs\n    log_path = Path(\"tests\") / \"logs\"\n\n    # tidy logs in subdirectories based on test module and class names\n    module = request.module\n    class_ = request.cls\n    name = request.node.name + \".log\"\n\n    if module:\n        log_path /= module.__name__.replace(\"tests.\", \"\")\n    if class_:\n        log_path /= class_.__name__\n\n    log_path.mkdir(parents=True, exist_ok=True)\n\n    # append last part of the name\n    log_path /= name\n\n    # enable the logger\n    logger.remove()\n    logger.configure(handlers=[{\"sink\": log_path, \"level\": \"TRACE\", \"mode\": \"w\"}])\n    logger.enable(\"my_package\")\n</code></pre> <p>Let's explain this code in details.</p> <ul> <li><code>@pytest.fixture(autouse=True)</code>: this decorator tells to always run the declared function before a test.</li> <li><code>def write_logs(request):</code>: the <code>request</code> parameter actually tells <code>pytest</code> to use its built-in <code>request</code> fixture! It allows to inspect the context of the current test function, the one that \"requested\" this fixture. We will retrieve the module, class and function name from the <code>request</code> object.</li> <li><code>log_path = Path(\"tests\") / \"logs\"</code>: <code>loguru</code> accepts File-like objects, isn't it nice? We will write our tests in the subdirectory \"tests/logs\".</li> <li>then we simply build the file path using the module name if it exists, the class name if it exists, and create the parents directory, ignoring errors if they already exists.</li> <li><code>logger.remove()</code>: <code>loguru</code> starts with a default handler. We remove it before adding our own.</li> <li>we set our handler using the <code>log_path</code> variable, the <code>TRACE</code> level (the lowest level to catch everything), and the mode <code>\"w\"</code>. The default mode is <code>\"a\"</code>, for append, but we want don't want to keep the logs of previous runs, so we overwrite them each time.</li> <li>finally, enable the logger for your package.</li> </ul> <p>We're set up.</p> <p>With this tests structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 test_cli.py\n\u2502  \u251c\u2500\u2500 def test_a\n\u2502  \u2514\u2500\u2500 def test_b\n\u2514\u2500\u2500 test_interface.py\n    \u251c\u2500\u2500 class TestFirstClass\n    \u2502  \u251c\u2500\u2500 def test_a\n    \u2502  \u2514\u2500\u2500 def test_b\n    \u2514\u2500\u2500 class TestSecondClass\n    \u2502  \u251c\u2500\u2500 def test_a\n    \u2502  \u2514\u2500\u2500 def test_b\n    \u2514\u2500\u2500 def test_c\n</code></pre> <p>You will have this logs tree:</p> <pre><code>tests/\n\u2514\u2500\u2500 logs/\n    \u251c\u2500\u2500 test_cli/\n    \u2502  \u251c\u2500\u2500 test_a.log\n    \u2502  \u2514\u2500\u2500 test_b.log\n    \u2514\u2500\u2500 test_interface/\n        \u251c\u2500\u2500 TestFirstClass/\n        \u2502  \u251c\u2500\u2500 test_a.log\n        \u2502  \u2514\u2500\u2500 test_b.log\n        \u2514\u2500\u2500 TestSecondClass/\n        \u2502  \u251c\u2500\u2500 test_a.log\n        \u2502  \u2514\u2500\u2500 test_b.log\n        \u2514\u2500\u2500 test_c.log\n</code></pre> <p>OK great! Now let's configure GitLab CI to use this directory as an artifact.</p>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#logs-artifact-on-gitlab-ci","title":"Logs artifact on GitLab CI","text":"<p>You'll typically want to download the logs when a job failed.</p> <p>Add this in your test job:</p> <pre><code>artifacts:\n  paths:\n    - tests/logs\n  when: on_failure\n  expire_in: 1 week\n</code></pre> <p>Now when a job fails, you'll be able to download or browse the logs:</p> <p></p> <p>Or even from the pipelines pages:</p> <p></p>"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#logs-artifact-in-github-workflow","title":"Logs artifact in GitHub Workflow","text":"<p>Same thing, but for a GitHub workflow:</p> <pre><code>jobs:\n\n  tests:\n\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python-version: [3.6, 3.7, 3.8, 3.9]\n\n    runs-on: ${{ matrix.os }}\n\n    # setup steps, etc.\n\n    - name: Run the test suite\n      run: poetry run duty test  # or your actual test command\n\n    - name: Archive tests logs\n      uses: actions/upload-artifact@v2\n      if: ${{ failure() }}\n      with:\n        name: tests-logs-${{ matrix.os }}-py${{ matrix.python-version }}\n        path: |\n          tests/logs\n        retention-days: 7\n</code></pre> <p>You'll be able to download the artifacts from the workflow page:</p> <p></p>"},{"location":"posts/somewhat-modern-python-development/","title":"Somewhat-modern Python","text":"<p>In this post I present my latest project template for Python projects, copier-pdm, based on the great Copier.</p> <p>This parodic title was inspired by the \"Hypermodern Python\" article series by Claudio Jolowicz. Indeed, my Python development setup has parts that could considered modern by some, and old-school by others, so I found it funny to call it \"more-or-less modern\".</p>"},{"location":"posts/somewhat-modern-python-development/#introduction","title":"Introduction","text":"<p>As Claudio points it out in his article series, the Python landscape has changed considerably this last decade, and even more so these last few years. Python is now one of the most used languages in the world, and we see a plethora of new projects coming up to life. Code formatters, linters, test runners, task runners, documentation generators, project managers, wheel builders and installers, dependency resolvers, CLI/TUI/API frameworks, static/runtime type checkers. It can be hard for new-comers to choose their tools and libraries, or even learn about their existence. Seasoned developers might see a lot of these new projects as new \"toys\" that will just be unmaintained one year from now. Sometimes, they are right. But I think it's important to keep an open-mind and try new fancy projects, even if they're not widely accepted/used by the community of Python developers (experts and beginners), or even create your own tools and libraries. Sometimes, it pays. Sometimes, you fill a niche and get progressive adoption.</p> <p>That's what I've been doing for a few years now. I'm far from being the most experienced Python developer, but I've grown to know the ecosytem quite well. I have my opinions, which not everybody will share. And I know (almost) exactly what I need as a Python developer. This is why I have created my own template for Python projects. It is very opinionated, and makes use of some of my own tools which are probably completely unknown to the community.</p> <p>This post presents all the tools and libraries used in my template, categorized and ordered from general aspects of project templating and Python projects, to very specific things like \"what do I want to see in the standard output of my task runner\":</p> <ul> <li>Project templating: creating a template, generating projects, updating generated projects.</li> <li>Project management: metadata, dependencies, building and publishing wheels.</li> <li>Running tasks: setting the project up, running quality checks or tests suites, etc.</li> <li>Documentation: writing docs/docstrings, automatic code reference, publishing.</li> <li>Continuous Integration: DRY configuration between local and CI</li> <li>Workflow: Git commit messages, automatic changelog generation, new releases.</li> </ul>"},{"location":"posts/somewhat-modern-python-development/#project-templating-copier","title":"Project templating: Copier","text":"<p>Copier is a project templating tool. It is able to generate a project from a given template, prompting the user for some information, and injecting this information into the generated files. A template is basically a git repository with templated files and folders, as well as a configuration file for Copier. Files ending with a <code>.jinja</code> extension will be rendered using Jinja. The Jinja context will be populated with the user's answers.</p> <pre><code>% copier gh:pawamoy/copier-pdm test-project\n\ud83c\udfa4 Your project name\n   Test Project\n\ud83c\udfa4 Your project description\n   This is just a test.\n\ud83c\udfa4 Your full name\n   Timoth\u00e9e Mazzucotelli\n\ud83c\udfa4 Your email\n   dev@pawamoy.fr\n\ud83c\udfa4 Your username (e.g. on GitHub)\n   pawamoy\n...\n</code></pre> <p>A template is very helpful when you create lots of new project: you avoid writing boilerplate code again and again.</p> <p>Now, Copier does not just allow to generate projects. It is also able to update generated projects when newer versions of templates are released.</p> Story time: from Cookiecutter to Copier <p>Most of Python developers probably know about CookieCutter. It's a project templating tool that allows to write language-and-technology-agnostic templates. Templates are basically a folder, with files and other folders in it, whose names and contents can be \"templated\", i.e. use variables, conditions and loops. The templating language used is Jinja2, another well known project.</p> <p>Despite its popularity, CookieCutter lacks a crucial feature: upstream-to-downstream updates. Let say you create your perfect Python project template with CookieCutter. You use it to generate 5 or 6 projects with it. With time, these 5 or 6 projects will start to diverge, and there's no integrated way into CookieCutter to prevent it. You will apply clever things from project N back into the template itself, but there's no way to apply these same modifications onto the other projects. It effectively makes your template \"single-use\" or \"one-time\" only, because once a project is generated, there's no way to keep it in sync with the template evolutions themselves. And this is something users are trying to achieve, see cookiecutter#784 and cookiecutter#1004.</p> <p>In fact, there is a way, and it's called Cruft (other alternatives exist but I feel like Cruft is the most popular). But Cruft is a wrapper around CookieCutter so it can only do so much to improve it. It does not have access to CookieCutter's internals, and therefore cannot provide a completely integrated, native project-updating experience (at least in my opinion, I will let you try it out if you are using CookieCutter, and let me know if I'm wrong ).</p> <p>This is where Copier shines: that feature is built-in, by design. One of Copier's main goals is to make it easy to update your already-generated projects when your template evolves. It means that, even when you manage dozens of projects that were generated using a single template, you can easily, almost automatically keep them in sync, while still allowing small differences here and there. I won't go into the details of how the update mechanism works (git operations, basically), but you can see a diagram and explanation here.</p> <p>To quote Copier's docs:</p> <p>Although Copier was born as a code scaffolding tool, it is today a code lifecycle management tool. This makes it somehow unique.</p>"},{"location":"posts/somewhat-modern-python-development/#creating-a-template","title":"Creating a template","text":"<p>To create a template using Copier, you have to know the basics of Jinja. Then you can simply read the Quickstart section of Copier's docs. This will give you an idea of what is possible. If you already know and use CookieCutter, you'll see that it's similar: you can put the templated stuff into a subdirectory of your git repository. One small difference is that the context is available globally rather than under the <code>cookiecutter</code> key.</p> <p>A minimal Copier template could look like this:</p> <pre><code>\ud83d\udcc1 template/\n\u251c\u2500\u2500 \ud83d\udcc1 project/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 README.md.jinja\n\u2514\u2500\u2500 \ud83d\udcc4 copier.yml\n</code></pre> template/copier.yml<pre><code>_subdirectory: project\n\nproject_name:\n  help: What is your project name?\n</code></pre> template/project/README.md.jinja<pre><code># {{ project_name }}\n</code></pre>"},{"location":"posts/somewhat-modern-python-development/#generating-a-project","title":"Generating a project","text":"<p>To get a feel of what it's like to generate a project from a real-life template, you can try to generate a project using my <code>copier-pdm</code> template. Just run this in a terminal:</p> <pre><code>pip install --user pipx\npipx run copier gh:pawamoy/copier-pdm newproject\n</code></pre> <p></p> <p>An important thing to know is that you should immediately stage and commit every generated file and folder, even if you want to delete some of them, because this will help later during updates.</p> <pre><code>cd newproject\ngit init\ngit add -A\ngit commit -m \"feat: Generate initial project\"\n# now modify and commit as much as you want!\n</code></pre>"},{"location":"posts/somewhat-modern-python-development/#updating-generated-projects","title":"Updating generated projects","text":"<p>Let say you just changed something in your template. All you need to be able to update your projects generated from this template, is to push a new tag, then go into your generated project(s) and run <code>copier -f update</code>. Copier will automatically apply the modifications to your project, while keeping the project's own changes intact. You can review the changes in your favorite tool (VSCode \"Source Control\" tab, <code>git add -p</code>, etc.), stage and commit. That's it!</p> <p>You can see all my \"template upgrade\" commits on GitHub with this search: \"chore: Template upgrade user:pawamoy\".</p> <p>For more information on the update process, see how the update works.</p>"},{"location":"posts/somewhat-modern-python-development/#project-management-pdm","title":"Project management: PDM","text":"<p>Now that we have templates and can generate projects, we must choose a project management tool. Traditional setups include <code>setuptools</code>' <code>setup.py</code> file, with some <code>MANIFEST.in</code> and <code>requirements.txt</code> files. We can spice things up with pip-tools or other alternatives. We can use modern, minimalist tools like Flit. Or we can use full-fledge, all-in-one managers like Hatch, Poetry, or PDM.</p> Story time: from Poetry to PDM <p>When the community came to learn about Poetry, it was a revolution: finally a tool that removed all the boilerplate and complexity around packaging and publishing Python projects. Everything was declared in <code>pyproject.toml</code>, and nothing else. You could manage your dependencies, virtualenvs being handled for you (great for beginners), you could build wheels and source distributions and publish them to PyPI with simple commands like <code>poetry build</code> and <code>poetry publish</code>. Poetry seriously improved the UX around project management and packaging in the Python ecosystem. I'll forever be thankful for that.</p> <p>But Poetry also took some annoying design decisions. They looked good at first, but were a huge hindrance for projects, depending on their environment. To make it short, I'm referring to this, \"Allow user to override PyPI URL without modifying pyproject.toml\", and other things like the inconsistencies in how to configure things / how the configuration is used, and the lack of option to disable SSL verification. I was maybe a bit thown-off by the complete absence of answer to my request for guidance in the PR I sent (working change, just hard to test), but I really can't blame them for this because they were literally assaulted with issues and PRs.</p> <p>Then I learned about PDM. It had all the good things Poetry had, and it removed the previously mentioned pain points, namely: it was able to read <code>pip</code>'s configuration. It made using private indexes so easy: setup <code>pip</code>'s index-url and trusted-host and you're done. Your own configuration does not contaminate other users through <code>pyproject.toml</code> (note: PDM 2.0 doesn't read <code>pip</code>'s configuration anymore, but you can still configure your index outside of <code>pyproject.toml</code>).</p> <p>Even more interesting, it offered support for the (draft) PEP 582. Since then, it gained several other outstanding features, like a pnpm-like cache, a powerful plugin system, development dependencies groups, etc. @noirbizarre posted a very nice comment about PDM, summarizing all these wonderful features. @frostming is doing a fantastic job at maintaining PDM.</p> <p>One of the main interests in using PDM is its ability to use PEP 582's <code>__pypackages__</code> installation folders. No more virtualenvs! Combined with its package cache, dependencies are installed insanely fast, and your environment won't break when you update your Python versions. Obviously, it also drastically reduces the disk-space taken by your dependencies.</p> <p></p> <p>If you still want to use virtualenvs, PDM natively supports them as well, and can manage them for you, just like Poetry.</p>"},{"location":"posts/somewhat-modern-python-development/#installing-and-using-pdm","title":"Installing and using PDM","text":"<p>My recommandation to install PDM is to use pipx:</p> <pre><code>python -m pip install --user pipx\npipx install pdm\n</code></pre> <p>I also like to enable PEP 582 support globally (docs), so that <code>python</code> is always aware of packages installed in a <code>__pypackages__</code> folder:</p> <pre><code>pdm --pep582 &gt;&gt; ~/.bash_profile\n</code></pre> <p>The commands written to <code>.bash_profile</code> simply add a subfolder of PDM to the <code>PYTHONPATH</code> environment variable. This folder contains a <code>sitecustomize.py</code> module, which Python executes upon starting. The module in turn will add subfolders of <code>__pypackages__</code> (when it exists) to <code>sys.path</code>. The end result is that you can run <code>python</code> and import dependencies normally, or even call modules with <code>python -m thing</code>, without having to use <code>pdm run python ...</code>. One limitation is that you still won't be able to call installed script directly, for example <code>pytest</code>, like in an activated virtual environment. For this, you'll have to run <code>pdm run pytest</code> (but remember that if packages support it, you can run <code>python -m pytest</code> instead).</p>"},{"location":"posts/somewhat-modern-python-development/#declaring-your-project-metadata","title":"Declaring your project metadata","text":"<p>PDM reads project metadata in <code>pyproject.toml</code>. It relies on PEP 517 (build system) and PEP 621 (project metadata):</p> pyproject.toml<pre><code>[build-system]\nrequires = [\"pdm-pep517\"]\nbuild-backend = \"pdm.pep517.api\"\n\n[project]\nname = \"package-name\"\ndescription = \"The package description.\"\nversion = \"0.1.0\"\nauthors = [{name = \"Timoth\u00e9e Mazzucotelli\", email = \"dev@pawamoy.fr\"}]\nlicense = \"ISC\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.7\"\nkeywords = [\"some\", \"keywords\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Typing :: Typed\",\n]\n</code></pre> <p>This example hardcodes the version but you can fetch it from SCM (Git tags).</p> <p>Projects using the <code>src</code> layout are supported:</p> pyproject.toml<pre><code>[tool.pdm.build]\npackage-dir = \"src\"\n</code></pre>"},{"location":"posts/somewhat-modern-python-development/#declaring-your-dependencies","title":"Declaring your dependencies","text":"<p>Dependencies and optional dependencies can be declared as specified by PEP 621:</p> pyproject.toml<pre><code>[project]\ndependencies = [\n   \"griffe&gt;0.22\",\n]\n\n[project.optional-dependencies]\ntest = [\n  \"pytest&gt;=7.1\",\n]\n</code></pre> <p>Note that this is not specific to PDM. Any tool that relies on PEP 621 will be able to read these dependencies.</p> <p>Development dependencies however are not specified by PEP 621, so the next part is specific to PDM and will not work with other dependencies managers. Development dependencies can be separated into multiple groups, just like optional dependencies:</p> pyproject.toml<pre><code>[tool.pdm.dev-dependencies]\ndocs = [\n    \"mkdocs&gt;=1.3\",\n    \"mkdocs-material&gt;=7.3\",\n    \"mkdocstrings[python]&gt;=0.18\",\n    \"markdown-exec&gt;=0.5\",\n]\ntests = [\n    \"pytest&gt;=6.2\",\n    \"pytest-cov&gt;=3.0\",\n    \"pytest-randomly&gt;=3.10\",\n    \"pytest-xdist&gt;=2.4\",\n]\n</code></pre> <p>When installing dependencies, you can then select all of them, or specific groups only:</p> <pre><code>pdm install  # installs everything\npdm install --prod  # installs production dependencies only\npdm install -G docs  # installs production dependencies and docs group\npdm install --no-default -G docs -G tests  # installs docs and tests groups only\n</code></pre>"},{"location":"posts/somewhat-modern-python-development/#building-and-publishing-wheels","title":"Building and publishing wheels","text":"<p>To build source and wheel distributions, simply run <code>pdm build</code>. To publish them, simply run <code>pdm publish</code>! Or use Twine.</p> <p>It is possible to customize the build process, see PDM's docs for more info.</p>"},{"location":"posts/somewhat-modern-python-development/#task-runner-duty","title":"Task runner: duty","text":"<p>At this point we are able to declare and install dependencies and development dependencies. We will want to use some of them to run quality analysis and execute test suites.</p> <p>PDM supports npm-like scripts, but I don't use them because I find them not expressive/powerful enough. I prefer writing my tasks with the full power of Python. That makes them more verbose, but way more flexible, and you can use code from dependencies directly instead of calling subprocesses.</p> <p>I tried many task runners and eventually ended up writing my own: duty.</p> Story time: from tox to Invoke to duty, and why not nox <p>A few years ago, when I was writing Django-apps, I was using tox to test my apps on multiple Django versions. I was happy with it, but found it a bit slow when running jobs on a P*D-sized matrix (Python versions, Django versions). Fortunately detox was able to run tox envs in parallel.</p> <p>At some point I stopped writing Django apps, and started searching an alternative to tox with a lighter output, because I really don't like seeing walls of text when a single line is enough (test passed? just print ok).</p> <p>I started playing with Invoke, but quickly had issues with it, notably in Windows runs on GitHub. I also felt limited in that I couldn't run Python code directly, only subprocesses. I tried to implement such a feature in Invoke, but couldn't manage to do it.</p> <p>I also played with Nox, but found it too slow as well, even when reusing virtualenvs. And the console output was again too heavy for my taste. Now that PDM supports Nox, I should definitely give it another try though!</p> <p>In the end, I wrote my own task runner, duty, which is heavily inspired by Invoke.</p> <p>With duty, tasks are declared by default in a <code>duties.py</code> file at the root of the repository. You import the decorator, and decorate functions with it:</p> duties.py<pre><code>from duty import duty\n\n\n@duty\ndef lint(ctx):\n    ctx.run(\"flake8 src\")\n</code></pre> <p>You can then run decorated function (duties) on the command line: <code>duty lint</code>.</p> <p>The <code>ctx</code> (for context) argument is injected by duty when running the task. Its <code>run</code> method accepts strings (shell commands), list of strings, and Python callables. Yes, you can run callables directly, no need for subprocesses. Depending on the library, it will be easy or hard to replace the subprocess with a direct call:</p> duties.py<pre><code>from duty import duty\n\n\n@duty\ndef lint(ctx):\n    from flake8.main.cli import main as flake8\n\n    ctx.run(flake8, args=[\"src\"])\n\n\n@duty\ndef docs(ctx):\n    from mkdocs.__main__ import cli as mkdocs\n\n    cli([\"build\"])\n</code></pre> <p>I urge Python developers to expose their CLI entrypoint outside of <code>__main__</code>, and to make it accept command line parameters. Otherwise we have to import from <code>__main__</code>, and/or patch <code>sys.argv</code>, which is ugly. Library first, CLI second!</p> <p>In the worst cases, you'll have to rebuild part of the CLI logic by copy/pasting code to make it work. For these cases, maybe it's easier to just run a subprocess.</p> <p>By default, duty will print the command that runs, and update the line with a check mark if it returns a success value (0 for subprocess, 0, None, True or truthy value for callables). If the command fails (non-zero, False or exception), duty will also print the captured output. Bye bye walls of text, hello very sexy output!</p> <p>Success:</p> <p></p> <p>Failure:</p> <p></p> <p>You can also output <code>tap</code>. Refer to duty's documentation for many more examples.</p>"},{"location":"posts/somewhat-modern-python-development/#project-setup-running-tasks","title":"Project setup, running tasks","text":"<p>When I clone one of my projects, I want to be able to set it up in one command. Setting it up is basically installing the dependencies. With PDM, that command would be <code>pdm install</code>. But that installs dependencies for only one Python version, and since I'm not using tox or nox, I need to install dependencies for every Python version I want to support.</p> <pre><code>for py in 3.7 3.8 3.9 3.10 3.11; do\n    pdm use -f python$py\n    pdm install\ndone\n</code></pre> <p>Again, I'm not using tox or nox, and I also need to be able to run some tasks on all supported Python versions. Typically tests, but quality analysis as well, because it needs to pass whatever the Python version is.</p> <pre><code>for py in 3.7 3.8 3.9 3.10 3.11; do\n    pdm use -f python$py\n    pdm run duty test\ndone\n</code></pre> <p>The end result is that I have two Bash scripts, <code>setup.sh</code> and <code>multirun.sh</code> in a scripts folder. Bash is usually available, even on Windows thanks to Git-for-Windows. However I don't want to run these scripts with <code>bash scripts/setup.sh</code> or <code>./scripts/multirun.sh duty test</code>. It's way too long to type. So I decided to also write a Makefile. Make is not easily available on Windows, so I lose a bit of cross-platformness, but it's just so convenient on GNU/Linux systems.</p> <p>Checkout my Makefile on GitHub.</p> <p>With this Makefile, I can simply type <code>make setup</code> after cloning a project. This action will try to install pipx if it's not available, and then it will try to install PDM with pipx if it's not available. Finally, it will run <code>pdm install</code> for a defined set of Python versions (at the time of writing: 3.7 up to 3.11).</p> <p>I can also type <code>make format</code>, or <code>make check</code>, or <code>make test</code>, and depending on the action, it will run it only once, using the Python version currently selected through PDM, or run it on all supported Python versions.</p> <p>If my duties (tasks) accept arguments, I can pass them from the command line: <code>make release version=1.0.0</code>, or <code>make test match=\"this or that\"</code>.</p> <p>Once I fixed something or added a new feature, and wrote tests for it, I can format my code, check its quality, run the tests, generate a coverage HTML report, generate a changelog entry, commit everything, build and publish a new release, and then publish docs with a single line:</p> <pre><code>make format check test coverage changelog release version=2.13.1\n</code></pre> <p>No time is lost installing dependencies for each task since I pre-installed my dependencies for every Python version, and all the output is unified and minimal, showing details only if something went wrong.</p> <p>Here's a real run that published dependenpy v3.3.2 on pypi.org:</p> <p></p> <p>The next sections will present each action (format, check, test, etc.) in details.</p>"},{"location":"posts/somewhat-modern-python-development/#quality-analysis","title":"Quality analysis","text":"<p>I typically split quality analysis in two: code quality and type-checking.</p> <p>For static typing analysis, I use mypy.</p> <p></p> dutymypy configOutput duties.py<pre><code>@duty\ndef check_types(ctx):\n    ctx.run(\"mypy --config-file config/mypy.ini src\", title=\"Type-checking\")\n</code></pre> <p>My configuration file is very basic and not very strict. It can probably be improved a lot.</p> config/mypy.ini<pre><code>[mypy]\nignore_missing_imports = true\nwarn_unused_ignores = true\nshow_error_codes = true\n</code></pre> <pre><code>$ make check-types\n&gt; Currently selected Python version: python3.10\n&gt; pdm run duty check-types (python3.7)\n\u2713 Type-checking\n&gt; pdm run duty check-types (python3.8)\n\u2713 Type-checking\n&gt; pdm run duty check-types (python3.9)\n\u2713 Type-checking\n&gt; pdm run duty check-types (python3.10)\n\u2713 Type-checking\n&gt; pdm run duty check-types (python3.11)\n\u2713 Type-checking\n&gt; Restored previous Python version: python3.10\n</code></pre> <p>For code quality analysis, I use Flake8 with a battery of plugins:</p> <ul> <li>darglint: docstrings linter that checks (missing) parameters, returns and exceptions in docstrings</li> <li>flake8-bandit: plugin that integrates Bandit</li> <li>flake8-black: plugin that checks if Black would make changes</li> <li>flake8-bugbear: \"finds likely bugs and design problems in your program\"</li> <li>flake8-builtins: checks that you don't shadow builtins with variables or parameters</li> <li>flake8-comprehensions: helps you write better list/set/dict comprehensions</li> <li>flake8-docstrings: plugin that integrates pydocstyle</li> <li>flake8-pytest-style: checks common style issues or inconsistencies with pytest-based tests</li> <li>flake8-string-format: checks formatted-strings and f-strings for errors</li> <li>flake8-tidy-imports: I use this one to forbid relative imports</li> <li>flake8-variables-names: helps to write more readable variables names</li> <li>pep8-naming: famous PEP8 checks</li> <li>wps-light: a lighter fork (no dependencies) of wemake-python-styleguide, a very strict and opinionated plugin for Flake8. You'll see that I disable a good part of its warnigns in my template, because they feel too cumbersome to me. The rest of the warnings can be annoying, but are very valuable to me: they definitely force you to write more readable and maintainable code.</li> </ul> dutyFlake8 configOutput duties.py<pre><code>@duty\ndef check_quality(ctx, files=\"src tests\"):\n    ctx.run(f\"flake8 --config=config/flake8.ini {files}\", title=\"Checking code quality\")\n</code></pre> config/flake8.ini<pre><code>[flake8]\nexclude = fixtures,site\nmax-line-length = 132\ndocstring-convention = google\nban-relative-imports = true\nignore =\n    ...\n</code></pre> <p>I truncated the file because it is a bit long, please check it out on GitHub.</p> <pre><code>$ make check-quality\n&gt; Currently selected Python version: python3.10\n&gt; pdm run duty check-types (python3.7)\n\u2713 Checking code quality\n&gt; pdm run duty check-types (python3.8)\n\u2713 Checking code quality\n&gt; pdm run duty check-types (python3.9)\n\u2713 Checking code quality\n&gt; pdm run duty check-types (python3.10)\n\u2713 Checking code quality\n&gt; pdm run duty check-types (python3.11)\n\u2713 Checking code quality\n&gt; Restored previous Python version: python3.10\n</code></pre>"},{"location":"posts/somewhat-modern-python-development/#code-formatters","title":"Code formatters","text":"<p>Code formatting is done in three steps:</p> <ol> <li>remove unused imports using autoflake</li> <li>sort imports using isort</li> <li>format code using Black</li> </ol> dutyBlack and isort configsOutput duties.py<pre><code>@duty\ndef format(ctx):\n    ctx.run(\n        \"autoflake -ir --exclude tests/fixtures --remove-all-unused-imports src/ tests/\",\n        title=\"Removing unused imports\",\n    )\n    ctx.run(\"isort src/ tests/\", title=\"Ordering imports\")\n    ctx.run(\"black src/ tests/\", title=\"Formatting code\")\n</code></pre> pyproject.toml<pre><code>[tool.black]\nline-length = 120\nexclude = \"tests/fixtures\"\n\n[tool.isort]\nline_length = 120\nnot_skip = \"__init__.py\"\nmulti_line_output = 3\nforce_single_line = false\nbalanced_wrapping = true\ndefault_section = \"THIRDPARTY\"\nknown_first_party = \"markdown_exec\"\ninclude_trailing_comma = true\n</code></pre> <pre><code>$ make format\n\u2713 Removing unused imports\n\u2713 Ordering imports\n\u2713 Formatting code\n</code></pre> <p>Idea: use ssort</p> <p>\\ I'm also looking at ssort, but it probably needs some enhancements before we're able to use it.</p>"},{"location":"posts/somewhat-modern-python-development/#security-analysis","title":"Security analysis","text":"<p>Security analysis is partly with safety, which queries a curated PyUP.io database to show dependencies that are vulnerable.</p> dutyOutput <p>The duty is a bit long here, because I try to ensure safety's modules are not patched by malicious packages installed in the current environment. Rebuilding the CLI logic also allows us to lighten up or even rework the output.</p> duties.py<pre><code>@duty\ndef check_dependencies(ctx):\n    # undo possible patching\n    # see https://github.com/pyupio/safety/issues/348\n    for module in sys.modules:  # noqa: WPS528\n        if module.startswith(\"safety.\") or module == \"safety\":\n            del sys.modules[module]  # noqa: WPS420\n\n    importlib.invalidate_caches()\n\n    # reload original, unpatched safety\n    from safety.formatter import SafetyFormatter\n    from safety.safety import calculate_remediations\n    from safety.safety import check as safety_check\n    from safety.util import read_requirements\n\n    # retrieve the list of dependencies\n    requirements = ctx.run(\n        [\"pdm\", \"export\", \"-f\", \"requirements\", \"--without-hashes\"],\n        title=\"Exporting dependencies as requirements\",\n        allow_overrides=False,\n    )\n\n    # check using safety as a library\n    def safety():\n        packages = list(read_requirements(StringIO(requirements)))\n        vulns, _ = safety_check(packages=packages, ignore_vulns=\"\")\n        output_report = SafetyFormatter(\"text\").render_vulnerabilities(\n            announcements=[],\n            vulnerabilities=vulns,\n            remediations=[],\n            full=True,\n            packages=packages,\n        )\n        if vulns:\n            print(output_report)\n            return False\n        return True\n\n    ctx.run(safety, title=\"Checking dependencies\")\n</code></pre> <pre><code>$ make check-dependencies\n\u2713 Exporting dependencies as requirements\n\u2717 Checking dependencies (1)\n  &gt; safety()\n  +==============================================================================+\n\n                                    /$$$$$$            /$$\n                                    /$$__  $$          | $$\n                /$$$$$$$  /$$$$$$ | $$  \\__//$$$$$$  /$$$$$$   /$$   /$$\n                /$$_____/ |____  $$| $$$$   /$$__  $$|_  $$_/  | $$  | $$\n              |  $$$$$$   /$$$$$$$| $$_/  | $$$$$$$$  | $$    | $$  | $$\n                \\____  $$ /$$__  $$| $$    | $$_____/  | $$ /$$| $$  | $$\n                /$$$$$$$/|  $$$$$$$| $$    |  $$$$$$$  |  $$$$/|  $$$$$$$\n              |_______/  \\_______/|__/     \\_______/   \\___/   \\____  $$\n                                                                /$$  | $$\n                                                              |  $$$$$$/\n        by pyup.io                                              \\______/\n\n  +==============================================================================+\n\n  REPORT \n\n    Safety v2.1.1 is scanning for Vulnerabilities...\n    Scanning dependencies in your environment:\n\n    -&gt; temp_file\n\n    Using non-commercial database\n    Found and scanned 118 packages\n    Timestamp 2022-09-04 14:18:55\n    3 vulnerabilities found\n    0 vulnerabilities ignored\n\n  +==============================================================================+\n  VULNERABILITIES FOUND\n  +==============================================================================+\n\n  -&gt; Vulnerability found in numpy version 1.21.1\n    Vulnerability ID: 44716\n    Affected spec: &lt;1.22.0\n    ADVISORY: Numpy 1.22.0 includes a fix for CVE-2021-41496: Buffer\n    overflow in the array_from_pyobj function of fortranobject.c, which allows\n    attackers to conduct a Denial of Service attacks by carefully constructing an\n    array with negative values. NOTE: The vendor does not agree this is a\n    vulnerability; the negative dimensions can only be created by an already\n    privileged user (or internally).https://github.com/numpy/numpy/issues/19000\n    CVE-2021-41496\n    For more information, please visit\n    https://pyup.io/vulnerabilities/CVE-2021-41496/44716/\n\n\n  -&gt; Vulnerability found in numpy version 1.21.1\n    Vulnerability ID: 44717\n    Affected spec: &lt;1.22.0\n    ADVISORY: Numpy 1.22.0 includes a fix for CVE-2021-34141: An\n    incomplete string comparison in the numpy.core component in NumPy before\n    1.22.0 allows attackers to trigger slightly incorrect copying by constructing\n    specific string objects. NOTE: the vendor states that this reported code\n    behavior is \"completely harmless.\"https://github.com/numpy/numpy/issues/18993\n    CVE-2021-34141\n    For more information, please visit\n    https://pyup.io/vulnerabilities/CVE-2021-34141/44717/\n\n\n  -&gt; Vulnerability found in numpy version 1.21.1\n    Vulnerability ID: 44715\n    Affected spec: &lt;1.22.2\n    ADVISORY: Numpy 1.22.2  includes a fix for CVE-2021-41495: Null\n    Pointer Dereference vulnerability exists in numpy.sort in NumPy in the\n    PyArray_DescrNew function due to missing return-value validation, which\n    allows attackers to conduct DoS attacks by repetitively creating sort arrays.\n    NOTE: While correct that validation is missing, an error can only occur due\n    to an exhaustion of memory. If the user can exhaust memory, they are already\n    privileged. Further, it should be practically impossible to construct an\n    attack which can target the memory exhaustion to occur at exactly this\n    place.https://github.com/numpy/numpy/issues/19038\n    CVE-2021-41495\n    For more information, please visit\n    https://pyup.io/vulnerabilities/CVE-2021-41495/44715/\n\n  Scan was completed. 3 vulnerabilities were found. \n\n  +==============================================================================+\n\nmake: *** [Makefile:49: check-dependencies] Error 1\n</code></pre> <p>I'm considering using/creating tooling around the OSV database instead.</p> <p>The code quality analysis, presented earlier, also checks the code for security risks using Bandit.</p>"},{"location":"posts/somewhat-modern-python-development/#test-suite","title":"Test suite","text":"<p>Tests are written and executed using pytest and plugins:</p> <ul> <li>sometimes pytest-asyncio, to test asynchronous code</li> <li>pytest-cov, which uses Coverage.py to build code coverage reports</li> <li>pytest-randomly, to randomize the order of tests, to make sure each test is independant from the others</li> <li>pytest-xdist, to run tests in parallel and therefore speed up the test runs</li> <li>sometimes hypothesis, to test huge ranges of input values</li> </ul> dutypytest configcoverage configOutput duties.py<pre><code>@duty(silent=True)\ndef coverage(ctx):\n    ctx.run(\"coverage combine\", nofail=True)\n    ctx.run(\"coverage report --rcfile=config/coverage.ini\", capture=False)\n    ctx.run(\"coverage html --rcfile=config/coverage.ini\")\n\n\n@duty\ndef test(ctx, match: str = \"\"):\n    py_version = f\"{sys.version_info.major}{sys.version_info.minor}\"\n    os.environ[\"COVERAGE_FILE\"] = f\".coverage.{py_version}\"\n    ctx.run(\n        [\"pytest\", \"-c\", \"config/pytest.ini\", \"-n\", \"auto\", \"-k\", match, \"tests\"],\n        title=\"Running tests\",\n    )\n</code></pre> config/pytest.ini<pre><code>[pytest]\nnorecursedirs =\n  .git\n  .tox\n  .env\n  dist\n  build\npython_files =\n  test_*.py\n  *_test.py\n  tests.py\naddopts =\n  --cov\n  --cov-config config/coverage.ini\ntestpaths =\n  tests\n</code></pre> config/coverage.ini<pre><code>[coverage:run]\nbranch = true\nparallel = true\nsource =\n  src/\n  tests/\n\n[coverage:paths]\nequivalent =\n  src/\n  __pypackages__/\n\n[coverage:report]\nprecision = 2\nomit =\n  src/*/__init__.py\n  src/*/__main__.py\n  tests/__init__.py\n</code></pre> <pre><code>$ make test coverage\n&gt; Currently selected Python version: python3.10\n&gt; pdm run duty test (python3.7)\n\u2713 Running tests\n&gt; pdm run duty test (python3.8)\n\u2713 Running tests\n&gt; pdm run duty test (python3.9)\n\u2713 Running tests\n&gt; pdm run duty test (python3.10)\n\u2713 Running tests\n&gt; pdm run duty test (python3.11)\n\u2713 Running tests\n&gt; Restored previous Python version: python3.10\nName                                       Stmts   Miss Branch BrPart     Cover\n-------------------------------------------------------------------------------\nsrc/markdown_exec/formatters/base.py          26      2      6      2    87.50%\nsrc/markdown_exec/formatters/bash.py          12      6      0      0    50.00%\nsrc/markdown_exec/formatters/console.py       33     23     10      0    23.26%\nsrc/markdown_exec/formatters/markdown.py       4      1      2      0    50.00%\nsrc/markdown_exec/formatters/pycon.py         33     23     10      0    23.26%\nsrc/markdown_exec/formatters/python.py        24      0      6      0   100.00%\nsrc/markdown_exec/formatters/sh.py            12      0      0      0   100.00%\nsrc/markdown_exec/formatters/tree.py          43      9     14      2    70.18%\nsrc/markdown_exec/logger.py                   20      4      4      0    75.00%\nsrc/markdown_exec/mkdocs_plugin.py            25     25      6      0     0.00%\nsrc/markdown_exec/rendering.py                82     32     38      5    54.17%\ntests/conftest.py                              7      0      2      0   100.00%\ntests/test_python.py                          17      0      0      0   100.00%\ntests/test_shell.py                           12      0      0      0   100.00%\ntests/test_tree.py                             5      0      0      0   100.00%\ntests/test_validator.py                        6      0      0      0   100.00%\n-------------------------------------------------------------------------------\nTOTAL                                        361    125     98      9    59.04%\n</code></pre> <p>The coverage action also generates an HTML report that can later be integrated into the docs.</p> <p>Idea: use ward</p> <p> I'm also looking at ward, but it feels a bit hard to leave pytest and its huge community.</p>"},{"location":"posts/somewhat-modern-python-development/#documentation-mkdocs","title":"Documentation: MkDocs","text":"<p>To write documentation for your project, I suggest using MkDocs. The well-known alternative is of course Sphinx. Sphinx is battle-tested and more targeted towards Python project documentation, but I've found MkDocs to be easier to work with.</p> <p>Combined to the great Material for MkDocs theme, it gives you a beautiful and responsive website for your documentation pages, or even for your blog. And combined to the mkdocstrings plugin, you can auto-generate documentation for your Python API (modules, functions, classes, methods, attributes, etc.).</p> <p> </p> Story time: from Sphinx to MkDocs <p>A few years ago (around 2020), I bumped into the Pydantic project (again). I had already seen it before, mainly interested in its settings configuration ability that could replace most of the logic in my Django app django-appsettings. This time, when I landed on its docs pages, I thought: \"wow, this looks nice\". Then, a bit later, FastAPI exploded in our faces, and I thought again: \"hey, looks familiar, and I love it!\". So I looked around and saw that it used MkDocs and a theme called \"Material for MkDocs\".</p> <p>I started migrating my projects to MkDocs and Material for MkDocs from Sphinx, because it looked nicer than the ReadTheDocs theme, and was far easier to use. Indeed, for the six previous years I had been using Sphinx for my projects documentation, and I never quite enjoyed writing docs with it. This was before the MyST parser or recommonmark (which I used a bit later), so I was writing my documentation in reStructuredText. I constantly had to check the syntax of rST and the Sphinx docs to achieve basic things. In particular, I wanted to have ToC (Table of Contents) entries in the sidebar for every documented module, class or function auto-generated with Sphinx's autodoc extension. I posted a question on StackOverflow and then found a feature request on Sphinx's bugtracker: the answer was \"it's not possible (yet)\".</p> <p>So I thought, hey, why not bring that in MkDocs instead of Sphinx? At the time, the only viable option for autodoc in MkDocs was Tom Christie's mkautodoc. Tom expressed his lack of capacity to work on the project, and I had an itch to scratch, so I decided to create my own MkDocs plugin for auto-documentation. This is how mkdocstrings was born. Tom's code has been very helpful at the beginning of the project (mkdocstrings' <code>:::</code> syntax actually comes from mkautodoc), so thanks Tom!</p> <p>Today we have a thriving MkDocs community and ecosystem, and I think it gave a nice push to the Sphinx community which, to me, seemed to stagnate a bit.</p> <p>With MkDocs, you write your documentation in Markdown files, inside a <code>docs</code> folder. Then you configure MkDocs in <code>mkdocs.yml</code>, at the root of your repository:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 docs/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 index.md\n\u2514\u2500\u2500 \ud83d\udcc4 mkdocs.yml\n</code></pre> ./mkdocs.yml<pre><code>site_name: My project documentation\ntheme:\n  name: material\nplugins:\n  - search\n  - mkdocstrings\n</code></pre> <p>With mkdocstrings, you can inject documentation of an object using the <code>::: identifier</code> syntax. For example, if you have a project like this:</p> <pre><code>\ud83d\udcc1 ./\n\u2514\u2500\u2500 \ud83d\udcc1 project/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u2514\u2500\u2500 \ud83d\udcc4 module.py\n</code></pre> <p>...and this class in <code>module.py</code>:</p> ./project/module.py<pre><code>\"\"\"This module is not very useful.\"\"\"\n\n\nclass AlmostEmptyClass:\n    \"\"\"This class doesn't allow to do a lot of things.\"\"\"\n\n    def __init__(self, something: str) -&gt; None:\n        \"\"\"Initialize an almost empty instance.\n\n        Parameters:\n            something: Pass something to the instance.\n        \"\"\"\n        self.thing: str = something\n</code></pre> <p>...you can inject documentation for your class like so, in a Markdown file, for example <code>index.md</code>:</p> ./docs/index.md<pre><code>::: project.module.AlmostEmptyClass\n</code></pre> <p>Or you could inject the whole module docs, recursively:</p> ./docs/index.md<pre><code>::: project.module\n</code></pre> <p>There's even a recipe to automatically document all your modules.</p> <p>If you want to learn in details how to build documentation with MkDocs, Material for MkDocs, and mkdocstrings, I can't recommend this guide enough, from Real Python: Build Your Python Project Documentation With MkDocs.</p> <p>Building the docs locally is as easy as running <code>mkdocs build</code>, and deploying them onto GitHub Pages is just a matter of running <code>mkdocs gh-deploy</code>.</p> <p>The <code>copier-pdm</code> template provides these actions/duties to check, build and deploy your docs:</p> <ul> <li><code>make docs</code>: build the docs locally, in the <code>site</code> directory</li> <li><code>make docs-serve</code>: serve the docs locally, at http://localhost:8000</li> <li><code>make docs-deploy</code>: deploy your docs on GitHub Pages</li> <li><code>make check-docs</code>: build docs in strict mode, failing at any warning or error</li> </ul>"},{"location":"posts/somewhat-modern-python-development/#continuous-integration-github-actions","title":"Continuous Integration: GitHub Actions","text":"<p>GitHub Actions allows to do continuous integration on most major operating systems: Linux, MacOS and Windows, which is very nice when you build a cross-platform tool or library.</p> <p>To follow the DRY principle, what is executed in CI can be executed locally, and reciprocally. Locally, we use <code>duty</code> to run the tasks, so we do the same in CI:</p> .github/workflows/ci.yml<pre><code>name: ci\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Set up PDM\n        uses: pdm-project/setup-pdm@v2.6 # (1)\n        with:\n          python-version: '3.8'\n\n      - name: Resolving dependencies\n        run: pdm lock\n\n      - name: Install dependencies\n        run: pdm install -G duty -G docs -G quality -G typing -G security # (2)\n\n      - name: Check if the documentation builds correctly\n        run: pdm run duty check-docs # (3)\n\n      - name: Check the code quality\n        run: pdm run duty check-quality\n\n      - name: Check if the code is correctly typed\n        run: pdm run duty check-types\n\n      - name: Check for vulnerabilities in dependencies\n        run: pdm run duty check-dependencies\n\n  tests:\n\n    strategy:\n      matrix:\n        os:\n          - ubuntu-latest\n          - macos-latest\n          - windows-latest\n        python-version:\n          - '3.7'\n          - '3.8'\n          - '3.9'\n          - '3.10'\n          - '3.11'\n\n    runs-on: ${{ matrix.os }}\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Set up PDM\n        uses: pdm-project/setup-pdm@v2.6\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: pdm install --no-editable -G duty -G tests # (4)\n\n      - name: Run the test suite\n        run: pdm run duty test # (5)\n</code></pre> <ol> <li>The PDM project provides this very convenient <code>setup-pdm</code> GitHub action.</li> <li>We install production dependencies and every necessary development dependency.</li> <li>And we run each check sequentially: docs, quality, types, security. That should be fast enough.</li> <li>We only install production dependencies and tests development dependencies. We tell PDM not to use an editable installation method, to be as close as a production environment as possible.</li> <li>We run the test suite.</li> </ol> <p>That's it. No special cases for CI. Whats runs up there can run down here with <code>make check test</code>.</p>"},{"location":"posts/somewhat-modern-python-development/#workflow-integrating-everything-together","title":"Workflow: integrating everything together","text":"<p>Now let see the typical workflow with all these tools integrated together.</p> <p>If you don't already have a local clone of your project, you clone it, enter it and install dependencies:</p> <pre><code>git clone ...\ncd ...\nmake setup\n</code></pre> <p>Then you can start working on a bugfix, feature or documentation improvement.</p> <pre><code>git switch -c fix-some-bug\n</code></pre> <p>For example, you add a test to your test suite to assert that it fails with the current code, and then run the test suite:</p> <pre><code># add a failing test, then\nmake test\n</code></pre> <p>It fails successfully on every Python version. You can now fix the code itself, and run the test suite again to make sure it's fixed for every Python version you support:</p> <pre><code># fix the code, then\nmake test\n</code></pre> <p>The test passes on every version, great. Make sure the code is well-formatted and there's no code smells:</p> <pre><code>make format check\n# fix any code smell\n</code></pre> <p>If you wrote or updated documentation, serve the site locally to check if everything renders correctly:</p> <pre><code>make docs-serve\n# go to localhost:8000\n</code></pre> <p>You are ready to commit. At some point you will want to release a new version, and therefore add a new entry in the changelog. You will be able to auto-generate such an entry thanks to git-changelog, but only if you follow the Angular commit-message convention, prefixing commit messages with a type like <code>feat</code> or <code>fix</code>.</p> <pre><code>git commit -am \"fix: Fix broken feature X\"\n</code></pre> <p>After some commits (and/or merged pull requests), and if you are ready to release a new version, you can update your changelog and see if the suggestion new version matches your expectations:</p> <pre><code>make changelog\n# review CHANGELOG.md\n</code></pre> <p>The new version is based on your latest Git tag.</p> <p>With a major version of <code>0</code>, fixes and code refactoring will increment the patch version by 1, and features will increment the minor version by 1. Breaking changes (detected using the body of the commit message) won't increment the major version.</p> <p>With a major version higher than 0, detected breaking changes will increment the major version.</p> <p>Changes since the last release will be nicely organized in sections such as \"Features\" and \"Bug fixes\".</p> <p>The changelog should only be updated when you are ready to publish a new release. Then, simply build and publish a new release of your project, providing the same version that was added to the changelog (you don't need to commit it):</p> <pre><code>make release version=1.2.3\n# check out your new release on pypi.org!\n</code></pre> <p>The changelog and release parts are explained a bit more in the Changelog and Releases sections of the template's documentation.</p>"},{"location":"posts/somewhat-modern-python-development/#conclusion","title":"Conclusion","text":"<p>What I like about this template and the projects generated with it, is that they are pretty much self-contained. You have everything you need at the tips of your fingers, and don't have to rely on particular online services to perform specific actions. Don't like GitHub Actions? Switch to something else! You'll always be able to run checks and tests locally, or publish a new release or a documentation update from your local machine.</p> <p>I hope you enjoyed the article, or at least learned one or two things. In any case, don't hesitate to leave a comment: I'll gladly fix any mistake I made, or elaborate if something isn't clear enough.</p>"},{"location":"posts/steam-linux-alvr-quest2/","title":"Playing Steam VR games running on Linux in the Oculus Quest 2 through ALVR (wireless)","text":"<p>A few days ago I was finally able to setup my Linux computer and Oculus Quest 2 to play some Steam VR games wirelessly. It tooks me several attempts over many months, and it was one of the most complicated process I had to discover and follow. So I figured I would share a small step-by-step guide to help others.</p> <p>Even though the process was painful for me at times, I would like to make it clear that the ALVR and SideQuest teams and their community have nothing to do with that: they are doing an amazing job and I'm very thankful for that.</p> <p>Update 2024-07-24</p> <p> Thanks to @patrickpluto who commented below, I was made aware that ALVR doesn't need to be compiled from sources and can instead be downloaded from GitHub releases, or installed from AUR as <code>alvr-bin</code>. Also, ALVR now provides an official app on the Meta Quest Store, so SideQuest is not needed anymore!</p>"},{"location":"posts/steam-linux-alvr-quest2/#preparation","title":"Preparation","text":"<ul> <li>I recommend doing this with a friend: you'll probably have to put the Quest 2 headset back and forth many times. It's easier if there's one person on the computer and another one wearing the headset. It's definitely doable alone though, just a bit more annoying.</li> <li>Make sure your headset and controllers are charged.</li> <li>Make sure your headset is up-to-date. Go to Settings, System, Updates, and install any available updates.</li> <li>Make sure your headset is connected to WiFi, and that your computer is connected to the same network, preferably over a wired connection. The Wifi emitter should broadcast on the 5GHz range. I don't know if this is mandatory or if other ranges work too (2.4GHz, 6GHz, etc.). I suppose it's just a matter of performance.</li> <li>Get a good USB-C cable. Preferably a long one: if wireless doesn't work in the end, you will at least have a long cable to allow you to move more freely when playing.</li> </ul> <p>At any step of the way, if something isn't working, it's probably a good idea to reboot.</p>"},{"location":"posts/steam-linux-alvr-quest2/#install-steamvr","title":"Install SteamVR","text":"<p>This one is easy. Open Steam, search for SteamVR in the store, and install it. It weighs 5GB so better start downloading now.</p> <p></p>"},{"location":"posts/steam-linux-alvr-quest2/#install-some-system-packages","title":"Install some system packages","text":"<p>ALVR will need some packages to allow the connection from your Quest 2.</p> <pre><code>yay -S pipewire pipewire-alsa pipewire-pulse\n</code></pre> <p>The last two packages will replace <code>pulseaudio</code> and <code>pulseaudio-alsa</code>. PipeWire seems to be a modern and promising solution, so I took the leap and replaced them. To make sure audio was still working, I rebooted the computer and played some audio files, as well as videos in the browser. All worked fine.</p> <p>ALVR also offers to configure your firewall to add rules allowing the Quest 2 client to connect. If you don't have a firewall like <code>ufw</code> installed, it's alright, you don't need it. ALVR will tell you it couldn't configure the firewall, but it shouldn't cause any issue: without a firewall the ports used by ALVR will be open. If you still want to install a setup a firewall, you can use <code>ufw</code>:</p> <pre><code>yay -S ufw\nsudo ufw enable\n</code></pre> <p>I think you will also need to install ADB, because if I understand correctly, the Oculus Quest 2 runs an Android system. I'm not sure because I have done all this many months ago.</p> <pre><code>yay -S adb\n</code></pre>"},{"location":"posts/steam-linux-alvr-quest2/#install-a-rust-toolchain","title":"Install a Rust toolchain","text":"<p>Update 2024-07-24</p> <p> ALVR doesn't need to be compiled from sources and can instead be downloaded from GitHub releases, or installed from AUR as <code>alvr-bin</code>.</p> <p>ALVR will be installed from sources, and it's written in Rust, so you'll need a Rust toolchain. Official guides recommend <code>rustup</code>:</p> <pre><code>yay -S rustup\nrustup update\n</code></pre> <p>Make sure to run <code>rustup update</code> if you already had <code>rustup</code> installed: compiling ALVR will require recent versions of <code>rustc</code>.</p>"},{"location":"posts/steam-linux-alvr-quest2/#install-alvr","title":"Install ALVR","text":"<p>Update 2024-07-24</p> <p> ALVR doesn't need to be compiled from sources and can instead be downloaded from GitHub releases, or installed from AUR as <code>alvr-bin</code>.</p> <p>Now you can install ALVR:</p> <pre><code>yay -S alvr\n</code></pre> <p>It takes a lot of RAM!</p> <p> Compiling ALVR took 13GB of RAM on my system. Make sure to close all other programs on your computer before installing ALVR. It can take quite some time too, even with a good CPU.</p>"},{"location":"posts/steam-linux-alvr-quest2/#setup-alvr","title":"Setup ALVR","text":"<p>Plug the headset</p> <p> At this point, I recommend plugging the headset onto the computer with the USB-C cable, and powering on the headset. I don't remember exactly when, but you'll get system popups in the headset asking you to allow USB data transfer, debug mode and other similar things. Also, it will let you know early if the USB-C cable is good enough for your headset to be detected by ALVR/SideQuest (months ago my headset was not detected and it probably was because my USB-C cable was not good enough).</p> <p>You should now have an <code>alvr_dashboard</code> command on your system, but before running it launch SteamVR once and close it. Official guides say it's needed to initialize SteamVR's configuration.</p> <p>Now run <code>alvr_dashboard</code>. It will run a setup wizard, asking you to:</p> <ul> <li>reset settings: click reset</li> <li>check your hardware: click next</li> <li>check software requirements: click download and set PipeWire audio (this should succeed!)</li> <li>configure hand gestures: click next</li> <li>configure firewall: click add firewall rules if you have a firewall, then click next</li> <li>read some information: click next</li> </ul> <p>ALVR and your system should now be configured correctly.</p> <p>On ALVR's Connections tab, you will see a warning that \"the streamer is not connected\". Simply click the \"Launch SteamVR\" button to run SteamVR from ALVR.</p> <p></p> <p>In the screenshot you see that I already have a trusted client. In your case, you shouldn't see any trusted client: don't worry, we will do that later.</p> <p>In the Installation tab, you'll see a button to \"register ALVR driver\". It is supposed to configure SteamVR so that launching it directly from Steam will also run ALVR alongside it. It does not work for me, so I always first open ALVR and then launch SteamVR from it.</p> <p></p> <p>Once SteamVR is running, you should see a small window and icons telling you that the headset is correctly detected:</p> <p></p>"},{"location":"posts/steam-linux-alvr-quest2/#install-sidequest","title":"Install SideQuest","text":"<p>Update 2024-07-24</p> <p> ALVR now provides an official app on the Meta Quest Store, so SideQuest is not needed anymore.</p> <p>SideQuest will allow you to \"sideload\" (upload) files onto your Quest 2. We will use it to sideload the SideQuest app on the headset. Why isn't there a SideQuest app installable in the Meta Quest Store? I don't know. Probably because the devs didn't have time or capacity to do that yet, or because of policy reasons, like Meta not allowing it </p> <ul> <li>Download the SideQuest \"easy installer\" here: https://sidequestvr.com/setup-howto.</li> <li>Go to your Downloads folder and extract the archive.</li> <li>Go into the extracted folder and run <code>./sidequestlite</code>.</li> </ul> <p></p> <p>SideQuest greets you with a button to open some setup instructions. Click on it to watch a video showing you stop steps to follow: enabling developer mode on the headset and installing ADB.</p> <p></p> <p>I did that months ago, so I don't remember if I had any issues (I think I didn't). The only annoying thing is that I had a mess of Meta, Facebook, Oculus and email accounts, and I had to ask support to merge my accounts. In these cases, support asks for the headset and controllers unique identifiers, and I had to watch a video on YouTube made by a random person to learn how to \"open\" the headset because the official docs were very poor on how to do this.</p> <p>Upon completing the setup, you will get popups in the headset asking you to authorize the connection: allow it and if asked, always remember your choice.</p> <p>Once you have completed the setup, the home tab of SideQuest should display an \"Install SideQuest in your headset\" button. Click on it, wait a bit. In my case, since it's already installed, it shows \"Reinstall SideQuest\".</p> <p></p> <p>Leave SideQuest opened on your compute, and head into the headset (pun intended).</p>"},{"location":"posts/steam-linux-alvr-quest2/#install-alvr-app-in-headset","title":"Install ALVR app in headset","text":"<p>Update 2024-07-24</p> <p> ALVR now provides an official app on the Meta Quest Store, so SideQuest is not needed anymore.</p> <p>In the headset, go to Applications, and filter with \"unknown sources\". You should see the SideQuest app. If you don't, go back to SideQuest on the computer and click on \"Reinstall\" or \"Update\". Do that until the app shows up in the headset.</p> <p>Once the SideQuest app shows up in your \"unknown sources\" applications, you can launch it, and close SideQuest on your computer.</p> <p>The SideQuest app will ask you to register your headset by going to https://sdq.st/link and entering the code displayed in your headset. I think you will need to create an account for that. Once you entered the code, a popup in the headset will ask you to accept the connection. Accept it.</p> <p>You should now be able to explore SideQuest's contents in your headset. Search for ALVR and install it. Why isn't there an ALVR app directly in the Meta Quest Store? I don't know, and it's probably because of the same reasons there isn't a SideQuest app either.</p> <p>Once installed, run the ALVR app in your headset. It will ask you to \"Trust\" the client on your computer, so go back to your computer, in the ALVR Dashboard, and click \"Trust\" in the Connections tab. If everything was configured correctly, the headset should now be linked and trusted. Back into the headset, the ALVR app should now bring you to a virtual room, displaying a board with all your Steam VR games.</p>"},{"location":"posts/steam-linux-alvr-quest2/#play-games","title":"Play games","text":"<p>Now each time you want to play VR games wirelessly in your Quest 2 headset, you'll have to:</p> <ul> <li>run <code>alvr_dashboard</code> on your computer</li> <li>launch SteamVR through it</li> <li>launch the ALVR app in your headset</li> <li>launch a Steam VR game from the virtual ALVR room</li> </ul> <p>You might get some issues, like some games detecting wrong controllers, preventing you to play, or audio not coming out of the headset.</p> <p>For the controllers, it might be possible to change some settings in SteamVR settings (click the menu icon in the small SteamVR window), but for some reason the settings window is bugged on my computer, so I can't configure anything there.</p> <p>For the audio issue, you can easily open <code>pavucontrol</code> to output audio on something else, like a sound system plugged on your computer. I'm not sure yet how to fix it for good and have audio come out of the headset, and even official guides say it's very experimental yet.</p> <p>In any case, make sure to check out ALVR's wiki, particularly any Troubleshooting page.</p> <p>Happy VR!</p>"},{"location":"posts/steam-linux-alvr-quest2/#further-notes","title":"Further notes","text":"<p>I recently learned about Steam Link. I told myself: \"amazing! finally something straightforward, I won't have to tinker with third-party stuff!\". I didn't even wonder if it would work on Linux, because, you know, it works over the network so if I can install it both on the computer and the headset, and Steam can run the game, then the OS doesn't matter, right?</p> <pre><code>flowchart LR\ns[Steam]\nslc[Steam Link]\nw[Wifi]\nslh[Steam Link]\nq[Quest 2]\n\ns --- slc --- w --- slh --- q</code></pre> <p>Well no, it doesn't work on Linux  And I find this very surprising given SteamOS itself is based on Linux.</p> <p>Or maybe it's only the Quest2 part that doesn't work? I have no idea. I'll be happy to update the post if someone knows this </p> <p>Anyway, I'll keep an eye on it!</p>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/","title":"Testing a FastAPI application using Ormar models and Alembic migrations","text":"<p>In the previous post I showed how to add Alembic migrations to an existing FastAPI + Ormar project. In this post we will see how to write unit tests for such applications.</p> <p>We start with the following project layout:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n</code></pre>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#database-models","title":"Database models","text":"<p>Let say we have three models: Artist, Album and Track. To keep things simple, we just add a <code>name</code> field on each.</p> src/project/models.py<pre><code>\"\"\"Database models.\"\"\"\n\nimport os\nimport databases\nimport ormar\nimport sqlalchemy\n\nSQLITE_DB = os.getenv(\"SQLITE_DB\", \"sqlite:///db.sqlite\")\nDB_PATH = SQLITE_DB.replace(\"sqlite:///\", \"\")\n\n\nclass BaseMeta(ormar.ModelMeta):\n    database = databases.Database(SQLITE_DB)\n    metadata = sqlalchemy.MetaData()\n\n\nclass Artist(ormar.Model):\n    class Meta(BaseMeta): ...\n\n    id: int = ormar.Integer(primary_key=True)\n    name: str = ormar.String(max_length=100)\n\n\nclass Album(ormar.Model):\n    class Meta(BaseMeta): ...\n\n    id: int = ormar.Integer(primary_key=True)\n    name: str = ormar.String(max_length=100)\n    artist: Artist = ormar.ForeignKey(Artist, nullable=False)\n\n\nclass Track(ormar.Model):\n    class Meta(BaseMeta): ...\n\n    id: int = ormar.Integer(primary_key=True)\n    name: str = ormar.String(max_length=100)\n    album: Album = ormar.ForeignKey(Album, nullable=False)\n</code></pre>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#database-and-migrations-helpers","title":"Database and migrations helpers","text":"<p>Now lets create helpers to easily (re)create, update (migrate) or stamp the database. We will put everything that is related to migrations in a migrations subpackage:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n</code></pre> <p>We will define helpers in the <code>__init__</code> module. Loguru will be used to log things, but that's optional and you can remove logging lines or use another logging framework.</p> src/project/migrations/__init__.py<pre><code>\"\"\"Database migrations modules.\"\"\"\n\nfrom functools import wraps\nfrom pathlib import Path\n\nimport sqlalchemy\nfrom alembic import command as alembic\nfrom alembic.config import Config\nfrom loguru import logger\n\nfrom project.models import DB_PATH, SQLITE_DB, BaseMeta\n\n\ndef get_alembic_config(db_url: str = SQLITE_DB) -&gt; Config:\n    alembic_cfg = Config()\n    alembic_cfg.set_main_option(\"script_location\", \"project:migrations\")\n    alembic_cfg.set_main_option(\"sqlalchemy.url\", str(db_url))\n    return alembic_cfg\n\n\ndef upgrade_database(revision: str = \"head\", db_url: str = SQLITE_DB) -&gt; None:\n    alembic_cfg = get_alembic_config(db_url)\n    alembic.upgrade(alembic_cfg, revision)\n\n\ndef stamp_database(revision: str = \"head\", db_url: str = SQLITE_DB) -&gt; None:\n    alembic_cfg = get_alembic_config(db_url)\n    alembic.stamp(alembic_cfg, revision)\n\n\ndef create_database(db_url: str = SQLITE_DB) -&gt; None:\n    engine = sqlalchemy.create_engine(db_url, connect_args={\"timeout\": 30})\n    BaseMeta.metadata.create_all(engine)\n\n\ndef db_lock(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        lock = Path(DB_PATH).parent / \".dblock\"\n        try:\n            lock.mkdir(parents=True, exist_ok=False)\n        except FileExistsError:\n            logger.debug(\"Migrations are already being applied\")\n            return\n        logger.debug(\"Applying migrations\")\n        try:\n            func(*args, **kwargs)\n        finally:\n            lock.rmdir()\n\n    return wrapper\n\n\n@db_lock\ndef apply_migrations(db_url: str = SQLITE_DB) -&gt; None:\n    if Path(DB_PATH).exists():\n        upgrade_database(db_url=db_url)\n    else:\n        create_database(db_url=db_url)\n        stamp_database(db_url=db_url)\n</code></pre> <p>Note how each function accepts a <code>db_url</code> parameter: this will be very useful to support different environments, such as development, production and testing.</p> <p>We still need the Alembic configuration module:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n</code></pre> src/project/migrations/env.py<pre><code>import os\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\n\nfrom project.models import BaseMeta\n\nconfig = context.config\ntarget_metadata = BaseMeta.metadata\n\n\ndef get_url():\n    # allow configuring the database URL / filepath using an env var, useful for production\n    return os.getenv(\"SQLITE_DB\", config.get_main_option(\"sqlalchemy.url\"))\n\n\ndef run_migrations_offline():\n    url = get_url()\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n        render_as_batch=True,  # needed for sqlite backend\n    )\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    configuration = config.get_section(config.config_ini_section)\n    configuration[\"sqlalchemy.url\"] = get_url()\n    connectable = engine_from_config(\n        configuration,\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            render_as_batch=True,  # needed for sqlite backend\n        )\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n</code></pre>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#automatic-database-creationupdate","title":"Automatic database creation/update","text":"<p>Now, lets configure our FastAPI app so that the database is automatically created or updated every time we run our app (using uvicorn for example):</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n</code></pre> src/project/app.py<pre><code>\"\"\"FastAPI application.\"\"\"\n\nfrom fastapi import FastAPI\n\nfrom project.migrations import apply_migrations\nfrom project.models import BaseMeta\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\nasync def startup() -&gt; None:\n    apply_migrations(str(BaseMeta.database.url))\n    if not BaseMeta.database.is_connected:\n        await BaseMeta.database.connect()\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown() -&gt; None:\n    if BaseMeta.database.is_connected:\n        await BaseMeta.database.disconnect()\n</code></pre> <p>Creating or updating the database in the <code>startup</code> event allows several things:</p> <ul> <li>in a development environment, developers can simply run the server, and the database is automatically created. They don't have to worry about running a database creation command. Similarly, they can simply delete the <code>db.sqlite</code> file and restart the server to empty their local database, or copy a pre-populated SQlite file to reset their local database to a particular state.</li> <li>in a production environment, in which you have no control (no possibility to run custom shell commands), migrations are applied automatically upon starting the server. If there are no (new) migrations, the startup event is a no-op: nothing happens and the server starts normally, with the current database untouched. Note that we ensure only one instance of the server will apply the migrations, as to prevent multiple parallel/concurrent accesses to a potentially shared storage space (for example multiple pods accessing the same persistent volume on a Kubernetes infrastructure).</li> <li>in a testing environment, tests will be able to provide a unique database URL (a local file path) so that they each have their own temporary database. It means tests will be able to run in parallel, for example using pytest-xdist.</li> </ul>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#pytest-fixture","title":"Pytest fixture","text":"<p>Now lets create a Pytest fixture that will allow each test to get access to its own unique, temporary database:</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u2514\u2500\u2500 \ud83d\udcc4 conftest.py\n</code></pre> tests/conftest.py<pre><code>\"\"\"Configuration for the pytest test suite.\"\"\"\n\nimport os\nfrom pathlib import Path\n\nimport databases\nimport pytest\nfrom asgi_lifespan import LifespanManager\nfrom httpx import AsyncClient\n\nfrom project.app import app\nfrom project.migrations import create_database, stamp_database\nfrom project.models import BaseMeta\n\n\n@pytest.fixture()\nasync def async_client(tmp_path: Path, monkeypatch):\n    \"\"\"\n    Provide an HTTPX asynchronous HTTP client, bound to an app using a unique, temporary database.\n\n    Arguments:\n        tmp_path: Pytest fixture: points to a temporary directory.\n        monkeypatch: Pytest fixture: allows to monkeypatch objects.\n\n    Yields:\n        An instance of AsyncClient using the FastAPI ASGI application.\n    \"\"\"\n    db_url = f\"sqlite:///{tmp_path}/db.sqlite\"\n    create_database(db_url=db_url)\n    stamp_database(db_url=db_url)\n    database = databases.Database(db_url)\n    monkeypatch.setattr(BaseMeta, \"database\", database)\n\n    lifespan = LifespanManager(app)\n    httpx_client = AsyncClient(app=app, base_url=\"http://testserver\")\n\n    async with httpx_client as client, lifespan:\n        yield client\n</code></pre> <p>You'll notice that we use asgi-lifespan. Without it, the startup and shutdown ASGI events would not be triggered.</p> <p>In the startup event, we apply migrations using the URL in <code>BaseMeta.database.url</code>. This allows us to monkeypatch the <code>database</code> attribute in our fixture to change the database URL for each test.</p>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#model-instances-factories","title":"Model instances factories","text":"<p>In our tests, we'll want to insert some rows in the database to test our API. Doing so manually can be cumbersome, as you have to define each instance one after the other, linking them together. To ease the process, we use factory-boy, with which we'll be able to define model factories. With these factories, it will be very easy to create instances of models in our tests.</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u251c\u2500\u2500 \ud83d\udcc4 conftest.py\n    \u2514\u2500\u2500 \ud83d\udcc4 factories.py\n</code></pre> tests/factories.py<pre><code>\"\"\"Factory classes to build models instances easily.\"\"\"\n\nimport factory\n\nfrom project import models\n\n\nclass ArtistFactory(factory.Factory):\n    class Meta:\n        model = models.Artist\n\n    id = 1\n    name = \"artist name\"\n\n\nclass AlbumFactory(factory.Factory):\n    class Meta:\n        model = models.Album\n\n    id = 1\n    name = \"album name\"\n    artist = factory.SubFactory(ArtistFactory)\n\n\nclass TrackFactory(factory.Factory):\n    class Meta:\n        model = models.Track\n\n    id = 1\n    name = \"track name\"\n    album = factory.SubFactory(AlbumFactory)\n</code></pre> <p>With these factories you can now create an artist, album and track, all linked together, using a single line of code:</p> <pre><code>from tests import factories\n\ntrack = factories.TrackFactory()\n</code></pre> <p>You can change arbitrary attributes when creating instances:</p> <pre><code>track = factories.TrackFactory(\n    name=\"other track name\",\n    album__name=\"other album name\",\n    album__artist__name=\"other artist name\",\n)\n</code></pre> <p>Refer to factory-boy's documentation for more examples. You could also use Faker to set more relevant default values to your instances attributes.</p>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#populating-the-database-with-data","title":"Populating the database with data","text":"<p>Creating instances is nice, but they are not magically inserted in the database for us. Since instances are Ormar model instances, we could technically use the <code>save()</code> method on the instances we create to save them in the database, however I did not try that and cannot guarantee it will work for multiple instances linked together at once.</p> <p>Instead, and only if you have added CRUD operations to your API, you can call your API routes to create the instances in the database.</p> <p>For this, I chose to create a new helper module, but that's probably not the best design you can come up with, so feel free to discard the next suggestions and follow your instincts.</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u251c\u2500\u2500 \ud83d\udcc4 conftest.py\n    \u251c\u2500\u2500 \ud83d\udcc4 factories.py\n    \u2514\u2500\u2500 \ud83d\udcc4 helpers.py\n</code></pre> tests/helpers.py<pre><code>\"\"\"Helpers for tests.\"\"\"\n\nfrom project.models import Artist, Album, Track\nfrom tests import factories\n\n\nasync def create_artist(client) -&gt; Artist:\n    artist = factories.ArtistFactory()\n\n    payload = artist.dict()\n    response = await client.post(\"/artists\", json=payload)\n    response.raise_for_status()\n\n    return artist\n\n\nasync def create_album(client) -&gt; Album:\n    album = factories.AlbumFactory()\n\n    # create artist first\n    payload = album.artist.dict()\n    response = await client.post(\"/artists\", json=payload)\n    response.raise_for_status()\n\n    # then create album\n    payload = album.dict()\n    response = await client.post(\"/albums\", json=payload)\n    response.raise_for_status()\n\n    return album\n\n\nasync def create_track(client) -&gt; Track:\n    track = factories.TrackFactory()\n\n    # create artist first\n    payload = track.album.artist.dict()\n    response = await client.post(\"/artists\", json=payload)\n    response.raise_for_status()\n\n    # then create album\n    payload = track.album.dict()\n    response = await client.post(\"/albums\", json=payload)\n    response.raise_for_status()\n\n    # finally create track\n    payload = track.dict()\n    response = await client.post(\"/tracks\", json=payload)\n    response.raise_for_status()\n\n    return track\n</code></pre>"},{"location":"posts/testing-fastapi-ormar-alembic-apps/#example-tests","title":"Example tests","text":"<p>Now you can easily populate the database in tests, and call other API routes to test their behavior and output.</p> <pre><code>\ud83d\udcc1 ./\n\u251c\u2500\u2500 \ud83d\udcc1 src/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 project/\n\u2502       \u251c\u2500\u2500 \ud83d\udcc1 migrations/\n\u2502       \u2502   \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 env.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n\u2502       \u251c\u2500\u2500 \ud83d\udcc4 app.py\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 models.py\n\u2514\u2500\u2500 \ud83d\udcc1 tests/\n    \u251c\u2500\u2500 \ud83d\udcc4 __init__.py\n    \u251c\u2500\u2500 \ud83d\udcc4 conftest.py\n    \u251c\u2500\u2500 \ud83d\udcc4 factories.py\n    \u251c\u2500\u2500 \ud83d\udcc4 helpers.py\n    \u2514\u2500\u2500 \ud83d\udcc4 test_tracks.py\n</code></pre> tests/test_tracks.py<pre><code>\"\"\"Tests for the `tracks` routes.\"\"\"\n\nimport pytest\n\nfrom tests import factories, helpers\n\n\n@pytest.mark.asyncio()\nasync def test_tracks_create(async_client):\n    track = await helpers.create_track(async_client)\n    # ...then test other API routes\n</code></pre> <p>Note how we use the previously defined fixture <code>async_client</code>. Just adding that fixture as a parameter to our test function ensures we have a temporary, dedicated database for this test.</p>"},{"location":"posts/the-insiders-journey/","title":"The Insiders journey","text":"<p>This post will embark you on my journey about making a living off my open source projects. I'm still at the beginning of the journey, but there is already a lot to say and I wanted to share my experience. I like the idea of keeping a journal of what I go through, and who knows, maybe it will help or inspire others.</p> <p>In this post I write a bit about me, but I also provide technical instructions on how to set things up.</p>"},{"location":"posts/the-insiders-journey/#a-bit-about-me","title":"A bit about me","text":"<p>I was born and live in France. Not that it's super important, but it definitely played a role in my ability to become who I am today.</p> <p>I went to university to learn computer science, for a total of six years (should have been five if I studied more seriously). Each year cost around 400 euros to my parents. I lived at their place, and they paid for everything, even during the last two years when I had a small income thanks to the apprenticeship. Merci maman, merci papa </p> <p>Once I got my master's degree, I was hired by the same people I did my apprenticeship with, and worked two more years with them (so a total of four years). This was my very first job as a developer. I was the only developer on my team, probably the only one on the whole site (hundreds of people). I was working with researchers, building a website for families and professionals, and had the \"full-stack\" hat on: Python, Django, Docker, Linux, PostgreSQL, HTML/CSS/JS, Bootstrap, Highcharts, GitLab, to name a few tools and technologies. A super interesting and fulfilling project, but hard. I eventually burnt out from working alone without experience or guidance, so decided to quit.</p> <p>For a year, I worked on my open-source projects, still living at my parents' place. It was great but I didn't have any income, and I knew that at some point I would need to get a job if I wanted to leave the nest. Lucky me, someone I knew offered me an interview at the company they worked for, and just like that, I got a job that I didn't even search for. My open-source work and experience helped: the people interviewing me on the Python side of things were convinced of my skills just by looking at my projects on GitHub. I stayed three years at this company (the length of the mission at the client), discovered what it was to actually work with other developers and engineers, made very good friends  and eventually left for another project at another company which offered a higher salary and four-day weeks. I wanted to free some time to continue working on my open-source projects, some of which had gained a lot more popularity (mkdocstrings).</p> <p>After just three months at the new company, I quit again because I couldn't stand working for someone else anymore, on projects that didn't really interest me, thinking all the time of my own projects while not being able to work on them. I decided I would try to make a living thanks to my projects, and this is where my journey begins </p>"},{"location":"posts/the-insiders-journey/#the-sponsorware-strategy","title":"The sponsorware strategy","text":"<p>For some time I had been following the progress of Martin Donath, @squidfunk on GitHub, and his Material for MkDocs project. Material for MkDocs is a framework for MkDocs (a static site generator or SSG) with awesome features and visuals. The project follows a \"sponsorware strategy\", where Martin's sponsors get exclusive access to some nifty features.</p> <p>I was amazed by how well it was working for Martin, and decided I would reach out to him to ask a few questions. I wanted to try and start a similar strategy for mkdocstrings, an autodoc plugin for MkDocs. Martin actually reached out first, to thank me for becoming his sponsor:</p> <p></p> <p>This was the start of a conversation that continues to this day, where Martin is extremely friendly and helpful, answering my questions, providing me with advice, sharing his own experience. Vielen Dank Martin </p> <p>Thanks to Martin I was able to start my own sponsorware strategy when I felt ready for it, one year after the start of our conversation. It's hard to quantify, but relying on a tried and proven model probably saved me months of work, if not years, to get things right and rolling.</p> <p>So, what is this sponsorware strategy exactly?</p> <p>For the long, up-to-date explanations, see Material for MkDocs Insiders page and my own. You will notice (unless they changed drastically since the time of writing) that my page is heavily (completely) based on Martin's, with his blessing.</p> <p>For a shorter explanation: new features are available to sponsors only, and when reaching a funding goal (in dollars per month), a specific set of features becomes free. For example, when the maintainer(s) reach $1000/month, features A, B and C become available to everyone (not just sponsors). When they reach $2000/month, features D, E and F become available to everyone. At the same time, funding from sponsorships actually helps developing the aforementioned features, as well as the next ones.</p> <p>I think we could call this a win-win-win: the maintainers are paid for their work (and work on projects they like), the sponsors get early access to nice features and ensure the growth (features) and reliability of the project (security/bug fixes), and the rest of the community eventually gets those same features for free.</p> <p>You could ask: \"If people sponsor you, why not releasing features to everyone immediately?\" The answer is in the question: unfortunately, people, and especially companies, seem to sponsor only if they are incentivized to do so. Before I set up my sponsorware strategy, I had my GitHub sponsor profile active for two years, and reached $35/month maximum, from individuals exclusively (no company). The landscape is evolving, as well as the mentalities, but we're not there yet. It seems few companies are willing to fund the open-source projects they use, mainly because they are used to pay in exchange of a service. If they can get this service for free, why paying? And when paying, they want strong insurances, contracted support, etc., which open-source maintainers cannot generally provide. This is obviously a simplistic view, and the reality is more complicated, but I wanted to emphasize that users and companies need incentive to fund your work, and this incentive is achieved through sponsors-only features or contents.</p> <p>To summarize:</p> <ul> <li> a sponsorware strategy makes it possible for users and companies to fund your projects </li> <li> they get immediate access to projects or features that are available to sponsors only </li> <li> upon reaching funding goals, these features become available to everyone </li> </ul>"},{"location":"posts/the-insiders-journey/#platforms-and-tiers","title":"Platforms and tiers","text":"<p>There are many platforms that make it possible to get funding for open-source projects. To name a few: GitHub sponsors (integrated with Stripe), Open Collective, Ko-fi, Patreon, Liberapay, etc.</p> <p>You are looking for one that allows to set up different tiers, with possibly different rewards. For example, a $5/month tier that grants access to a single private repository, and a $15/month tier that grants access to all your private repositories. In my case, individuals get access to everything for $10/month, and organizations get the same thing for $50/month.</p> <p>It is recommended to set low/medium tiers for individuals, and higher tiers for companies and organizations, as they have more funding power. You can offer things like logo placement in your README or documentation pages for organizations, as an incentive to select higher tiers. Sometimes companies do not have budget for open-source funding, but they do have budget for marketing, and logo placement is marketing, so offering it can be the decisive factor for a company to sponsor you! In my case I added $100/month and $200/month tiers, for Bronze  and Silver  sponsors, offering higher logo placement in the docs, and will probably add a Gold  tier later. You can see how tiers look on my GitHub sponsors page.</p> <p>You are also looking for a platform that will allow automation, for example to grant access to private repositories when new sponsorships are started.</p> <p>You are not limited to a single platform: feel free to create accounts on multiple ones. Initially, I created accounts on all these platforms, trying to maximize chances to get new sponsors. That can work, but it represents a lot more work, as they don't all work the same way, or provide the same APIs (JSON, GraphQL, webhooks, etc.). In the end, GitHub sponsors is my main driver, because my private repositories are also hosted on GitHub (more on that in the next section), making it super easy to automate granting/revoking access to/from users. Since GitHub removed the integration with PayPal (which a lot of users rely on for sponsorship payments because they don't have credit cards), I put my Ko-fi account back up for PayPal users, just in case. Recently, GitHub and Patreon started working together, so it might a good idea to have a Patreon account. In August 2023, Bloomberg gave $10,000 to MkDocs, and they used Open Collective to send the money, so my account there was also helpful (I received $1,000).</p> <p>As long as the platforms you choose allow you to automate users access to your private repositories, or at least to quickly react and manually add/remove users, it will cover 95% of your needs.</p> <p>To summarize:</p> <ul> <li> create an account on one or more platforms handling sponsorships </li> <li> set up multiple tiers (for individuals, for organizations) </li> <li> add incentive to select higher tiers </li> <li> prepare for some automation work </li> </ul>"},{"location":"posts/the-insiders-journey/#setting-up-an-insiders-program","title":"Setting up an Insiders program","text":"<p>Once you and your projects are ready to follow a sponsorware strategy, the next thing you can do is pick a name for the sponsors-exclusive projects or features. I went with Insiders (pawamoy Insiders, mkdocstrings Insiders), once again inspired by Martin's own program, but you could use another common name like Extras, Premium, Plus, or a fancier one like Ultimate, Orichalcum or Diamond. For the rest of the post, I'll refer to \"Insiders\" or \"Insiders projects\". You can also pick a theme for giving names to your funding goals. For example Martin names them after chili peppers (he's growing them on his balcony!), like Piri Piri, Carolina Reaper, etc. Most of my projects being documentation or development tools, they make me think of the paper manuals you get when you buy furniture, home appliance, or any kind of devices really, so I went a \"futuristic home appliance manuals\" theme, and asked ChatGPT to invent a few names for me  GraviFridge User Manual, HyperLamp Quick Start Guide, etc. Initially I wanted to use a \"musical genre\" theme, like Brutal Black, Nintendo Chill, Clown Core , or Lovecraftian Dark Ambient, but it felt both too vague and self-centered, while not being related enough to my actual projects.</p>"},{"location":"posts/the-insiders-journey/#repositories","title":"Repositories","text":"<p>Each one of your project following the sponsorware strategy will exist in two versions: the community version, available to everyone, and the Insiders version, available to sponsors only. That means two Git repositories per project.</p> <p>Through my own Insiders program, sponsors get access to all my Insiders projects. To facilitate granting access to all the repositories at once, I created a dedicated organization on GitHub called <code>pawamoy-insiders</code> (<code>pawamoy</code> is my username). Inside of that organization, I created a team. This team serves as a bridge between projects and users in the organization. It lets me choose which repositories users can access, meaning I can still have completely private repositories in that organization, that no one will see until I deem them ready. Sponsors added to the team gain access to all of the team's repositories at once.</p> <pre><code>flowchart TB\nuser[\"user\"]\nteam[\"insiders (team)\"]\n\nsubgraph github [\"GitHub\"]\n  user --&gt;|added to| team\n  subgraph org [\"@pawamoy-insiders\"]\n    team --&gt;|gives access to| repo1\n    team --&gt;|gives access to| repo2\n    repo3 --- repo3\n  end\nend\n\nlinkStyle 3 display:none;</code></pre> <p>Each repository in the Insiders organization is a soft fork of its corresponding public repository. By \"soft fork\" I mean that new features implemented in the Insiders repository eventually get merged back into the public repository. The Insiders repository is not required to actually be a fork in the GitHub sense (you can create it manually without using the \"Fork\" button on the public repository), nor to have the same name as the public repository. Public repositories can be hosted in any other GitHub organization or user account. For example in my case I have:</p> <pre><code>flowchart TB\nrepo_a[\"python\"]\nrepo_b[\"markdown-exec\"]\nirepo_a[\"mkdocstrings-python\"]\nirepo_b[\"markdown-exec\"]\n\nsubgraph github [\"GitHub\"]\n  subgraph mkdocstrings [\"@mkdocstrings\"]\n    repo_a --- repo_a\n  end\n  subgraph pawamoy [\"@pawamoy\"]\n    repo_b --- repo_b\n  end\n  subgraph insiders [\"@pawamoy-insiders\"]\n    irepo_a --- irepo_a\n    irepo_b --- irepo_b\n  end\n  repo_a --&gt;|soft forked as| irepo_a\n  repo_b --&gt;|soft forked as| irepo_b\nend\n\nlinkStyle 0 display:none;\nlinkStyle 1 display:none;\nlinkStyle 2 display:none;\nlinkStyle 3 display:none;</code></pre> <p>If your project already exists and you want to start developing insiders features, all you have to do is create the insiders repository in your dedicated organization. If you start a project from scratch, you have to create both the public and insiders repositories.</p> <p>Create and setup GitHub repositories with my <code>insiders</code> tool</p> <p> I created a tool called <code>insiders</code> to help me manage my insiders projects. It is able to create public and insiders repositories on GitHub, given you already have an insiders organization. See the CLI reference for creating projects. This project is available to sponsors only  (see what I did here??)</p> <p>You should also reserve a name on PyPI, for when the project becomes public. This short script will do this for you (replace my name and email with yours!):</p> reserve-pypi<pre><code>#!/usr/bin/env bash\n\ncat &lt;&lt;EOF &gt;/tmp/setup.py\nfrom setuptools import setup\n\nsetup(\n    name=\"$1\",\n    version=\"0.0.0\",\n    description=\"\",\n    long_description=\"\",\n    author=\"Timoth\u00e9e Mazzucotelli\",\n    author_email=\"dev@pawamoy.fr\",\n    packages=[],\n    classifiers=[\"Development Status :: 1 - Planning\"],\n)\nEOF\n\ncd /tmp || exit 1\nrm -rf dist\n\n# the following lines require setuptools, wheel and twine to be installed:\n# python -m pip install -U setuptools wheel twine\npython setup.py bdist_wheel\ntwine upload -r pypi dist/* --skip-existing\n</code></pre> <p>The script assumes you have a <code>~/.pypirc</code> file with the following contents:</p> <pre><code>[distutils]\nindex-servers =\n    pypi\n\n[pypi]\nusername: __token__\n</code></pre> <p>To authenticate against PyPI using a token, see \"How can I use API tokens to authenticate with PyPI?\"</p> <p>Register PyPI names with my <code>insiders</code> tool</p> <p> My <code>insiders</code> tool is able to register names on PyPI. See the CLI reference for registering PyPI names. Again, this project is available to sponsors only!</p>"},{"location":"posts/the-insiders-journey/#granting-or-revoking-access","title":"Granting or revoking access","text":"<p>Ideally, we want to automatically grant access to the Insiders projects when users start new sponsorships, and we want to revoke their access as soon as they cancel it. Depending on the platform(s) you use to collect sponsorships, you will have to query their API, react to their webhooks, or both. I only automated access for GitHub Sponsors, so this is what I will show you, but the approach is probably very similar for other platforms.</p> <p>With GitHub Sponsors, you have a few different ways to automate access to your Insiders projects. The easiest, but not the most efficient, is to create a cron job through GitHub Actions. This cron job can run every 30 minutes for example. When a user starts a new sponsorship, or update their current sponsorship to be eligible to the Insiders program, they will have to wait at most 30 minutes to receive an invitation. A more efficient way is to react to webhooks sent by GitHub: users will get the invitation within a minute. But it's also a bit more complicated and requires more infrastructure setup.</p> <p>Before anything, we need some code to grant and revoke access. I won't go into details, but you can find Python code in my sponsors repository that has everything we need. It expects to find a <code>TOKEN</code> environment variable, which is a GitHub token with <code>admin:org</code> and <code>read:user</code> permissions. When running it with <code>python insiders.py</code>, it will fetch all current members of the configured organization team, all the pending invitations, all the current sponsors, and grant or revoke access for each user depending on their sponsorship.</p>"},{"location":"posts/the-insiders-journey/#github-actions-cron-job","title":"GitHub Actions cron job","text":"<p>The action is relatively easy to configure:</p> <pre><code>name: insiders\n\non:\n  repository_dispatch:  # (1)!\n  workflow_dispatch:  # (2)!\n  schedule:  # (3)!\n    - cron: 0 * * * *\n\njobs:\n  insiders:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install dependencies\n        run: python -m pip install httpx\n      - name: Grant/revoke access\n        env:\n          TOKEN: ${{ secrets.TOKEN }} # (4)!\n        run: python insiders.py\n      - name: Commit changes\n        run: |\n          git config --global user.name \"Timoth\u00e9e Mazzucotelli\"\n          git config --global user.email \"pawamoy@users.noreply.github.com\"\n          if git commit -am \"CI: Update JSON data\"; then git push; fi\n</code></pre> <ol> <li>The <code>repository_dispatch</code> makes it possible to trigger the job with the GitHub API. This will be helpful if we want to react to webhooks sent by GitHub.</li> <li>The <code>workflow_dispatch</code> makes it possible to manually trigger the job from GitHub's web interface. It can be useful if you want to quickly synchronize Insiders access with your latest sponsorship updates.</li> <li>The actual cron scheduling. Here it says: \"execute this at minute 0 of every hour\".</li> <li>The <code>TOKEN</code> secret can be set in the Security / Secrets and variables / Actions tab of your repository settings.</li> </ol> <p>No pull request trigger!</p> <p> We must absolutely avoid running the worklow on pull requests, otherwise anyone could grant oneself access to the Insiders organization by adding their username in the Python script and opening a PR. A push trigger would be secure enough though, as only the maintainers of the repository holding the workflow can push to the main branch, so you might want to add <code>push: [main]</code> to the <code>on:</code> list.</p>"},{"location":"posts/the-insiders-journey/#github-webhooks","title":"GitHub webhooks","text":"<p>GitHub allows to configure webhooks in your sponsors dashboard:</p> <p></p> <p>When a user creates a new sponsorship, or cancels it, GitHub will send a payload to the configured URLs. The idea here is to deploy a small service somewhere (for example on Heroku, or as a server-less function in various cloud providers) that will receive this payload and act on it. It could validate the payload, making sure it comes from GitHub, before granting/revoking access for the user specified in the payload, or it could simply trigger our workflow defined above without any validation, since the workflow itself cannot be tempered with.</p> <p>Triggering the workflow for each payload uses a bit more GitHub resources, but it makes sure that no mistake can happen due to potential webhook race conditions.</p> <p>For a working solution that simply triggers the workflow without payload validation, take a look at my sponsors webhook app (feel free to fork the entire repository, which also has Heroku configuration). Upon receiving payloads, it will batch workflow triggers to avoid triggering the workflow too many times in a row.</p> <p>If you want to validate the payload in Python, you can use code like the following:</p> <pre><code>import hmac\nimport hashlib\nfrom fastapi import FastAPI, Request, HTTPException\nfrom loguru import logger\n\n# The webhook secret, configured when creating the webhook in your sponsors dashboard.\nWEBHOOK_SECRET = os.getenv(\"WEBHOOK_SECRET\")\n\napp = FastAPI()\n\n\ndef is_valid_signature(secret: str, payload: bytes, their_hash: str):\n    our_hash = hmac.new(secret.encode(\"utf8\"), payload, hashlib.sha256).hexdigest()\n    logger.debug(f\"Our computed hash: {our_hash}\")\n    logger.debug(f\"GitHub's hash: {their_hash}\")\n    return hmac.compare_digest(our_hash, their_hash.replace(\"sha256=\", \"\"))\n\n\n@app.post(\"/\")\nasync def handle_webhook(request: Request):\n    payload_data = await request.body()\n    payload_hash = request.headers[\"X-Hub-Signature-256\"]\n    if not is_valid_signature(WEBHOOK_SECRET, payload_data, payload_hash):\n        logger.error(\"Invalid payload hash\")\n        raise HTTPException(status_code=400, detail=\"Invalid payload hash.\")\n    ...  # grant/revoke access, or trigger the workflow\n</code></pre> <p>Once your GitHub action is cron-scheduled, and/or your service is up, everything is ready for you to start working on your Insiders features and projects! Using a project template can be a good idea, as it makes it easy to create new projects with all the boilerplate Insiders documentation and setup. I myself use my own Copier template, called Copier PDM, which has support for Insiders setups.</p> <p>To summarize:</p> <ul> <li> create private repositories in an Insiders organization </li> <li> automate access granting/revoking to your Insiders projects </li> <li> automate everything you can to help you kickstart new projects </li> </ul>"},{"location":"posts/the-insiders-journey/#workflow","title":"Workflow","text":"<p>At this point, you should have a working infrastructure to offer access to an Insiders program with projects and features exclusively available to sponsors, with automatic granting/revoking of access.</p> <p>Now lets talk about the workflow. At first sight it may seem simple: just add features in the Insiders repositories. But quickly there are a lot of questions that come to mind.</p> <p>What if an Insiders feature needs a refactor in public parts of the code? When I'm ready to release an Insiders version, how do I tag it? How will users install the Insiders version if it's not on PyPI? Should I have two documentation sites, one for the public project and one for the Insiders project? If I keep only one docs site, from which project do I deploy it, public or Insiders? What if an Insiders project depends on another Insiders project? etc.</p> <p>After a few months working on Insiders projects and features, here is the workflow I developed, presented as a list of tips.</p>"},{"location":"posts/the-insiders-journey/#where-to-work","title":"Where to work?","text":"<p>Do the maximum amount of work in the public repository. Any change (refactor, bug fix) that is not directly related to Insiders should be done in the public repository and then only, pulled into the Insiders one. I always define an <code>upstream</code> remote in my local Insiders repositories:</p> <pre><code>git remote add upstream git@github.com:pawamoy/project\n</code></pre> <p>...that I can pull from using:</p> <pre><code>git pull upstream main\n</code></pre> <p>Documentation should always be updated in the public repository, even parts that are directly related to Insiders.</p> <p>The less differences there are between your public and Insiders repositories, the easier it will be to merge Insiders features back into the public repository when funding goals are reached. It represents more work, because of the potential back and forth between commits pushed to the public repo, and pulled into the Insiders one, but it maintains a more consistent code base. I like to see my public repositories as the single sources of truth for my projects history: the Insiders repositories are ephemeral and should eventually get completely merged (or cherry-picked, or bluntly copied) into the public ones.</p>"},{"location":"posts/the-insiders-journey/#insiders-compatibility","title":"Insiders compatibility?","text":"<p>If possible, your Insiders versions should always be compatible with the public ones. By compatible, I mean that, in a project that depend on the Insiders version, downgrading it to the equivalent public version should not cause any issue. Ideally, public versions are aware of Insiders features, but these features are no-op. At the very least, usage of Insiders features or configuration while using a public version should not crash users' code.</p> <p>If you follow the previous tip, and do most of the work in the public version, keeping changes in the Insiders version to a minimum, it should help in maintaining compatibility between public and Insiders versions.</p>"},{"location":"posts/the-insiders-journey/#how-to-tag","title":"How to tag?","text":"<p>Tag your public releases normally (using semantic versioning or whatever suits you), and prefix your Insiders tags with the public ones: if your latest public release is version <code>1.2.3</code>, and your latest Insiders changes brought you to Insiders version <code>4.5.6</code>, then tag your commit in the Insiders repository with <code>1.2.3.4.5.6</code>. It means: Insiders 4.5.6 is based on the public version 1.2.3.</p> <p>The benefit is that it makes it clear which public version an Insiders version is based on, and the Insiders version is always a bit higher than the public one, PEP 440-ly speaking, so will take precedence over public versions during dependency resolution (given it's available in a local index, see How to install).</p> <p>Some projects use local version identifiers to make the Insiders version more readable, like <code>1.2.3+4.5.6</code> or even <code>1.2.3+insiders-4.5.6</code>, but for new projects I recommend against that, because local version identifiers are not always accepted by PyPI-like registries. It means that sponsors who would like to index your Insiders versions into their private registry, for convenience within their infrastructure, wouldn't be able to.</p>"},{"location":"posts/the-insiders-journey/#how-to-release","title":"How to release?","text":"<p>To release public versions, I use a local <code>release</code> task from my machine. There is nothing clever here, just use what suits you.</p> <p>To release Insiders versions, I tag the latest commit and push both commit and tag:</p> <pre><code>git tag 1.2.3.4.5.6\ngit push\ngit push --tags\n</code></pre> <p>CI then automatically creates a GitHub release (see my templated CI job). The built distributions (source and wheel) are attached as artifacts to the GitHub release.</p> <p>I recommend to always release a new Insiders version after a public release. Example: if you were at version 1.2.2 and 1.2.2.4.0.0 for Insiders, and just released 1.2.3, don't let Insiders behind at 1.2.2.4.0.0! Pull upstream changes into your Insiders local repository, and tag plus push a new Insiders version 1.2.3.4.0.0.</p> <p>Marketing again...</p> <p> I released the first version of a tool called Devboard that will help me (and maybe you) keep track of this, and other Insiders-related things. You guessed it: it's only available to sponsors </p> <p>To prevent mistakes like releasing an Insiders version to PyPI.org, I recommend adding a check to any release script to make sure it aborts with an error when ran in an Insiders project, whether it runs locally or in CI.</p>"},{"location":"posts/the-insiders-journey/#how-to-deploy-docs","title":"How to deploy docs?","text":"<p>Since documenting Insiders features will often require these features to actually be available, I recommend to always deploy docs from the Insiders project, whether it is done locally or in CI.</p> <p>To prevent mistakes like deploying from the public project, where the necessary Insiders features are not available, I also recommend adding a check to any deployment script, to make sure docs are never deployed from the public project.</p>"},{"location":"posts/the-insiders-journey/#how-to-install","title":"How to install?","text":"<p>Users with access to the private repositories can install Insiders versions with <code>pip install git+ssh://git@github.com/...</code>, or <code>pip install git+https://${GH_TOKEN}@github.com/...</code>, optionally passing Git references to the command, such as tags.</p> <p>A disadvantage of installing from GitHub using SSH or HTTPS is that this won't work for your contributors without access to Insiders. One solution is to default to the public versions in your dependencies specification, and override them with the Insiders ones in CI.</p> <p>Another solution is to use my PyPI Insiders project (sponsors only ), which lets you keep and update Insiders distributions in a local package index (or in any other private index, self-hosted or not). It lets you specify dependencies as if they were public, while tools will pick up Insiders versions from your local index if there are any available. Contributors without access will transparently get the public versions.</p> Enough with the tips! <p> </p>"},{"location":"posts/the-insiders-journey/#how-to-run-ci","title":"How to run CI?","text":"<p>You should run your complete test matrix in the public project CI, like you would normally do. Since they are private, Insiders repositories will need to be more frugal, as we only have (at the time of writing) 2000 CI minutes per month with GitHub's free plan. I exploded my 2000 minutes in a few days the first month, lesson learned.</p> <p>To avoid too much repetition in my GitHub workflows, I ended up with an additional job that will filter out some cells of the test matrix when running in an Insiders repository. See the <code>exclude-test-jobs</code> job in my CI workflow template. Basically, it detects if it runs in an Insiders repository, and in that case outputs a JSON list of patterns that is used by the next job to match and exclude test jobs.</p>"},{"location":"posts/the-insiders-journey/#how-to-merge","title":"How to merge?","text":"<p>Once a funding goal is reached, you will have to merge back features listed in that goal into the public versions of your projects. There are several ways to do that, and they will depend on how disciplined your Git history is.</p> <p>All methods except the last one require that you configure an Insiders remote in your public project and fetch it:</p> <pre><code>git remote add insiders git@github.com:pawamoy-insiders/project\ngit fetch insiders\n</code></pre> <ol> <li> If you develop Insiders features in branches, and keep these branches locally or remotely, you can try to pull them directly into a branch of your local public project. I did not try that because I just thought about it while writing this section, so I cannot guarantee this is the best way to merge Insiders features, but it looks promising (less conflicts than the next two methods).</li> <li>  If you don't have Insiders branches to pull from, but at least have a well ordered Git history, i.e. all changes related to a single Insiders feature are grouped together (in a chain of commits), you can cherry-pick these commits with a range selection: <code>git cherry-pick 0a0a0a0^..1b1b1b1</code> (both ends included). Be prepared to resolve conflicts.</li> <li>  If you don't have clean chains of commits, and if you're patient and like resolving conflicts, you can cherry-pick commits one by one. I tried that, it's not fun, really, which led me to the last method below.</li> <li> The probably dumbest and most efficient method is to recursively copy every file from your local Insiders clone into your local public clone, with a command like <code>cp -r insiders/project/* project/</code>. Then, make sure to only stage changes related to the features you want to merge, and discard the rest. You can stage and commit several times, once for each feature.</li> </ol> <p>The last method has two benefits:</p> <ul> <li>your feature commits now even pack bug fixes (where cherry-picking would have kept them separate). This is a subjective opinion of course, but as said previously in the post, Insiders repositories should be considered ephemeral (destined to disappear at some point), so keeping their Git history intact is not important. It's a bit like squashing commits before merging a pull request.</li> <li>you can take this opportunity to synchronize back any divergence between your public and Insiders versions, typically: documentation. This will further simplify future Insiders-to-public merges, as well as Git pulls from upstream in your Insiders project.</li> </ul> <p>Now, what happens to a project when Insiders features are merged back into its public version? There are a few different scenarios to consider:</p> <ul> <li>The entire project was under Insiders, and is now free: you might want to remove Insiders-related documentation, but keep your Insiders changelog, and links to it from now-free features. Also keep any documentation your have about completed funding goals. It's important to let users know these features were developed thanks to the funding from sponsors, especially if you have other Insiders projects. If you plan on adding new Insiders features to this project very soon, just keep everything.</li> <li>Some of the Insiders features for this project are now merged: update their documentation to remove any \"Sponsors only\" label. As above, keep links to changelogs under each previously-Insiders feature.</li> <li>All Insiders features for this project are now merged, and you will keep adding Insiders features soon or later: like above.</li> <li>All Insiders features for this project are now merged, and every new feature will immediately be free (this project stops being an Insiders project): you might want to remove all Insiders-related documentation, while keeping feature-changelog links and funding goals data.</li> </ul> <p>To simplify removing and adding back Insiders-related documentation to my projects when they go back and forth between these different states, my project template has a question \"Is it an Insiders project?\" that handles creation/removal of related files when updating a project. Then I just have to stage or discard changes with Git.</p> <p>Only merge upstream into Insiders main branch when you're ready to release!</p> <p> Since users likely install the Insiders version from the main branch (using a Git + HTTPS/SSH URL), you must make sure to merge upstream changes into the main Insiders branch only when you are also ready to publish a new Insiders release. Otherwise users will get changes that could break their project. And if you have to quickly cut out a bugfix release, you won't be able to because these changes from upstream are blocking the way. It is still useful to merge upstream changes early, especially when they are complex and you don't want to deal with hard conflicts, but in that case you should merge in a secondary branch.</p>"},{"location":"posts/the-insiders-journey/#communication","title":"Communication","text":"<p>I am definitely not an expert in comms/marketing, but here are some obvious tips on how to advertise your work.</p> <p>Your Insiders organization can have both a public README and a member-only README. You can use that to your advantage by telling about your Insiders program in the public README, and giving useful tips and instructions to your sponsors in the member-only README. For example, in the public README:</p> <pre><code># pawamoy Insiders\n\nThis organization contains all my [Insiders](https://pawamoy.github.io/insiders/) projects.\n[Learn what Insiders is](https://pawamoy.github.io/insiders/#what-is-insiders),\nor how to gain access by [becoming a sponsor](https://pawamoy.github.io/insiders/#how-to-become-a-sponsor).\n</code></pre> <p></p> <p>And in the member-only README:</p> <pre><code># pawamoy Insiders\n\nWelcome to [Insiders](https://pawamoy.github.io/insiders/)!\n\nThanks to your sponsorship, you became a member of this organization,\nand gained access to all of my Insiders projects.\n\nTo get started,\n[see how to install the Insiders version of a project](https://mkdocstrings.github.io/python/insiders/installation/)\n(for example *mkdocstrings-python Insiders*).\n\nMake sure you have read the (very short) [terms of use](https://pawamoy.github.io/insiders/#terms).\n\nIf you have any question regarding your sponsorship, or the Insiders program,\nsend me an email at insiders@pawamoy.fr.\n</code></pre> <p></p> <p>The public repositories of your Insiders projects (when the whole project is available to sponsors-only) should have a README that explains this project is for sponsors only, with links to your Insiders program documentation as well as the project's documentation. For example:</p> <pre><code># Project Name\n\nThis project is currently available to [sponsors](https://github.com/sponsors/pawamoy) only.\nSee the documentation here: https://pawamoy.github.io/project-name.\n</code></pre> <p>You can also set up a GitHub description like:</p> <pre><code>Short description of the project. Only available to sponsors.\n</code></pre> <p>Here is how it looks (at the time of writing) for my MkDocs Manpage project:</p> <p></p> <p>Automate GitHub Releases creation for both public and Insiders repositories. GitHub users can subscribe to releases activity, in their feed or through notifications. It makes your progress and your project's evolution visible.</p> <p>In your documentation, you can add a dismissable banner at the top to let readers know that there is an Insiders version, available to sponsors only. Each documented Insiders feature should also clearly mention that it is available to sponsors only, with a link to your Insiders program explanation. Readers landing on your pages thanks to search engines probably don't know anything about your motivations and why some features are only available behind paywalls, so it is very important to explain your strategy to them. Your documentation is the main funnel to new sponsorships: take good care of it!</p> <p>Use the \"Sponsor\" button feature of GitHub: add a <code>.github/FUNDING.yml</code> file in your repositories with contents like:</p> <pre><code>github: your_username\nko_fi: your_username\ncustom:\n  - https://www.paypal.me/your_username\n</code></pre> <p>Don't be shy - sometimes users just want to say thank you with $20 and will prefer sending them quickly to you using a platform they already have an account on. One-time donations are not what we are aiming at, to build reliable funding for our project(s), but they sure help and are very much appreciated.</p> <p>Add links to your Insiders program and/or funding pages on your GitHub profile, on your GitHub Sponsors profile, on your website, in your docs, in your blog posts. It is OK to advertise your work. You deserve to get funding for your projects. Don't spam though! People don't like spam. Try to stay authentic and humble. You don't need to disguise yourself as someone you're not: you just need to show that you care about your projects and about your users.</p>"},{"location":"posts/the-insiders-journey/#final-note","title":"Final note","text":"<p>A short summary.</p> <p>Sponsorware strategies benefit both users and developers while keeping your projects open-source. There are a number of platforms allowing developers to set up sponsorship accounts and tiers, to receive money and fund their open-source projects. You don't need a fancy infrastructure to manage sponsors and private projects, a GitHub organization and team work just fine. Juggling between public projects and their private counterparts necessitates a bit of discipline, and is made easier with good tooling. Hopefully the whole sponsorware setup can be automated away one step at a time.</p> <p>You may have noticed a few mentions to my Insiders project throughout this post  I'm creating quite a number of those, and have many more ideas. I would like to create a more unified experience around them, for managing Insiders programs from start to finish. For now, it's a lot of instructions and a bit of tooling/automation: I'm working to provide more of the latter. So, stay tuned! And if you start your own journey: good luck!</p>"},{"location":"posts/the-insiders-journey/#where-im-at","title":"Where I'm atJune, 2019September-October, 2020January, 2021October, 2021February, 2022April-June, 2022June, 2022September-November, 2022December, 2022January-March, 2023April, 2023May, 2023June, 2023July, 2023August, 2023September, 2023October, 2023November, 2023December, 2023January, 2024February, 2024","text":"<p>In the very first paragraph of this post, I wrote about \"keeping a journal\". I realise most of this post is more like a \"how-to\", so I'll reserve this section for personal updates that you might find entertaining.</p> <p>Creation of my GitHub Sponsors profile \ud83d\ude24</p> <p>First three sponsors: Chad Smith, Christoph Rieke, and Gunther Klessinger \ud83d\udc9c</p> <p>First user trying to game the system \ud83d\udc79</p> <p>New sponsor.</p> <p>Beginning of a long preparation for my sponsorware strategy.</p> <p>Three new sponsors.</p> <p>One-time donation of $550 from GitHub \ud83c\udf89</p> <p>Two new sponsors.</p> <p>First sponsorship cancellation \ud83d\ude22.</p> <p>Finalization of my Insiders setup. Five new sponsors, two cancellations. I quit my job to work full-time on my projects.</p> <p> New feature (markdown-exec): A Pyodide fence.Official launch of my Insiders program \ud83d\ude80One-time donation of $500 from Pydantic \ud83c\udf8aMartin ups his sponsorship to $100, supporting the MkDocs ecosystem. Two new sponsors.</p> <p> New feature (mkdocstrings-python): Cross-references for type annotations in signatures. New project: griffe2md, a tool to output API docs to Markdown using Griffe.</p> <p> New feature (mkdocstrings-python): Symbol types in headings and table of contents. New project: MkDocs Manpage, a plugin to generate a manpage from the documentation site. New project: PyPI Insiders, a self-hosted PyPI server with automatic updates for Insiders versions of projects.First two sponsorships from organizations, Pydantic and Kolena. Two new sponsors, one cancellation.</p> <p> New project: Griffe Pydantic, a Griffe extension for Pydantic. New project: Griffe Typing Deprecated, a Griffe extension for <code>@typing.deprecated</code> (PEP 702).</p> <p> New feature (mkdocstrings-python): Auto-summary of object members.One-time donation of $10,000 to MkDocs, by Bloomberg (I receive $1,000 \ud83e\udd38\u200d\u2642\ufe0f). Two new sponsors, one cancellation.</p> <p> New feature (mkdocstrings-python): Automatic rendering of function signature overloads New project: Markdown PyCon, a Markdown extension to parse <code>pycon</code> code blocks without indentation or fences. New project: Griffe TUI, a Textual user interface for Griffe. New project: mkdocstrings-shell, a shell scripts/libraries handler for mkdocstrings.One-time donation of $1,000 by Sebasti\u00e1n Ram\u00edrez \ud83e\udd2f Three new sponsors.</p> <p> New project: insiders, a tool to manage your Insiders projects.Creation of my company, a French micro-entreprise called pawamoy-insiders. Four new sponsors (two orgs), one cancellation.</p> <p> New project: Devboard, a development dashboard for your projects. New project: Griffe Inherited Docstrings, a Griffe extension to inherit docstrings from parent classes.Eight new sponsors, two cancellations.</p> <p>French administration sends me my SIREN number (company number). Three new sponsors.</p> <p>First funding goal reached! \ud83c\udf89 New feature (griffe): Markdown output for the <code>check</code> command. New feature (griffe): GitHub output for the <code>check</code> command.</p> <p>I declare my taxes for the first time \ud83d\udc76</p>"},{"location":"posts/the-insiders-journey/#whats-next","title":"What's next?","text":"<p>In this post, I only described the technical aspects of a sponsorware strategy. To truly make a living out of your projects, you also need to make it legal. It is an entirely different beast to grapple with.</p> <p>Once I am more comfortable with the administrative process in France for micro-entreprises, I would like to add a second part to this post. There is an awful lot of things to know when creating a micro-entreprise, things that are almost impossible to learn by ourselves. I would like to try and summarize the things I learned from professional counseilors.</p> <p>In the meantime, I will keep updating this post with the things I learn along the way </p>"},{"location":"posts/tingling-sensation-witnessing-existence/","title":"The tingling sensation of witnessing your own existence","text":"<p>I sometimes have a tingling sensation, a soft and warm chill flowing from the top of my spine up to my skull, when looking at someone using something that belongs to me (a physical object or something I created, like code).</p> <p>Do you too?</p> <p>This sensation is something I have experienced since I was a kid. One occurrence in particular I remember, is when a good friend of mine stayed at my home for a week or two (we were living in different cities). One day, as I was showing him all the drawings in my school agenda, drawn by both school friends and myself, he offered to draw one too. Or maybe I asked him to? In any case, I gave him a pen, and he started drawing something in the school agenda. He was sitting cross-legged on the carpet of the living room, and drawing on the coffee table. I was a bit setback, my back against the sofa. The room was silent. As I was watching him, both of us silent, I noticed how focused he seemed. He was very focused, drawing something in an object I used everyday and that was kind of intimate. He was using an object that was mine.</p> <p>That's when I started getting this chill in my spine and skull, a soft and warm chill like the one you can get when taking a hot shower, or when your crush lightly touches the back of your neck. It's the warm chill that makes your hairs stand up. It was coming in soft waves. It made me feel so relaxed, so serene.</p> <p>While drawing, my friend sometimes spoke to me, telling me what he was drawing, or asking me what I would like him to draw. I found that the less I interacted with him, and the more he was focused on drawing, without paying attention to me, the more I could maintain this tingling sensation. It was like this sensation was triggered when I was looking at someone using something of mine, from a physical and emotional distance.</p> <p>I couldn't really explain it, but it felt like I was witnessing my own existence. Indeed, someone that isn't me is using something that is connected to me. This thing could probably not exist without me, or not the way it exists now, and this other being is using it, focused on it. I felt like this somehow proved that I exist, and this is how I explain why these situations give me this warm and pleasant, physical sensation.</p> <p>I remember other occurrences, like when a high school friend was practicing his pen-spinning skills using my own customized pen (looking something like this). Yesterday, I got it again while reading code from a random person on GitHub making use of one of my Python libraries.</p> <p>This does not happen often, to the point that most of the time I forget about it. Since I forget about it, I never try to force it either, and that's probably for the best. When it happens though, I do try to maintain it as long as I can, because it feels great.</p> <p>For a long time, this was kind of a cute secret, since I had never told anyone. Since then, I told a few close friends about it, but none of them seem to experience the same thing.</p> <p>Do you?</p>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/","title":"Tips for writing good Python CLI/libraries","text":"<p>This post shows a few things that I consider best-practices, and a few other things that you should avoid when writing a Python command-line tool and/or library.</p>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/#motivation","title":"Motivation","text":"<p>Stems from duty, task runner that can run Python callables. Toot on Mastodon, quote from docs \"call to developers\".</p> <ul> <li>people will want to use your tool programatically,     whether you provide a public interface or not</li> <li>it is just easier to write, maintain and test</li> </ul>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/#kindness-disclaimer","title":"Kindness disclaimer","text":"<ul> <li>it's fine not to care about programatic use (though it makes things easier)</li> <li>no judgments on the projects I mention as examples of what to do or not:     written by people when they didn't have experience,     or didn't think/care about programatic use</li> </ul>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/#dos-and-donts","title":"Do's and don'ts","text":"<ul> <li>don't tie logic to your CLI (BE: black, coverage)</li> <li>think \"library first\" (BE: pydeps)</li> <li>decide what you expose in the CLI, write a function for each</li> <li>accept only built-in types in these functions</li> <li>the CLI just translates CLI arguments to Python arguments</li> <li>capture stdout/stderr of subprocess and re-print it</li> <li>don't exit the process in functions</li> <li>avoid fancy IO management</li> <li>avoid fancy decorators</li> <li>use <code>main.py</code> and <code>cli.py</code></li> <li>@ionelmc's wisdom <code>__main__.py</code></li> <li>accepts args in your CLI entrypoints</li> <li>(unsure) if you operate on files, accepts directories and implement discovery/recursion</li> <li>(tyro) https://brentyi.github.io/tyro/examples/01_basics/01_functions/ not good,   hides actual CLI args</li> </ul>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/#benefits","title":"Benefits","text":"<ul> <li>providing a callable means it will work for every version of your project     (impossible otherwise)</li> </ul>"},{"location":"posts/tips-for-writing-good-python-cli-libraries/#bad-examples","title":"Bad examples","text":"<ul> <li>Twine adds logic to its <code>__main__.main</code> function which does not accept arguments.</li> <li>pyproject-build defines its main method in <code>__main__</code>.</li> </ul>"},{"location":"posts/unify-logging-for-a-gunicorn-uvicorn-app/","title":"Unify Python logging for a Gunicorn/Uvicorn/FastAPI application","text":"<p>I recently started playing with FastAPI and HTTPX, and I am deploying my app with Gunicorn and Uvicorn workers.</p> <p>But when serving, the logs from each component looks quite different from the others. I want them to all look the same, so I can easily read them or exploit them in something like Kibana.</p> <p>After a lot of hours trying to understand how Python logging works, and how to override libraries' logging settings, here is what I have...</p> <p>A single <code>run.py</code> file! I didn't want to split logging configuration, Gunicorn configuration, and the rest of the code into multiple files, as it was harder to wrap my head around it.</p>"},{"location":"posts/unify-logging-for-a-gunicorn-uvicorn-app/#gunicorn-uvicorn-version","title":"Gunicorn + Uvicorn version","text":"<p>Everything is contained in this single file:</p> <pre><code>import os\nimport logging\nimport sys\n\nfrom gunicorn.app.base import BaseApplication\nfrom gunicorn.glogging import Logger\nfrom loguru import logger\n\nfrom my_app.app import app\n\n\nLOG_LEVEL = logging.getLevelName(os.environ.get(\"LOG_LEVEL\", \"DEBUG\"))\nJSON_LOGS = True if os.environ.get(\"JSON_LOGS\", \"0\") == \"1\" else False\nWORKERS = int(os.environ.get(\"GUNICORN_WORKERS\", \"5\"))\n\n\nclass InterceptHandler(logging.Handler):\n    def emit(self, record):\n        # get corresponding Loguru level if it exists\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # find caller from where originated the logged message\n        frame, depth = sys._getframe(6), 6\n        while frame and frame.f_code.co_filename == logging.__file__:\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n\n\nclass StubbedGunicornLogger(Logger):\n    def setup(self, cfg):\n        handler = logging.NullHandler()\n        self.error_logger = logging.getLogger(\"gunicorn.error\")\n        self.error_logger.addHandler(handler)\n        self.access_logger = logging.getLogger(\"gunicorn.access\")\n        self.access_logger.addHandler(handler)\n        self.error_logger.setLevel(LOG_LEVEL)\n        self.access_logger.setLevel(LOG_LEVEL)\n\n\nclass StandaloneApplication(BaseApplication):\n    \"\"\"Our Gunicorn application.\"\"\"\n\n    def __init__(self, app, options=None):\n        self.options = options or {}\n        self.application = app\n        super().__init__()\n\n    def load_config(self):\n        config = {\n            key: value\n            for key, value in self.options.items()\n            if key in self.cfg.settings and value is not None\n        }\n        for key, value in config.items():\n            self.cfg.set(key.lower(), value)\n\n    def load(self):\n        return self.application\n\n\nif __name__ == \"__main__\":\n    intercept_handler = InterceptHandler()\n    # logging.basicConfig(handlers=[intercept_handler], level=LOG_LEVEL)\n    # logging.root.handlers = [intercept_handler]\n    logging.root.setLevel(LOG_LEVEL)\n\n    seen = set()\n    for name in [\n        *logging.root.manager.loggerDict.keys(),\n        \"gunicorn\",\n        \"gunicorn.access\",\n        \"gunicorn.error\",\n        \"uvicorn\",\n        \"uvicorn.access\",\n        \"uvicorn.error\",\n    ]:\n        if name not in seen:\n            seen.add(name.split(\".\")[0])\n            logging.getLogger(name).handlers = [intercept_handler]\n\n    logger.configure(handlers=[{\"sink\": sys.stdout, \"serialize\": JSON_LOGS}])\n\n    options = {\n        \"bind\": \"0.0.0.0\",\n        \"workers\": WORKERS,\n        \"accesslog\": \"-\",\n        \"errorlog\": \"-\",\n        \"worker_class\": \"uvicorn.workers.UvicornWorker\",\n        \"logger_class\": StubbedGunicornLogger,\n    }\n\n    StandaloneApplication(app, options).run()\n</code></pre> <p>If you are in a hurry, copy-paste it, change the Gunicorn options at the end, and try it!</p> <p>If you're not, I will explain each part below.</p> <pre><code>import os\nimport logging\nimport sys\n\nfrom gunicorn.app.base import BaseApplication\nfrom gunicorn.glogging import Logger\nfrom loguru import logger\n</code></pre> <p>This part is easy, we simply import the things we need. The Gunicorn <code>BaseApplication</code> so we can run Gunicorn directly from this script, and its <code>Logger</code> that we will override a bit. We are using Loguru later in the code, to have a pretty log format, or to serialize them.</p> <pre><code>from my_app.app import app\n</code></pre> <p>In my project, I have a <code>my_app</code> package with an <code>app</code> module. My FastAPI application is declared in this module, something like <code>app = FastAPI()</code>.</p> <pre><code>LOG_LEVEL = logging.getLevelName(os.environ.get(\"LOG_LEVEL\", \"DEBUG\"))\nJSON_LOGS = True if os.environ.get(\"JSON_LOGS\", \"0\") == \"1\" else False\nWORKERS = int(os.environ.get(\"GUNICORN_WORKERS\", \"5\"))\n</code></pre> <p>We setup some values from environment variables, useful for development vs. production setups. <code>JSON_LOGS</code> tells if we should serialize the logs to JSON, and <code>WORKERS</code> tells how many workers we want to have.</p> <pre><code>class InterceptHandler(logging.Handler):\n    def emit(self, record):\n        # get corresponding Loguru level if it exists\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # find caller from where originated the logged message\n        frame, depth = sys._getframe(6), 6\n        while frame and frame.f_code.co_filename == logging.__file__:\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n</code></pre> <p>This code is copy-pasted from Loguru's documentation! This handler will be used to intercept the logs emitted by libraries and re-emit them through Loguru.</p> <pre><code>class StubbedGunicornLogger(Logger):\n    def setup(self, cfg):\n        handler = logging.NullHandler()\n        self.error_logger = logging.getLogger(\"gunicorn.error\")\n        self.error_logger.addHandler(handler)\n        self.access_logger = logging.getLogger(\"gunicorn.access\")\n        self.access_logger.addHandler(handler)\n        self.error_logger.setLevel(LOG_LEVEL)\n        self.access_logger.setLevel(LOG_LEVEL)\n</code></pre> <p>This code was copied from this GitHub comment by @dcosson. Thanks! It will allow us to override Gunicorn's own logging configuration so its logs can be formatted like the rest.</p> <pre><code>class StandaloneApplication(BaseApplication):\n    \"\"\"Our Gunicorn application.\"\"\"\n\n    def __init__(self, app, options=None):\n        self.options = options or {}\n        self.application = app\n        super().__init__()\n\n    def load_config(self):\n        config = {\n            key: value\n            for key, value in self.options.items()\n            if key in self.cfg.settings and value is not None\n        }\n        for key, value in config.items():\n            self.cfg.set(key.lower(), value)\n\n    def load(self):\n        return self.application\n</code></pre> <p>This code is taken from Gunicorn's documentation. We declare a simple Gunicorn application that we will be able to run. It accepts all Gunicorn's options.</p> <pre><code>if __name__ == \"__main__\":\n    intercept_handler = InterceptHandler()\n    # logging.basicConfig(handlers=[intercept_handler], level=LOG_LEVEL)\n    # logging.root.handlers = [intercept_handler]\n    logging.root.setLevel(LOG_LEVEL)\n</code></pre> <p>We simply instantiate our interception handler, and set the log level on the root logger.</p> <p>Once again, I fail to understand how this works exactly, as the two commented lines have no impact on the result. I did a lot of trial and error and ended up with something working, but I cannot entirely explain why. The idea here was to set the handler on the root logger so it intercepts everything, but it was not enough (logs were not all intercepted).</p> <pre><code>seen = set()\nfor name in [\n    *logging.root.manager.loggerDict.keys(),\n    \"gunicorn\",\n    \"gunicorn.access\",\n    \"gunicorn.error\",\n    \"uvicorn\",\n    \"uvicorn.access\",\n    \"uvicorn.error\",\n]:\n    if name not in seen:\n        seen.add(name.split(\".\")[0])\n        logging.getLogger(name).handlers = [intercept_handler]\n</code></pre> <p>Here we iterate on all the possible loggers declared by libraries to override their handlers with our interception handler. This is where we actually configure every logger to behave the same.</p> <p>For a reason that I fail to understand, Gunicorn and Uvicorn do not appear in the root logger manager, so we have to hardcode them in the list.</p> <p>We also use a set to avoid setting the interception handler on the parent of a logger that is already configured, because otherwise logs would be emitted twice or more. I'm not sure this code can handle levels of nested loggers deeper than two.</p> <pre><code>logger.configure(handlers=[{\"sink\": sys.stdout, \"serialize\": JSON_LOGS}])\n</code></pre> <p>Here we configure Loguru to write on the standard output, and to serialize logs if needed.</p> <p>At some point I was also using <code>activation=[(\"\", True)]</code> (see Loguru's docs), but it seems it's not required either.</p> <pre><code>options = {\n    \"bind\": \"0.0.0.0\",\n    \"workers\": WORKERS,\n    \"accesslog\": \"-\",\n    \"errorlog\": \"-\",\n    \"worker_class\": \"uvicorn.workers.UvicornWorker\",\n    \"logger_class\": StubbedGunicornLogger,\n}\n\nStandaloneApplication(app, options).run()\n</code></pre> <p>Finally, we set our Gunicorn options, wiring things up, and run our application!</p> <p>Well, I'm not really proud of this code, but it works!</p> <p></p> <p></p>"},{"location":"posts/unify-logging-for-a-gunicorn-uvicorn-app/#uvicorn-only-version","title":"Uvicorn-only version","text":"<p>Added Nov 11, 2020.</p> <p>The Uvicorn-only version is way more simple. Note that since this post was published the first time, a new Uvicorn version was released, which contained a fix for its logging configuration: could be in 0.11.6 (Don't override the root logger) or 0.12.0 (Dont set log level for root logger).</p> <p>This simplifies a lot the <code>setup_logging</code> function, which now makes more sense and is easier to understand:</p> <pre><code>import os\nimport logging\nimport sys\n\nfrom uvicorn import Config, Server\nfrom loguru import logger\n\nLOG_LEVEL = logging.getLevelName(os.environ.get(\"LOG_LEVEL\", \"DEBUG\"))\nJSON_LOGS = True if os.environ.get(\"JSON_LOGS\", \"0\") == \"1\" else False\n\n\nclass InterceptHandler(logging.Handler):\n    def emit(self, record):\n        # get corresponding Loguru level if it exists\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # find caller from where originated the logged message\n        frame, depth = sys._getframe(6), 6\n        while frame and frame.f_code.co_filename == logging.__file__:\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n\n\ndef setup_logging():\n    # intercept everything at the root logger\n    logging.root.handlers = [InterceptHandler()]\n    logging.root.setLevel(LOG_LEVEL)\n\n    # remove every other logger's handlers\n    # and propagate to root logger\n    for name in logging.root.manager.loggerDict.keys():\n        logging.getLogger(name).handlers = []\n        logging.getLogger(name).propagate = True\n\n    # configure loguru\n    logger.configure(handlers=[{\"sink\": sys.stdout, \"serialize\": JSON_LOGS}])\n\n\nif __name__ == \"__main__\":\n    server = Server(\n        Config(\n            \"my_app.app:app\",\n            host=\"0.0.0.0\",\n            log_level=LOG_LEVEL,\n        ),\n    )\n\n    # setup logging last, to make sure no library overwrites it\n    # (they shouldn't, but it happens)\n    setup_logging()\n\n    server.run()\n</code></pre>"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/","title":"Write and use a tox plugin from inside your package","text":"<p>So let's say you wrote a program that is using tox and you want to add some options to tox's command-line. Reading at tox's documentation about plugins, you see that you have to make a pip installable plugin and setup some entry point. In fact, you can skip the pip installable side and integrate the plugin directly within your code.</p> <p>It can be done in three very simple steps.</p> <ul> <li>Add a tox_module in your package</li> <li>Setup the entry points</li> <li>Update your code</li> </ul>"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#add-a-tox_module-in-your-package","title":"Add a tox_module in your package","text":"<p>Create a new module somewhere. Its name should begin with <code>tox_</code>.</p> <pre><code># tox_your_module.py\nfrom tox import hookimpl\n\n\n@hookimpl\ndef tox_addoption(parser):\n    parser.add_argument(\"my_new_option\")\n    # read argparse's doc online to see more complex examples\n</code></pre>"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#setup-the-entry-points","title":"Setup the entry points","text":"<p>Tox has an automagic system that will see installed plugins and load them when you call it. Just set the new <code>tox</code> entry point:</p> <pre><code># setup.py\nsetup(\n    ...\n    entry_points={\n        'console_scripts': 'your_program = your_program.main:main',  # to be adapted\n        'tox': ['your_module = your_program.tox_your_module']\n    },\n    ...\n)\n</code></pre> <p>Installing your package will make it visible to tox.</p>"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#update-your-code","title":"Update your code","text":"<p>In your code you will now be able to use the <code>my_new_option</code> option in tox's config object, and do whatever you want with it!</p> <pre><code>from tox.session import prepare\n\nconfig = prepare(args)\n\nprint(config.option.my_new_option)\n</code></pre> <p>Voil\u00e0.</p> <p>Of course your plugin could be much more complex, but this is another story.</p>"},{"location":"showcase/aria2p/","title":"aria2p","text":"<ul> <li>Repository: https://github.com/pawamoy/aria2p</li> <li>Documentation: https://pawamoy.github.io/aria2p</li> </ul> <p>I like automating things, or at least making them scriptable, controllable from the command line. One of those \"things\" is downloading stuff from the internet. I tried some GUI applications for torrents (Transmission, Deluge), and was always disappointed by their lack of CLI-friendliness.</p> <p>On r/torrents, people pointed me to Rtorrent (or is it RTorrent? rTorrent?), which seems to be extremely configurable, scriptable, and powerful. But it also seemed very complicated to grasp, particularly because it implements its own configuration language.</p> <p>So here I went again... doing things myself. Well, not really myself. I chose to write a client for aria2, because it offered an XML-RPC and a JSON-RPC interface.</p> <p>And this is how <code>aria2p</code> was born! I started by creating a client with nothing more, nothing less than what <code>aria2</code> exposes in its JSON-RPC interface. And then I built upon that client to provide more high-level methods. The manual page of <code>aria2</code> is really well detailed, so it helped a lot.</p> <p>I'm now running it on a RaspberryPi on which a hard-drive is plugged. I can define callbacks (as Python functions) to process a download's files when it finishes, like moving them in the right directory based on their extension.</p> <p>I really enjoyed writing this library and command-line tool, and I poured all my experience in Python in this project's management and configuration. I always use it as my sandbox for new cutting edge analysis tools or static site generators.</p> <p>I also learned how to write an HTOP-like interface, something I always admired and wanted to try coding. There were some challenges, like managing vertical+horizontal scroll while following a particular line or not, as well as separating the data from the logic and the presentation, or making the interface snappy, reacting fast to user input, without loosing them. It's not finished though! There is room for a lot of improvements, like charts at the top of the window, a status bar, additional views for download details, configuration option, more actions, etc.</p> <p>I want to write a tutorial on how to write an HTOP-like interface in Python someday, using <code>aria2p</code> as an example. Such a tutorial would have helped me tremendously back then, so I'm sure it would help other developers.</p>"},{"location":"showcase/copier-pdm/","title":"copier-pdm","text":"<ul> <li>Repository: https://github.com/pawamoy/copier-pdm</li> <li>Documentation: https://pawamoy.github.io/copier-pdm/</li> </ul> <p>I was first introduced to project templating with CookieCutter and especially the cookiecutter-pylibrary from @ionelmc. I learned many, many things about Python thanks to Ionel and his template, discovering its features, design choices, opinions, and selected tools during the years I worked with it. I eventually forked it into my own cookiecutter-pydjama, which was mostly designed to quickstart Django apps. Then I slowly stopped using Django, and therefore stopped maintaing my cookiecutter, which is now archived.</p> <p>Back when I was still using cookiecutters, one of the main challenges was to keep generated projects up-to-date with the latest change in the upstream cookiecutters. I'm always working on a lot of Python projects in parallel, so without a way to keep each one of them tightly structured, they all drift apart and it's hell to maintain them and switch from one to another.</p> <p>A solution to this, discussed on CookieCutter's issue tracker, was to set up a <code>template</code> branch on the generated repositories, branch that was dedicated to receiving templates updates. These updates were then merged into the main branch, so as to allow a bit of drifting from the template, but still stay up-to-date. It was implemented using a clever mix of git and bash commands. And it was not perfect.</p> <p>Then cruft (by @timothycrosley) made its appearance. At that time I was not really using templates anymore, but was still lurking on cookiecutter#784 and cookiecutter#1004 to make sure I would not miss any great attempt at solving this problem.</p> <p>Cruft is basically a wrapper around CookieCutter that brings this updating functionality using some git magic. Like a more robust and efficient version of my previous shell script. I used it for some time, discovered some issues, sent one or two PRs. But I was not really satisfied, and the project was not really active (it is now, thanks to @samj1912 who really improved it).</p> <p>And then, Copier made its entrance. A modest entrance. But a very interesting one. In a few hours of checking it and testing it, I was hooked. I took my old cookiecutter and started translating it to a Copier template, opening issues with feedback in the process. And it worked so well! I used this opportunity to also update my tools stack, trying out new bleeding-edge cool things to build the best Python project template I could.</p> <p>And this is how copier-poetry was born!</p> <p>Wait what? copier-poetry? not -pdm?</p> <p>Well yeah, before PDM, I was using Poetry (and am still using it). Poetry simply revolutionized Python packaging and dependency management. I'll let you see for yourself. At some point though I discovered PDM, and it promised such great things! The exceptional UX of Poetry, but more in touch with the new cool PEPs around <code>pyproject.toml</code> and packaging, like (and I quote):</p> <ul> <li>PEP 582 local package installer and runner, no virtualenv involved at all.</li> <li>PEP 621 project metadata format.</li> </ul> <p>And it reads <code>pip</code>'s configuration, which makes it so easier to integrate in corporate environments! And it has groups for development dependencies </p> <p>(It's funny because @sdispater and @frostming, authors of Poetry and PDM, seem to have worked together before!)</p> <p>I couldn't resist and duplicated copier-poetry into copier-pdm. It's the same thing, but with PDM instead of Poetry. If you are using copier-poetry, I'm sorry, all my love will now go into copier-pdm and I likely won't update copier-poetry anymore  But I'll review your PRs of course!</p> <p>To go back to Copier: Copier is a templating system and CLI tool with the ability to generate projects from templates (using Jinja2) and the ability to update them, which is built-in, when the template evolves! Not being a wrapper around another tool allows it to have great features, such as migrations, tasks, self-templating, multi-templating, interactive prompts, and more! At the time of writing, @Yajo has put and is putting a lot of work into improving Copier, and I can't wait for v6!</p>"},{"location":"showcase/dependenpy/","title":"dependenpy","text":"<ul> <li>Repository: https://github.com/pawamoy/dependenpy</li> <li>Documentation: https://dependenpy.readthedocs.io/en/latest/index.html</li> </ul>"},{"location":"showcase/duty/","title":"duty","text":"<ul> <li>Repository: https://github.com/pawamoy/duty</li> <li>Documentation: https://pawamoy.github.io/duty</li> </ul> <p>A few years ago, when I was writing Django-apps, I was using tox to test my apps on multiple Django versions. I was happy with it, but found it a bit slow when running jobs on a P*D-sized matrix (Python versions, Django versions). Fortunately detox was able to run tox envs in parallel.</p> <p>At some point I stopped writing Django apps, and started searching an alternative to tox with a lighter output, because I really don't like seeing walls of text when a single line is enough (test passed? just print ok).</p> <p>I started playing with Invoke, but quickly had issues with it, notably in Windows runs on GitHub. I also felt limited in that I couldn't run Python code directly, only subprocesses. I tried to implement such a feature in Invoke, but couldn't manage to do it.</p> <p>I also played with Nox, but found it too slow as well, even when reusing virtualenvs. And the console output was again too heavy for my taste. Now that PDM supports Nox, I should definitely give it another try though!</p> <p>In the end, I wrote my own task runner, duty, which is heavily inspired by Invoke. duty is based on failprint, an other project of mine that deals with running commands and capturing their output. On top of that, duty adds the concept of tasks, task collections, and other helpful utilities, like callables for popular Python tools.</p>"},{"location":"showcase/git-changelog/","title":"git-changelog","text":"<ul> <li>Repository: https://github.com/pawamoy/git-changelog</li> <li>Documentation: https://pawamoy.github.io/git-changelog</li> </ul>"},{"location":"showcase/griffe/","title":"griffe","text":"<ul> <li>Repository: https://github.com/mkdocstrings/griffe</li> <li>Documentation: https://mkdocstrings.github.io/griffe</li> </ul>"},{"location":"showcase/mkdocstrings/","title":"mkdocstrings","text":"<ul> <li>Repository: https://github.com/mkdocstrings/mkdocstrings</li> <li>Documentation: https://mkdocstrings.github.io/</li> </ul> <p>A few years ago (around 2020), I bumped into the Pydantic project (again). I had already seen it before, mainly interested in its settings configuration ability that could replace most of the logic in my Django app django-appsettings. This time, when I landed on its docs pages, I thought: \"wow, this looks nice\". Then, a bit later, FastAPI exploded in our faces, and I thought again: \"hey, looks familiar, and I love it!\". So I looked around and saw that it used MkDocs and a theme called \"Material for MkDocs\".</p> <p>I started migrating my projects to MkDocs and Material for MkDocs from Sphinx, because it looked nicer than the ReadTheDocs theme, and was far easier to use. Indeed, for the six previous years I had been using Sphinx for my projects documentation, and I never quite enjoyed writing docs with it. This was before the MyST parser or recommonmark (which I used a bit later), so I was writing my documentation in reStructuredText. I constantly had to check the syntax of rST and the Sphinx docs to achieve basic things. In particular, I wanted to have ToC (Table of Contents) entries in the sidebar for every documented module, class or function auto-generated with Sphinx's autodoc extension. I posted a question on StackOverflow and then found a feature request on Sphinx's bugtracker: the answer was \"it's not possible (yet)\".</p> <p>So I thought, hey, why not bring that to MkDocs instead of Sphinx? At the time, the only viable option for autodoc in MkDocs was Tom Christie's mkautodoc. Tom expressed his lack of capacity to work on the project, and I had an itch to scratch, so I decided to create my own MkDocs plugin for auto-documentation. This is how mkdocstrings was born. Tom's code has been very helpful at the beginning of the project (mkdocstrings' <code>:::</code> syntax actually comes from mkautodoc), so thanks Tom!</p> <p>Today mkdocstrings supports four languages (Crystal, Python, VBA and shell scripts) and will soon have support for more (TypeScript). In the future, with the help of my sponsors funding my work, I would like to support many more languages! Something I'm super proud of is that the two projects that initially made me want to create mkdocstrings, Pydantic and FastAPI, are now using it for their own docs \ud83c\udf89</p>"},{"location":"showcase/mvodb/","title":"mvodb","text":"<ul> <li>Repository: https://github.com/pawamoy/mvodb</li> </ul>"},{"location":"showcase/shell-history/","title":"shell-history","text":"<ul> <li>Repository: https://github.com/pawamoy/shell-history</li> </ul>"},{"location":"showcase/shelldemo/","title":"shelldemo","text":"<ul> <li>Repository: https://github.com/pawamoy/shelldemo</li> </ul>"},{"location":"showcase/shellman/","title":"shellman","text":"<ul> <li>Repository: https://github.com/pawamoy/shellman</li> </ul>"}]}